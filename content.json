{"posts":[{"title":"Appium commands that are commonly used on Mac","text":"In this example, we will use Appium Inspector to start a seesion on your emulator on Android Studio. Start appium First, start the connection with Appium by cmd appium --allow-cors. It also gives you the remote url and the automationName that can be used in the Appium Inspector Start Session deviceName: adb devices platformName - appium driver list platformVersion - adb shell getprop ro.build.version.release Now you can start a session in your emulator, but it always begins from the home page. To directly start a session on certain App, you also need to provide the appPackage and the appActivity. Session with AppsIf your app is downloaded from the PlayStore. It is possible that the apk file name/path is hidden. However, you can You can simply open the app and check its name by adb shell dumpsys window | grep mCurrentFocus. It returns the appPackage and the current appActivity of the App, which are both necessary for our seesion to start. However, sometimes the current activity may not be directly opened. Then we need to check all the possible activities this App contans and try them out. To check the activity list, we can use adb shell dumpsys package YOUR_APP_appPackage | grep -i activity. Now, we can start our emulator, and run the command adb shell am start -n YOUR_APP_appPackage/YOUR_APP_appActivity to see whether it can start the App successfully. If it works we can use it in our inspector.","link":"/appium/"},{"title":"Why you shouldn&#39;t deploy your Hexo webpage using GitHub Desktop?","text":"Methods to Deploy Hexo to GitHub.ioAssume you’ve created a repository on GitHub called &lt;username&gt;.github.io. Here are two common method you can deploy you Hexo Blog: Hexo CommandHexo’s documentations and Tutorial has provided sufficient instructions on deploying your personal website on your GitHub repository. According to the Hexo Tutorial, we can deploy the repository by using GitHub Actions. Create and Add the following contents to .github/workflows/pages.yml: name: Pages on: push: branches: - main # default branch jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 with: token: ${{ secrets.GITHUB_TOKEN }} submodules: recursive - name: Use Node.js 20 uses: actions/setup-node@v4 with: # Examples: 20, 18.19, &gt;=16.20.2, lts/Iron, lts/Hydrogen, *, latest, current, node # Ref: https://github.com/actions/setup-node#supported-version-syntax node-version: &quot;&gt;=20&quot; - name: Cache NPM dependencies uses: actions/cache@v4 with: path: node_modules key: ${{ runner.OS }}-npm-cache restore-keys: | ${{ runner.OS }}-npm-cache - name: Install Dependencies run: npm install - name: Build run: npm run build - name: Upload Pages artifact uses: actions/upload-pages-artifact@v3 with: path: ./public deploy: needs: build permissions: pages: write id-token: write environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v4 Install hexo-deployer-git. Add/Change the following configurations to _config.yml: deploy: type: git repo: https://github.com/&lt;username&gt;/&lt;project&gt; # for example, this blog is https://github.com/greenmeeple/greenmeeple.github.io branch: gh-pages After finishing your bog posts, Run hexo clean &amp;&amp; hexo deploy. GitHub DesktopMany Users installed GitHub Desktop for better visualization on changes, so do I. It provides more intuitive push and commit procedure and instruction compared to terminal. Most of the time I use it to make sure no unexpected line changes or modification. But soon I noticed that, every time after running hexo clean &amp;&amp; hexo deploy, GitHub Desktop will warn me that there’s something need to be pulled. When I pull it for merging it return Unable to merge unrelated histories in repository. Even in the image above, it shows that I should pull something. However, how would I need to pull if I’ve just push it? Security ProblemSo I inspect my repository, these two method actually deploy completely different content to the repository, even though they output the identical content on the webpage. When you deploy you webpage with Hexo command, it actually creates a folder .deploy_git, which is static HTML content without showing any configurations like your themes folder or _config.yml folder. In contrast, GitHub Desktop solely commit all folder that is not in .gitignore file and the website just rendered dynamically in the repository when someone visit. This create a huge security problem as much as it seems. All contents in your config is now visible to everyone. Since Hexo is a simple framework that depends heavily on Markdown and .yml files, there’s on where to hide all your settings and &lt;script&gt; if they just directly commit to your repository before building it statically. This may include not only your SEO and functionality of your webiste, but even some secret variable. GitTalk comment section and GitHub OAUTHFor example, the comment section below every posts in this blog are powered by GitTalk. It requires users to login through GitHub to comment. In order to handle the authorization of login, blog owners need to create an OAUTH App. Then they need to input their clientID and clientSecret initiate the plugin. For Hexo file structure, this will usually be stored in _config.yml. Therefore, if Blog Owner simply commit the whole folder using GitHub Desktop, their OAUTH App credentials are leaked to everyone. And this is how I start noticing the two deploy methods above are so different. Saving your sensitive information from Data Leak?As you may know, once you commit your issue on GitHub, it will leave a trace. This is because the version control nature of GitHub. But that also means everyone can always inspect your repositories’ history, even your newest version already removed your leaked data. Other than deleting your repository and start all over again, you may also cover and rewrite your commit history, and even rewrite the content by following this. Further Reading: Environment variable, How to push code to Github hiding the API keys?","link":"/github-desktop/"},{"title":"Mathjax prime superscript problem in Hexo theme","text":"Prime superscript problem (e.g. x’_i) in MathjaxWhen I was using Mathjax to create math formula in my blog post, I typed((q_1, q_2), a, (q'_1, q'_2)) \\in S \\times \\Sigma_{int} \\times S and it rendered as $((q_1, q_2), a, (q’_1, q’2)) \\in S \\times \\Sigma{int} \\times S$ However, when ((q_1, q_2), a, (q'_1, q'_2)) &amp; \\in S \\times \\Sigma_{int} \\times S are seperated, they rendered properly. $$((q_1, q_2), a, (q’_1, q’_2))$$ $$\\in S \\times \\Sigma_{int} \\times S$$ Maybe I should use \\left and \\right for (), just like \\lbrace and \\rbrace for {}? So I typed \\left( \\left( q_1, q_2 \\right), a, \\left( q'_1, q'_2 \\right) \\right) \\in S \\times \\Sigma_{int} \\times S, didn’t work out: $\\left( \\left( q_1, q_2 \\right), a, \\left( q’_1, q’2 \\right) \\right) \\in S \\times \\Sigma{int} \\times S$ Source of error Problem definitely comes from the first half of the formula, since second half are all variables. Turns out there may be an issue with how the prime symbol are being handled. For simple formula, q'_1 and q_1' are considered the identical.However, for more complicated formula, the only q_1' can be rendered correctly: $$((q_1, q_2), a, (q_1’, q_2’)) \\in S \\times \\Sigma_{int} \\times S$$ SolutionsAfterwards, I found people reported similar issue before. Two basic solutions: Stick to the format x_{Subscript}^{Superscript}, but for prime symbol ', use it as x_{Subscript}' Simply use {\\prime} for every situation, e.g. x_{i}^{\\prime}.","link":"/hexo-mathjax/"},{"title":"Using Hexo Scripts functions to create a custom Tag Plugin","text":"To color your personal Hexo Blog with more features, scripts and plugins are your powerful tools to use. Below we are trying to create our own tag plugin for the Hexo blog. Tag Plugin Be careful! Tag plugins are different from post tags. Tag plugins are special type of syntax that you may use in your Markdown file. Hexo has already provided some default Tag plugins like Block Quote and iframe. UsageFor example, the syntax of iframe tag is: {% iframe url [width] [height] %} Let say I want to embed a video from me and my friends’ YouTube video: {% iframe https://www.youtube.com/embed/XIOl6BU7s9I?si=yTYsHIXNM6o-Zl9Z 820 461%} And that’s how it looks like: ScriptLet say we want to create our own tag plugin, we can use the Hexo script function. Here’re the steps. Create a JavaScript file with function hexo.extend.tag.register(&quot;tag_name&quot;, args). You may also put your own function inside so that the second argument can also be function (args){} Here is an example of a function that create a tag named youtube, with embedding video function: hexo.extend.tag.register(&quot;youtube&quot;, function (args) { var id = args[0]; return ( '&lt;div class=&quot;video-container&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;http://www.youtube.com/embed/' + id + '&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;&lt;/div&gt;' ); }); Put your JavaScript files in the scripts folder. If your project folder is new, you may not be able to find it. This is because scripts folder is actually under the Themes folder. You may check here to see the structure of themes and create your own one. Or you may be simply find a template on the Hexo themes community then put your .js file into the theme. It is done! Hexo will load them during initialization and you may use them in your blog post designs. Beyond ScriptsIf you are not satisfied with creating a local tag plugin, but a public one that will be seen by the community, you should consider using Hexo plugin function instead. check here to continue the journey. Further Reading: Hexo Plugins and Scripts Hexo Tag Plugins Hexo Tag Api Hexo Plugins Community","link":"/hexo-tagplugin-1/"},{"title":"Using Hexo Plugin functions to create a custom Tag Plugin","text":"Disclaimer: For explaination on Tag Plugin and Scripts in Hexo, you may take a look of this post. Plugin Be Careful! Don’t mix Plugin, Tag, and Tag Plugin in Hexo, even though they look extremely similar. Usually, Plugin are for complicated functions. Yet if you want to publish your custom Tag Plugin to the NPM registry or even shown on the Hexo Community Page, Plugin would be a very good choice. From Script to PluginAssume you already have a script call index.js, and you want to turn it into package, you may do the following: Navigate node_modules folder in your project folder. This is the folder where Hexo stored all the packages for your blog. Create a folder inside and the name must begin with hexo- or Hexo will ignore it. Your folder must contain at least two files: the actual JavaScript code and package.json file that describes the purpose of the plugin and sets its dependencies. . ├── index.js └── package.json In package.json, it should at least have the name, version and main entries set. { &quot;name&quot;: &quot;hexo-my-plugin&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;main&quot;: &quot;index&quot; } In the root package.json of your hexo project, you also need to list your plugin as a dependency, for Hexo to detect and load it. Please remember that if your package contain other dependencies, also install and list them for testing and dubugging. { &quot;name&quot;: &quot;hexo-site&quot;, &quot;version&quot;: &quot;0.0.0&quot;, &quot;private&quot;: true, &quot;scripts&quot;: { &quot;build&quot;: &quot;hexo generate&quot;, &quot;clean&quot;: &quot;hexo clean&quot;, &quot;deploy&quot;: &quot;hexo deploy&quot;, &quot;server&quot;: &quot;hexo server&quot; }, &quot;hexo&quot;: { &quot;version&quot;: &quot;&quot; }, &quot;dependencies&quot;: { &quot;hexo&quot;: &quot;^7.3.0&quot;, ... &quot;hexo-my-plugin&quot;: &quot;0.0.1&quot;, &quot;my-plugin-dependency1&quot;: &quot;2.0.0&quot;, &quot;my-plugin-dependency2&quot;: &quot;2.0.0&quot; } } If you run command that check all the package after step 4, for exmaple hexo clean, it will check all the packages in node_modules and remove packages that are not publish on npm. Publish Plugin to npmTo publish your package on the NPM registry, don’t forget you have to setup your account on npm first. After creating the account, open your terminal and run npm login in the root of your package. Enter your username and password, then you should see a message like this if login is successful, Logged in as &lt;your-username&gt; on https://registry.npmjs.org/. Once you logged-in, you may simply publish your folder with npm publish command. Publish Plugin to HexoAfter publish your plugin package in npm, you can also publish it to Hexo official. Fork and CloneFirst of all, Fork hexojs/site from Github Then Clone the repository to your computer and install dependencies. $ git clone https://github.com/&lt;username&gt;/site.git $ cd site $ npm install Add your Plugin to the listCreate a new yaml file in source/_data/plugins/, use your plugin name as the file name Edit source/_data/plugins/&lt;your-plugin-name&gt;.yml and add your plugin. For example: Push the branchCreate a pull request and describe the change. Hexo official create a nice form to make sure you have included everything needed. ResultsYou may check the progress on the pull requests history.Once it is closed you can see your plugin on the Hexo Plugins Community. Example workAs you may see, I also made my Plugin “Hexo-zhruby” for Hexo and you can now see it on the community.For more details and see how it works, you may check here. Further Reading: Hexo Plugins and Scripts Hexo Tag Plugins Hexo Plugins Community npm-publish","link":"/hexo-tagplugin-2/"},{"title":"hexo-zhruby -- Implementing HTML Ruby tag in Hexo","text":"Implement the HTML tag &lt;ruby&gt; for Hexo using Tag Plugin feature. Provide auto pronounciation indication for Jyutping (Cantonese), Zhuyin (Taiwanese Mandarin), and Pinyin (Chinese Mandarin), and the default setting for general usage. Support Traditonal and Simplified Chinese characters. Inspired by the hexo-ruby-character by jamespan. Installnpm install hexo-zhruby --save Use casesRuby (ルビ) is also known as Furigana (振り仮名). It contains two basic use cases: To clarify or indicate the pronunciation for readers Gikun, in which the characters have different pronunciations than they seem due to convention or for a specific context. For example, the pronunciation of 煙草 in Japanese is tabako (tobacco). UsageTLDR: Usage: {% tag rb|rt %}; Tag options: ruby_def, ruby_jy, ruby_py, ruby_zy. For the 1st use case (pronunciation indication):ruby_def allows any language, and the spacing in rp will expand evenly with respect to the word length in rt. {% ruby_def 基本|きほん %} → 基本 (きほん) {% ruby_def 基本|기본 %} → 基本 (기본) {% ruby_def 基本|fundamental %} → 基本 (fundamental) {% ruby_def 基本|θεμελιώδες %} → 基本 (θεμελιώδες) {% ruby_def 基本|базовый %} → 基本 (базовый) {% ruby_def 基本|základní %} → 基本 (základní) {% ruby_def fundamental|基本 %} → fundamental (基本) ruby_jy, ruby_py, ruby_zy refers to Jyutping, Pinyin, Zhuyin respectively. No need to enter the pronunciation manually in rt; the value will automatically be returned. {% ruby_zy 基本 %} → 基本 (ㄐㄧ ㄅㄣˇ) {% ruby_py 基本 %} → 基本 (jī běn) {% ruby_jy 基本 %} → 基本 (gei1 bun2) For the 2nd use case (Gikun):Same usage for ruby_def. {% ruby_def special|basic %} → special (basic) {% ruby_def 特別|基本 %} → 特別 (基本) In ruby_jy, ruby_py, ruby_zy, you can also add |rt just like ruby_def. {% ruby_zy 特別|special %} → 特別 (special) {% ruby_py 特別|special %} → 特別 (special) {% ruby_jy 特別|special %} → 特別 (special) {% ruby_zy 特別|基本 %} → 特別 (ㄐㄧ ㄅㄣˇ) {% ruby_py 特別|基本 %} → 特別 (jī běn) {% ruby_jy 特別|基本 %} → 特別 (gei1 bun2) Notice that the rt output depends on the pronunciation in rt, but not rp. This feature is only available when rt is a Chinese Character in CJK Unified Ideographs (\\U4E00-\\U9FFF). If the input of rt is not in CJK Unified Ideographs, it is considered as ruby_def. Known issuesThe Chinese language contains a lot of Homophones, which can be resolved by context most of the time. However, for long sentences (&gt;= 15 characters), or very specific names and terms, the auto-generation from 1st use case may not be very sensitive. Please use ruby_def if it happens or contribute to this project by providing a more sensitive or advanced Chinese vocabulary library. References CJK Unified Ideographs (Unicode block) Homophone Gikun (japanese only) Gikun (English version under the article Kanji) &lt;ruby&gt;: The Ruby Annotation element Unicode/Character reference Universal Character Set characters (Unicode)","link":"/hexo-zhruby/"},{"title":"Railroad Diagram of version range operators","text":"This is a image version of the post here range-set: range-set ::= range ( logical-or range )* logical-or: logical-or ::= ' '* '||' ' '* referenced by: range-set range: range ::= hyphen | simple ( ' ' simple )* | '' referenced by: range-set hyphen: hyphen ::= partial ' - ' partial referenced by: range simple: simple ::= primitive | partial | tilde | caret referenced by: range primitive: primitive ::= ( '&lt;' | '&gt;' | '&gt;=' | '&lt;=' | '=' ) partial referenced by: simple partial: partial ::= xr ( '.' xr ( '.' xr qualifier? )? )? referenced by: caret hyphen primitive simple tilde xr: xr ::= 'x' | 'X' | '*' | nr referenced by: partial nr: nr ::= '0' | ['1'-'9] ['0'-'9]* referenced by: part xr tilde: tilde ::= '~' partial referenced by: simple caret: caret ::= '^' partial referenced by: simple qualifier: qualifier ::= ( '-' pre )? ( '+' build )? referenced by: partial pre: pre ::= parts referenced by: qualifier build: build ::= parts referenced by: qualifier parts: parts ::= part ( '.' part )* referenced by: build pre part: part ::= nr | [-0-9A-Za-z]+ generated by RR - Railroad Diagram Generator","link":"/json-dependencies-diagram/"},{"title":"Explaination of version range operators in package.json for package dependencies","text":"To see the syntax and Railroad diagram version, goto here Basic StructurePackage dependecies are a tuple of [major, minor, patch] with numeric values. { &quot;name&quot;: &quot;project name&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;description&quot;: &quot;description of the project&quot;, &quot;keywords&quot;: [ &quot;keyword 1&quot;, &quot;keyword 2&quot; ], &quot;author&quot;: &quot;John Doe&quot;, &quot;dependencies&quot;: { &quot;package-1&quot;: &quot;~0.6.2&quot;, &quot;package-2&quot;: &quot;&gt;=2.6.2&quot; } } Version Range operatorBasic RangeFor x in exmaple, see Advanced Range below. Operator Explaination Example = package version must be exactly matched 1.0.0 := =1.0.0(They are equivalent) &lt; package version must be less than indicated &lt;2.0.0 :=version from 0.0.1 to 1.x.x &lt;= package version must be less than or euqal to indicated &lt;=2.0.0 :=version from 0.0.1 to 2.0.0 &gt; package version must be greater than indicated &gt;2.0.0:= version from 2.0.1 to x (x &gt;= 2) &gt;= package version must be greater than or euqal to indicated &gt;=2.0.0 :=version from 2.0.0 to x (x &gt;= 2) || joined one or more operator &gt;2.0.1 || &lt;1.7.3 :=version greater than 2.0.1 or less than 1.7.3 space Intersected one or more operator &gt;=2.0.1 &lt;=1.7.3 :=version from 2.0.1 to 1.7.3 (inclusive) Advanced RangeAdvanced ranges may be combined in the same way as primitive comparators using space or ||. Operator Explaination Example X, x, * A Wildcard may be used for any values in the [major, minor, patch] tuple (missing value are consider using Wildcard x) * := (empty string) := &gt;=0.0.01.x.x := 1.x := 1 := &gt;=1.0.0 &lt;2.0.0 - Specifies an inclusive set of package version 1.2 - 2.3.4 := &gt;=1.2.0 &lt;=2.3.4 (missing pieces of first version are replaced with zeroes)1.2.3 - 2.3 := &gt;=1.2.3 &lt;2.4.x(missing pieces of second version replace with X-range) ~ Allows patch or minor level version changes, depends on specification ~1.2.3 := any version starts with 1.2 and greater than 1.2.3~1.2 := any version starts with 1.2 (same as 1.2.x) ^ Allows version changes in the [major, minor, patch] tuple without modify the left-most non-zero element. ^1.2.3 := &gt;=1.2.3 &lt;2.0.0(minor update)^0.2.3 := &gt;=0.2.3 &lt;0.3.0 (patch update)^0.0.3 := &gt;=0.0.3 &lt;0.0.4(no updates) Further Explaination on Caret Ranges ^Special interaction with Wildcard operator xWhen parsing caret ranges, minor and patch values with wildcard x desugars to the number 0 (missing values are consider as x): Example: ^1.2.x := &gt;=1.2.0 &lt;2.0.0-0 (equivalent to ^1.2.0) ^1.x := &gt;=1.0.0 &lt;2.0.0-0 (equivalent to ^1.0.0) However, when both major and minor versions are 0, Caret range allow flexibility within wildcard x: Example: ^0.0.x := &gt;=0.0.0 &lt;0.1.0-0 (NOT equivalent to ^0.0.0, but similar to ^0.1.0) Usage and Common PracticesCaret ranges usually ideally used when an author may make breaking changes. For example, between 0.2.4 and 0.3.0 releases, which is a common practice. However, it presumes that there will not be breaking changes between 0.2.4 and 0.2.5. It allows for changes that are presumed to be additive (but non-breaking), according to commonly observed practices. Further Reading: version range in npm-semver, setup-node in GitHub Actions","link":"/json-dependencies/"},{"title":"Parameterized Verification 1.1 -- Labeled Transition Systems","text":"This is a learning note of a course in CISPA, UdS. Taught by Swen Jacobs Labeled Transition Systems (LTS)LTS is a concept in theoretical computer science used in the study of computation. It is used to describe the potential behavior of discrete systems, e.g. Model Checking. It consists of states and transitions between states, which may be labeled with labels chosen from a set; the same label may appear on more than one transition. Mathematically, it can be described as directed graph. LabelsLabels can be used to describe the behaviour or distinguish between states.If the set of label is a singleton (only contains one label), then it can be omitted in the system. LTS v.s. Finite-state AutomataIn Transition systems: The set of states is not necessarily finite, or even countable. The set of transitions is not necessarily finite, or even countable. No “start” state or “final” states are given. Formal definitionLet $AP$ be a set of atomic propositions (statement or assertion that must be true or false),i.e., observable properties of the system. A labeled transition system (LTS) over $AP$ is a tuple $Q, Q_0, \\Sigma, \\delta, \\lambda$, where $Q$ is a set of states $Q_0 \\subseteq Q$ is the set of initial states $\\Sigma$ is the set of transition labels (or actions) $\\delta \\subseteq Q \\times \\Sigma \\times Q$ is the transition relation (also write as $ q\\buildrel a \\over\\rightarrow q’ \\in \\delta$) $\\lambda : Q \\rightarrow 2^{AP}$ is the state labeling. In this course, we assume finite LTS, which means $AP$ and $Q$ are finite, unless explicitly stated. Run of an LTSConsider an LTS $M = (Q, Q_0, \\Sigma, \\delta, \\lambda)$. PathA path of $M$ an is a finite sequence $q_0a_0q_1a_1…q_n \\in (Q\\Sigma)\\ast Q$, such that $ q_i \\buildrel a_i \\over\\rightarrow q_{i+1}$ for all $i &lt; n$or an infinite sequence $q_0a_0q_1a_1…q_n \\in (Q\\Sigma)^\\omega$, such that $ q_i \\buildrel a_i \\over\\rightarrow q_{i+1}$ for all $i \\in \\mathbb{N}$. RunA run of $M$ is a path of $M$ with $q_0 \\in Q_0$, and that is maximal, i.e., it cannot be extended. A run is deadlocked if it is finite. State-labeled paths$q_0q_1…$, the projection of a path onto states $Q$, Action-labeled paths$a_0a_1…$, the projection of a path onto actions $\\Sigma$, Traces$\\lambda (q_0)\\lambda q_1…$, the sequence of labels of a state-labeled path. Next post: Synchronization Primitives of Processes Further Reading: Transition System, Model Checking.","link":"/pv1-1/"},{"title":"AGV 1.1 -- Model Checking","text":"This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Model CheckingGiven a system model, we try to check whether it meets its specification automatically and exhaustively.To describe/represent the system and specification, we use Automata over infinite objects. Verification ProblemThe verification problem can be solved by: Constructing the intersection of system, with an automaton for the negation of the specification,then checking whether the language of the resulting automaton is empty. $\\textbf{Example 1.1. } \\text{Consider the concurrent program }\\small{\\text{TURN}}:$$$\\text{local $t$: boolean where initially $t$ = $false$}\\newlineP_0::\\left[ \\begin{array}{l}\\text{loop forever do}\\newline\\hspace{1cm}\\left[ \\begin{array}{l}\\ell_0: \\text{await }\\neg t; \\newline\\ell_1: \\text{critical;} \\newline\\ell_2: t := true; \\newline\\end{array} \\right]\\end{array} \\right]\\mid\\mid P_1::\\left[ \\begin{array}{l}\\text{loop forever do}\\newline\\hspace{1cm}\\left[ \\begin{array}{l}m_0: \\text{await } t; \\newlinem_1: \\text{critical;} \\newlinem_2: t := false; \\newline\\end{array} \\right]\\end{array} \\right]$$ The example above is a simple solution to the mutual exclusion problem:it ensures that at any given point of time, at most one process is in the critical region. From Program to automatonTo represent the above program using automaton, we need to define an alphabet $\\Sigma$. Firstly, we fix a set of atomic propositions (AP), that is (potentially) relevant to the program states.For the program in $\\small{\\text{Example 1.1}}$, $AP = \\lbrace\\ell_0,\\ell_1,\\ell_2,m_0,m_1,m_2,t\\rbrace$.i.e. The present program location and the current value of $t$. Then, we define the alphabet as $\\Sigma = 2^{AP}$, the set of subsets of $AP$.An $AP$ is included in the subset if it is currently $true$. This automaton is a safety automaton, it accepts all infinite repetitons of the sequence$\\lbrace\\ell_0,m_0\\rbrace \\lbrace\\ell_1,m_0\\rbrace \\lbrace\\ell_2,m_0\\rbrace \\lbrace\\ell_0,m_0,t\\rbrace \\lbrace\\ell_0,m_2,t\\rbrace$ Automata VerificationTo verify $\\small{\\text{TURN}}$ satisfies the mutual exclusion property ($P_0$ and $P_1$ are never in $\\ell_1$ and $m_1$ at the same time),we build an automaton that represents the negation (eventually $P_0$ and $P_1$ are simultaneously in location 01): This automaton is a Büchi automaton with accepting state $t_1$. A word is accepted if $t_1$ is visited infinitely often. For every word accepted by the system automaton, this property automaton stays in $t_0$ forever and therefore does not accept the word.This means there does not exist an infinite sequence that generated by the program, andaccepted by the negation automaton of the specification. In other words, $\\small{\\text{TURN}}$ is correct. Next post: Synthesis Further Reading: Büchi automaton","link":"/AGV/agv1-1/"},{"title":"Parameterized Verification 1.2 -- Synchronization Primitives of Processes","text":"This is a learning note of a course in CISPA, UdS. Taught by Swen Jacobs Last post: Labeled Transition Systems ProcessesA Process is an LTS $P = (Q, Q_0, \\Sigma, \\delta, \\lambda)$ with $\\Sigma = \\Sigma_{int} \\cup \\lbrace out_a , in_a \\mid a \\in \\Sigma_{sync} \\rbrace $, where $\\Sigma_{int}$ is a set of internal actions, $\\Sigma_{sync}$ a set of synchronizing actions, $out_a$ is an send action (or initiate action), $in_a$ is an receive action. Composition by SynchronizationThe composition $P^n$ of $n$ (uniform) processes wrt. $card$ is the LTS $(S, S_0, \\Sigma_{int} \\cup \\Sigma_{sync}, \\Delta, \\Lambda)$ with: $S = Q \\times Q$ $S_0 = Q_0 \\times Q_0$ $\\Delta \\subseteq S \\times (\\Delta_{int} \\cup \\Delta_{sync} \\times S)$ is the set of all transitions that satisfy one of the following: Internal Transition: For some $i \\in\\lbrace 1, … , n \\rbrace$ , $(s, a, s’) \\in S \\times\\Sigma_{int}\\times S$ is an element such that: $(s(i), a, s’(i)) \\in\\delta$ , and $s(j) = s’(j)$ for $i \\ne j \\in\\lbrace 1, …, n \\rbrace$ Process $s(i)$ take the action, other processes $s(j)$ remain their current states. Synchronous Transition For some $i \\in \\lbrace 1, … , n \\rbrace$ and some $I \\subseteq \\lbrace 1, …, n \\rbrace \\setminus \\lbrace i \\rbrace$ with $|I| \\in card$ ,$(s, a, s’) \\in S \\times \\Sigma_{int} \\times S$ is an element such that: $s(i) \\buildrel out_a \\over\\longrightarrow s’(i)$ is (a local transition) in $P$ One process $s(i)$ take the send action. for every $j \\in I, s(j) \\buildrel in_a \\over\\longrightarrow s’(j)$ is (a local transition) in $P$ $I$ is the set of processes that can take the receive action. (size of $I$ must not be larger then $card$) for every $j \\in\\lbrace 1, …, n \\rbrace \\setminus (I \\cup\\lbrace i \\rbrace), s’(j) = s(j)$ Other processes $s(j)$ that cannot take receive actions remain their current states. $I$ is maximal. There does not exist a larger set $I’ \\supset I$ with $|I’| \\in card$ that for all $j \\in I’$ ,there is a local transition from $s(j)$ that can take the receive action. $\\Lambda(s) = \\lbrace p_{i} \\mid p \\in \\text{AP and p} \\in\\lambda(s(i)), i \\in \\lbrace 1, …, n\\rbrace \\rbrace$ In this course, send action = $out_a$ = $a!!$ ; receive action = $in_a$ = $a??$. Example of Composition by Broadcast synchronization. $(card = \\lbrace 1 \\rbrace)$ Synchronization PrimitivesIn this course, there are 4 types of synchronization:Pairwise Rendezvous, Broadcast, Asynchronous Rendezvous, and Lossy Broadcast. Pairwise Rendezvous Exactly ONE process take $out_a$ action, ONE process take $in_a$ action. $(card = \\lbrace 1 \\rbrace)$ Broadcast ONE process take $out_a$ action, ALL process take $in_a$ action if they are able to $(card = \\mathbb{N}_0)$. Asynchronous Rendezvous ONE process take $out_a$ action, ZERO / ONE process take $in_a$ action $(card = {0, 1})$. Lossy Broadcast ONE process take $out_a$ action, ONE process take $in_a$ action $(card = \\mathbb{N}_0, I\\text{ not necessarily maximal})$. Next post: Further Reading: Synchronization, Parameterized Verification by Javier Esparza.","link":"/pv1-2/"},{"title":"Automata, Games, and Verification (Portal)","text":"This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Chapter 1. Introduction Sections Exercise 1.1. Model Checking 1.2. Synthesis 1.3. The Logic-Automata Connection Chapter 2. Büchi Automata Sections Exercise 2.1. Preliminaries 2.2. Automata over Infinite Words 2.3. The Büchi Acceptance Condition Chapter 3. Büchi’s Characterization Theorem Sections Exercise 3.1. Kleene’s Theorem 3.2. $\\omega$-regular language 3.3 Closure Properties of the Büchi-recognizable languages (Intersection and Union) 3.4 Closure Properties of the Büchi-recognizable languages (Concatenations) 3.5 Büchi’s Characterization Theorem Chapter 4. Deterministic Büchi Automata Sections Exercise 4.1. Deterministic vs. Nondeterministic Büchi Automata 4.2. Complementation of deterministic Büchi Automata Chapter 5. Complementation of Büchi automata Sections Exercise 5.1. Infinite Directed Acyclic Graph (DAG) 5.2. Ranking of DAG 5.3. Complement Büchi Automaton with Odd Ranking Chapter 6. Logics over Infinite Sequences Sections Exercise 6.1. Linear-Time Temporal Logic (LTL) 6.2. Expressing Program Properties using LTL 6.3. LTL and Counting Languages 6.4. Quantified Propositional Temporal Logic (QPTL) 6.5. Monadic Second-Order Logic of One Successor (S1S) 6.6. Express QPTL using S1S 6.7. S1S$_0$ and Büchi-recognizable LanguageBüchi-recognizable Chapter 7. Alternating Büchi Automata Sections Exercise 7.1. Alternating Büchi Automata 7.2. From LTL to Alternating Büchi Automata $\\varphi=(\\Diamond p)\\ \\mathcal{U}\\ (\\square q)$ 7.3. Translating Alternating to Nondeterministic automata Chapter 8. Linear Arithmetic Sections Exercise 8.1. Linear Arithmetic (Theory) 8.2 Encoding real numbers 8.3 Translation from Linear Arithmetic to Automata 8.4 Homogenous Inequality Testing is Automatic 8.5 From Linear Arithmetic to Automata Chapter 9. LTL Model Checking Sections Exercise 9.1 Automata-based LTL Model Checking with Sequential Circuits 9.2 Nested depth-first search 9.3 The Emerson-Lei algorithm Chapter 10. McNaughton’s Theorem Sections Exercise 10.1 The Muller Acceptance Condition 10.2 From Büchi automata to Muller automata 10.3 Closure Properties of Muller Automata under Boolean Operations Next chapter: Further Reading:","link":"/AGV/agv/"},{"title":"AGV 1.2 -- Synthesis","text":"Previous chapter: Model Checking This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner SynthesisIn synthesis, we check automatically if there exists a program that satisfies a given specification. If the answer is yes, we construct such a program. We solve it by determine the winner of a two-player game between a system player and an environment player. Task Goal System player choose the outputs of the system meet the specification Environment player choose the inputs of the system falsify the specification Therefore, the specification is satisfied if the System player wins.Such winning strategy can be translated into a program that is guaranteed to satisfy the specification. Example: coffee machineIn this example, we assume there’s a machine that “outputs coffee” whenever users press a button. Set of $AP = \\lbrace bu, co\\rbrace$, where input $AP_I = \\lbrace bu\\rbrace$ and output $AP_O = \\lbrace co\\rbrace$. Specification: $co$ should hold iff. $bu$ holds in every step. From this automaton, we construct a game arena where in each round, Environment player chooses the input ($bu$ or $\\neg bu$) System player chooses the output ($co$ or $\\neg co$) States of the game keep track of the corresponding state of the automaton,then we can determine who is the winner for each possible play. Here, system player gets to choose the next move with circles, and environment player choose with squares. System player has a simple winning strategy from starting position $t_0$: react input $bu$ with output $co$, and react input $\\neg bu$ with output $\\neg co$ In this way, the play visits $t_0$ infinitely often, which proves that it satisfies the specification. Next chapter: The Logic-Automata Connection Further Reading: Program synthesis","link":"/AGV/agv1-2/"},{"title":"AGV 1.3 -- The Logic-Automata Connection","text":"Previous chapter: Synthesis This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Verification or SynthesisIn applications like verification and synthesis, the automata- and game-theoretic machinery is usually “hidden” behind a logical formulation of the problem. Logics with corresponding Automata S1S‘s expressiveness exceeds that of LTL, andS2S‘s expressiveness exceeds that of CTL* (on binary trees) Logic Usage Example Linear-time temporal logic (LTL) sets of infinite words $\\square \\Diamond \\ell_1$ Computation-tree logic (CTL / CTL*) sets of infinite trees $\\textsf{EF}\\ell_1 \\wedge \\textsf{EF}m_1$ Monadic second-order logic with one successor (S1S) logic over infinite words $\\forall x . x \\in P \\rightarrow S(x) \\in P$ Monadic second-order logic with two successors (S2S) logic over infinite binary trees $\\forall x . x \\in P \\rightarrow S_1(x) \\in P \\vee S_2(x) \\in P$ Explaination $\\square \\Diamond \\ell_1$$P_0$ is infinitely often at location $\\ell_1$. $\\textsf{EF}\\ell_1 \\wedge \\textsf{EF}m_1$There exists a computation path in which $P_0$ reaches location $\\ell_1$, andthere is a (possibly different) computation path in which $P_1$ reaches location $m_1$. $\\forall x . x \\in P \\rightarrow S(x) \\in P$A (given) set of natural numbers $P$ is either empty, orconsists of all positions starting from some position of the word. $\\forall x . x \\in P \\rightarrow S_1(x) \\in P \\vee S_2(x) \\in P$A (given) set of nodes $P$ contains from each node $n \\in P$ an entire path starting in $n$. For example, mutual exclusion property can be expressed using linear-time temporal logic (LTL):$$\\square \\neg (\\ell_1 \\wedge m_1)$$ Satisfiability problemOther than verification and synthesis, another important application of the connection between logic and automata is to decide the satisfiability problem of the various logics. ExampleWe want to know if there exist two natural numbers $x$ and $y$ such that $x=y+1$ and $y=x+1$.This can be expressed as the S1S formula $x=S(y) \\wedge y=S(x)$, translate each conjunct into an automaton: Alphabet: subsets of $\\lbrace x,y\\rbrace$. There does not exist a word that is accepted by both automaton. Hence, the formula is unsatisfiable. Next chapter: Büchi automata (Preliminaries)","link":"/AGV/agv1-3/"},{"title":"AGV 10.1 -- The Muller Acceptance Condition","text":"Previous chapter: The Emerson-Lei algorithm This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntrodcutionWe already established that while the languages that can be recognized with nondeterministic Büchi automata are exactly the $\\omega$-regular languages, the languages that can be recognized with deterministic Büchi automata are a strictly smaller set. We now repair this deficiency with a more expressive acceptance condition, the Muller condition. McNaughton’s theorem states that the set of languages recognizable by deterministic Muller automata are again exactly the $\\omega$-regular languages. We will see later that it is very useful to have a deterministic automaton for a given $\\omega$-language, for example in synthesis, where we construct the game between the system and the environment from a deterministic automaton that recognizes the winning plays for the system player. Since the complementation of deterministic Muller automata is a very simple operation, McNaughton’s theorem also provides an alternative proof for the result of Section 5 that the $\\omega$-regular languages are closed under complementation. Muller Acceptance Condition $\\textbf{Definition 10.1. } \\text{The }\\textit{Muller Acceptance Condition }\\small\\text{MULLER} \\normalsize(\\mathcal{F})\\text{ on a set of sets of states }\\newline\\mathcal{F}\\subseteq 2^Q\\text{ is the set}$ $$\\small\\text{MULLER}\\normalsize(\\mathcal{F})=\\lbrace\\alpha\\in Q^\\omega\\mid\\text{Inf}(\\alpha)\\in\\mathcal{F}\\rbrace$$ $\\text{An automaton }\\mathcal{A} = (\\Sigma,Q,I,T,Acc) \\text{ with }Acc = \\small\\text{MULLER} \\normalsize(\\mathcal{F})\\text{ is called a }\\textit{Muller Automaton.}\\newline\\text{The set }\\mathcal{F}\\text{ is called the set of }\\textit{accepting subsets }(\\text{or the }\\textit{table})\\text{ of }\\mathcal{A}.$ Let’s do a small recap from section 2.3: Büchi Condition $\\small\\text{BÜCHI} \\normalsize(F) = \\lbrace\\alpha\\in Q^\\omega \\mid \\text{Inf}(\\alpha) \\cap F \\neq \\varnothing\\rbrace$ word $\\alpha$ visit some state in set $F$ infinitely often. Muller Acceptance Condition $\\small\\text{MULLER}\\normalsize(\\mathcal{F})$ word $\\alpha$ visit some set of states in set $\\mathcal{F}$ infinitely often. We can see Muller Acceptance Condition are more expressive in terms of visiting accepting states, because you can require the word to visit a set of states infinitely often instead just one state out of the whole set. ExampleConsider the deterministic automaton over the alphabet $\\Sigma=\\lbrace a, b\\rbrace$ shown below. For the table $\\mathcal{F}=\\lbrace\\lbrace q\\rbrace\\rbrace$, it means the automaton can only visit $\\lbrace q\\rbrace$ infinitely often. We obtain the Muller automaton $\\mathcal{A}$ recognizing the language $\\mathcal{L(A)}=(a+b)^\\ast b^\\omega$; For the table $\\mathcal{F}’=\\lbrace\\lbrace q\\rbrace\\lbrace p,q\\rbrace\\rbrace$, it means the automaton has to visit either $\\lbrace q\\rbrace$ or $\\lbrace p,q\\rbrace$ infinitely often. We obtain the Muller automaton $\\mathcal{A’}$ recognizing $\\mathcal{L(A’)}=(a^\\ast b)^\\omega$. SummaryWe introduced Muller Acceptance Condition, which is slightly more advanced than the Büchi Acceptance Condition: Aspect Büchi Acceptance Muller Acceptance Acceptance states At least one accepting state must appear infinitely often. The set of states that visited infinitely often must match a subset in $\\mathcal{F}$ Condition Set $F$ is the set of accepting states $\\mathcal{F}$ is the set of the set of accepting states (subset of the set) Expressiveness Less expressive (example) Fully expressive Complexity Simpler and easier to implement. More complex, requires tracking recurring state sets. In the following sections, we will discuss about the translation from Büchi automata to Muller automata. Next chapter: From Büchi automata to Muller automata Further Reading:","link":"/AGV/agv10-1/"},{"title":"AGV 10.3 -- Closure Properties of Muller automata Under Boolean Operations","text":"Previous chapter: From Büchi automata to Muller automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionWe now show that deterministic Muller automata are closed, like nondeterministic Büchi automata, under the Boolean operations (complementation, union, and intersection). First we introduce the construction of these automaton with operations, then we will prove they are close by the runs under those constructions. Automata construction of Complementation $\\textbf{Construction 10.3. } \\text{Let }\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{MULLER} \\normalsize (\\mathcal{F}))\\text{ be a complete and deterministic Muller}\\newline\\text{automaton, where we assume w.l.o.g that }Q\\neq\\varnothing.\\text{ We construct the deterministic Muller}\\newline\\text{automaton }\\mathcal{A}^C = (\\Sigma,Q,I,T,\\small\\text{MULLER} \\normalsize(2^Q\\setminus\\mathcal{F}))\\text{ with }\\mathcal{L(A^C)}=\\Sigma\\setminus\\mathcal{L(A)}.$ Automata construction of IntersectionWe use the function $pr_n$ for $n\\in\\mathbb{N}$ to project to the (n+1)th component of a arbitrary length tuple, for example: $pr_0(x, y)=x$ $pr_1(x, y)=y$ $pr_2(x, y),pr_3(x, y),\\dots pr_n(x, y) =\\text{Undefined}$ We can also apply the projection to a set and return a set of components: $pr_n(S) = \\bigcup_{s∈S}{pr_n(s)}.$ $\\textbf{Construction 10.4. } \\text{For Muller automata }\\mathcal{A_1} = (\\Sigma,Q,I,T,\\small\\text{MULLER} \\normalsize (\\mathcal{F_1}))\\text{ and}\\newline\\mathcal{A_2} = (\\Sigma,Q,I,T,\\small\\text{MULLER} \\normalsize (\\mathcal{F_2}))\\text{ over the same alphabet }\\Sigma.\\text{ We construct the Muller Automaton}\\newline\\mathcal{A}_\\cap = (\\Sigma,Q_1\\times Q_2,I_1\\times I_2,T_\\cap,\\small\\text{MULLER} \\normalsize(\\mathcal{F_\\cap}))\\text{ with }\\mathcal{L(A_\\cap)}=\\mathcal{L(A_1)}\\cap\\mathcal{L(A_2)}\\text{ and where }\\mathcal{A_\\cap}\\text{ is}\\newline\\text{deterministic if }\\mathcal{A_1}\\text{ and }\\mathcal{A_2}\\text{ are deterministic, as follows:}$ $\\begin{array}{l}\\hspace{1cm}\\cdot \\ T_\\cap=\\lbrace((q_1,q_2),\\sigma,(q’_1,q’_2))\\mid(q_1,\\sigma,q’_1)\\in T_1,(q_2,\\sigma,q’_2)\\in T_2\\rbrace\\newline\\hspace{1cm}\\cdot \\ \\mathcal{F}_\\cap = \\lbrace P\\subseteq Q_1\\times Q_2\\mid pr_0(P)\\in\\mathcal{F_1},pr_1(P)\\in\\mathcal{F_2}\\rbrace\\end{array}$ Closure Properties of under Boolean Operations $\\textbf{Theorem 10.3. } \\textit{The languages recognizable by deterministic Muller automata are closed}\\newline\\textit{under Boolean operations (complementation, union, intersection).}.$ Proof of Deterministic Muller automata are closed under complementationFor a deterministic Muller automaton $\\mathcal{A}$, the automaton $\\mathcal{A’}$ of Construction 10.3 recognizes the complement language, because any set $F\\notin F$ has to be in the complement, i.e., $F\\in2^Q\\setminus F$. Proof of Deterministic Muller automata are closed under IntersectionFor deterministic Muller automata $\\mathcal{A_1}$ and $\\mathcal{A_2}$, the automaton $\\mathcal{A}_\\cap$ of Construction 10.4 recognizes the intersection. Let $r_1 = q^1_0q^1_1\\dots$ and $r_2 = q^2_0q^2_1\\dots$ be accepting runs of $\\mathcal{A_1}$ and $\\mathcal{A_2}$ on some $\\alpha$. Then $r=(r^1_0,r^2_0)(r^1_1,r^2_1)\\dots$ is an accepting run of $\\mathcal{A}_\\cap$ on $\\alpha$ and vice versa. Proof of Deterministic Muller automata are closed under UnionIt can be proved by De Morgan’s laws if they are closed under complement and intersection: $$\\Sigma\\setminus(\\mathcal{L(A_1)}\\cap\\mathcal{L(A_2)})=(\\Sigma\\setminus\\mathcal{L(A_1)})\\cup(\\Sigma\\setminus\\mathcal{L(A_2)})$$ Regular language and Limit operatorSimilar to Büchi automata in section 4.1, we can define an $\\omega$-regular language from regular language, which is recognizable by deterministic Muller Automata: $\\textbf{Theorem 10.4. }\\textit{An language }L\\textit{ is recognizable by a deterministic Muller Automata if and only}\\newline\\textit{if }L\\textit{ is a Boolean combination of langauges }\\overrightarrow{W}\\textit{ where }W\\subseteq\\Sigma^*\\text{ is regular.}$ Proof$”\\Leftarrow”$If $W$ is regular, then $\\overrightarrow{W}$ is recognizable by a deterministic Büchi automaton. Hence, $\\overrightarrow{W}$ is recognizable by a deterministic Muller automaton. Thus, the boolean combination $\\mathcal{L}$ is recognizable by a deterministic Muller automaton. $”\\Rightarrow”$A deterministic Muller automaton $\\mathcal{A}$ accepts some word $\\alpha$ with a unique run $r$ if for some $F\\in\\mathcal{F}$ we have that $\\text{Inf}(r)=F$. Thus, there is some $F\\in\\mathcal{F}$ such that for all $q\\in F$ we have that $\\alpha\\in\\overrightarrow{W_q}$ and for all $q\\notin F$ we have that $\\alpha\\notin\\overrightarrow{W_q}$, where $\\overrightarrow{W_q}=\\mathcal{L(A_q)}$ for the finite-word automaton $\\mathcal{A}_q=(\\Sigma,Q,I,T,\\lbrace q\\rbrace)$. Hence, $$\\alpha\\in\\underset{F\\in\\mathcal{F}}{\\bigcup}\\left(\\underset{q\\in F}{\\bigcap}\\overrightarrow{W_q}\\cap\\underset{q\\notin F}{\\bigcap}(\\Sigma^\\omega\\setminus\\overrightarrow{W_q})\\right)$$ Next chapter: Semi-Deterministic Büchi Automata Further Reading: De Morgan’s laws","link":"/AGV/agv10-3/"},{"title":"AGV 10.2 -- From Büchi automata to Muller automata","text":"Previous chapter: The Muller Acceptance Condition This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner In this section, we want to prove that Muller automata is equivalent to Büchi Automata. Translate Büchi Automata into Muller Automata $\\textbf{Construction 10.1. } \\text{For a (deterministic) Büchi Automaton }\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{BÜCHI}\\normalsize (F))\\newline\\text{ we define the (deterministic) Muller automaton } \\mathcal{A’} = (\\Sigma,Q,I,T,\\small\\text{MULLER} \\normalsize (\\mathcal{F}))\\text{ using}$ $$\\mathcal{F}=\\lbrace S\\subseteq Q\\mid S\\cap F\\neq\\varnothing\\rbrace$$ Since the construction does not modify the transitions, the Muller automaton is again deterministic if the Büchi automaton is deterministic. It is straightforward to see that the automata recognize the same language. $\\textbf{Theorem 10.1. } \\textit{For every (deterministic) Büchi automaton }\\mathcal{A}\\textit{, there is a (deterministic)}\\newline\\textit{Muller automaton }\\mathcal{A’}\\textit{ such that }\\mathcal{L(A)}=\\mathcal{L(A’)}.$ ProofThe automaton $\\mathcal{A’}$ of Construction 10.1 complies with our requirements, according to previous section: $$\\small\\text{BÜCHI}\\normalsize (F)=\\lbrace\\alpha\\in Q^\\omega\\mid\\text{Inf}(\\alpha)\\cap F\\neq\\varnothing\\rbrace=\\lbrace\\alpha\\in Q^\\omega\\mid\\text{Inf}(\\alpha)\\in\\mathcal{F}\\rbrace=\\small\\text{MULLER}\\normalsize(\\mathcal{F})$$ Translate Muller Automata into Büchi AutomataA slightly more difficult construction is to translate the Muller automaton back into a Büchi automaton. $\\textbf{Construction 10.2. } \\text{Let }\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{MULLER} \\normalsize (\\lbrace F_1,\\dots,F_n\\rbrace))\\text{ be a Muller automaton}\\newline\\text{and }&lt;\\text{ some arbitrary total order on }Q.\\text{ We construct the Büchi automaton }\\mathcal{A’} = (\\Sigma,Q’,\\newline I’,T’,\\small\\text{BÜCHI} \\normalsize(F’))\\text{ with }\\mathcal{A’}\\textit{ such that }\\mathcal{L(A)}=\\mathcal{L(A’)}\\text{ as follows:}$ $\\begin{array}{llll}\\hspace{1cm} \\cdot \\ Q’&amp;=Q\\cup\\overset{n}{\\underset{i=1}{\\bigcup}}(\\lbrace i\\rbrace\\times F_i\\times F_i)\\newline\\hspace{1cm} \\cdot \\ I’&amp;=I\\newline\\hspace{1cm} \\cdot \\ T’&amp;=T\\cup\\lbrace(q,\\sigma,(i,q’,q’))\\mid 1\\leq i\\leq n,(q,\\sigma,q’)\\in T, q’\\in F_i\\rbrace\\newline&amp;\\hspace{0.9cm}{}\\cup\\lbrace((i,q,p),\\sigma,(i,q’,p’))\\mid 1\\leq i\\leq n,(q,\\sigma,q’)\\in T,\\newline&amp;\\hspace{1.5cm}p’=\\left\\lbrace\\begin{array}{ll} p &amp;\\text{if } q\\neq p\\newline\\text{min}(F_i)&amp;\\text{if } q=p=\\text{max}(F_i)\\newline\\text{min}(F_i\\setminus\\lbrace r\\mid r\\leq p\\rbrace)&amp;\\text{if } q=p&lt;\\text{max}(F_i),\\end{array}\\right.\\newline&amp;\\hspace{1.5cm}q,p,q’ \\in F_i\\rbrace\\newline\\hspace{1cm} \\cdot \\ F’&amp;=\\overset{n}{\\underset{i=1}{\\bigcup}}(\\lbrace i\\rbrace\\times \\lbrace\\text{min}(F_i)\\rbrace\\times \\lbrace\\text{min}(F_i)\\rbrace)\\newline\\end{array}$ Explained in Human languageA run of the Büchi automaton first simply simulates (while in states $Q$) the Muller automaton and then “guesses” the accepting subset of the Muller automaton. The accepting subset is express by the states $(\\lbrace i\\rbrace\\times F_i\\times F_i)$, where The first component = the index $i$ of the accepting subset, The second component = the currently visited state of the Muller automaton, and The third component = the “next” state (according to the order on the states) we need to see in order to make progress towards accepting the input word. The purpose of the order $&lt;$ on the states is that we can “step” through the states of the accepting subset in order to make sure that all states in the accepting subset actually occur infinitely often. In transitions $T’$, we have the transitions Transitions for all states $Q$ are described by $T$, same as in the original Muller Automaton, Transitions that contains both states in $Q$ and accepting subset $(\\lbrace i\\rbrace\\times F_i\\times F_i)$, it stays in the subset, Transitions inside the subset, the “next” state remain unchanged until the currently visited state visits it $(p=q)$, if the currently visited state visits “next” state and it is the last “step” of the subset, it means we visited the entire subset $F_i$ and we should start from the beginning $\\text{min}(F_i)$ again. otherwise, move one step ahead ($p’ &gt; p$ and $p, p’\\in F_i$) The Büchi automaton accepts if we step through the states of the accepting subset infinitely often.Recall that we used a similar trick in the construction of the Büchi automaton for the intersection of two Büchi-recognizable languages in Construction 3.2. $\\textbf{Theorem 10.2. } \\newline\\textit{For every Muller automaton }\\mathcal{A}\\textit{ there is a Büchi automaton }\\mathcal{A’}\\textit{ such that }\\mathcal{L(A)}=\\mathcal{L(A’)}.$ Proof $\\mathcal{L(A)}\\subseteq\\mathcal{L(A’)}$ (all word accepted by $\\mathcal{L(A)}$ must also be accepted by $\\mathcal{L(A’)}$): Let $\\alpha\\in\\mathcal{L(A)}$ and $r=q_0q_1q_2\\dots$ be an accepting run of $\\mathcal{A}$ on $\\alpha$. As $r$ is accepting, we have that: $\\text{Inf}(r)\\in\\mathcal{F}$, so $r$ must be in one of the accepting subset, i.e. $\\text{Inf}(r)=F_i$ for some $1\\leq i\\leq n$, Let $m$ be the first position that visit some accepting state: $q_j\\in\\text{Inf}(r)$ for all $j\\geq m$, Now consider some run of $\\mathcal{A’}$ on $\\alpha:\\ r’ = q_0q_1\\dots q_{m−1}(i, q_m, p_0)(i, q_{m+1}, p_1)(i, q_{m+2}, p_2)\\dots$ it nondeterministically switches to $(i, q_m, p_0)$ at position $m$. For the sake of contradiction, assume that $r$ is not accepting: Then there is a position $k\\geq 0$ such that $p_j=p_k$ for all $j\\geq k$ ($q$ never moves to $p_k$,“next” state got stuck). Then also $q_{m+j}\\neq p_j$ for all $j\\geq k$. However, this contradicts that $p_k\\in F_i$.(if q can never reach $p_k$, then it is not an accepting state and thus contradicts with the definition of the subset) $\\mathcal{L(A)}\\supseteq\\mathcal{L(A’)}$ (all word accepted by $\\mathcal{L(A’)}$ must also be accepted by $\\mathcal{L(A)}$): Let $\\alpha\\in\\mathcal{L(A’)}$, $r’ = q_0q_1\\dots q_{m−1}(i, q_m, p_0)(i, q_{m+1}, p_1)(i, q_{m+2}, p_2)\\dots$ be some accepting run of $\\mathcal{A’}$ on $\\alpha$: At some position $m$ it switches to some $(i, q_m, p_0)$, otherwise it would not be accepting. By construction, $q_j\\in\\text{Inf}(r)$ for all $j\\geq m$ (it starts staying in the accepting subset), and For each $p\\in F_i$ there are infinitely many positions $k$ such that $q_k=p_k=p$. ($q$ always reach every “next” state infinitely often at some positions) Thus, we can construct an accepting run $r=q_0q_1q_2\\dots$ of $\\mathcal{A}$ on $\\alpha$ by using every $q$ state in the run $r’$, because every second component in the tuple is accepting, i.e. $\\text{Inf}(pr_2(r’))=p_k=F_i$, SummaryNow we have proved that we can construct an Muller automaton from Büchi automaton and an Büchi automaton from Muller automaton. Therefore they are interchangably equivalent. In the next section, we will prove that deterministic Muller automata are actually closed. Next chapter: Closure Properties of Muller Automata under Boolean Operations Further Reading:","link":"/AGV/agv10-2/"},{"title":"AGV 10.5 -- From semi-deterministic Büchi to deterministic Muller","text":"Previous chapter: Semi-Deterministic Büchi Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionFrom the semi-deterministic Büchi automaton we now build a deterministic Muller automaton. The idea of the construction is to continuously simulate, in the deterministic automaton, the nondeterministic part of the semi-deterministic automaton and to ”attempt” a transition into the deterministic part whenever possible. In the state of the deterministic automaton we maintain an “array” of states that correspond to these attempts. Along each run of the automaton, there may of course be infinitely many such attempts; we only need a finite array, however, because we do not need to keep track of two different attempts to enter the deterministic part, if they both reach the same state (in this case, we simply track the attempt that entered the deterministic part earlier). We use an array of size $2m$, where m is the number of states of the deterministic part. The factor two allows us to leave a position of the array empty (“␣”) if an attempt is not continued. This is necessary to distinguish a situation where a previously started attempt failed and, at the same time, a new attempt enters the deterministic part, from the situation where the same attempt ran continuously. The deterministic automaton accepts if there is at least one attempt that runs forever after some point and reaches an accepting state infinitely often. Next chapter: Infinite Games (Basic Definitions) Further Reading:","link":"/AGV/agv10-5/"},{"title":"AGV 10.4 -- Semi-Deterministic Büchi Automata","text":"Previous chapter: Closure Properties of Muller automata Under Boolean Operations This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionTo prove McNaughton’s theorem, in this subsection, we will introdue the semi-deterministic Büchi automata. translate nondeterministic Büchi automata into semi-deterministic Büchi automata. translate from semi-deterministic Büchi automata to deterministic Muller automata. Semi-deterministic Büchi AutomataA semi-deterministic automaton is a (possibly nondeterministic) automaton where all accepting runs ultimately end up in a subset of the states from which all transitions are deterministic. $\\textbf{Definition 10.2. } \\text{A Büchi automata }\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))\\text{ is }\\textit{semi-deterministic}\\text{ if}\\newline Q = N \\uplus D\\text{ is a partition of }Q\\text{ such that }F\\subseteq D, pr_2(T\\cap(D\\times\\Sigma\\times Q))\\subseteq D\\text{, and }\\newline(\\Sigma,D,\\lbrace d\\rbrace,T\\cap(D\\times\\Sigma\\times D),\\small\\text{BÜCHI}\\normalsize(F)) \\text{ is deterministic for every }d\\in D.$ Explaination$Q = N \\uplus D:$ a disjoint union symbol $\\uplus$ indicates that $N$ and $D$ are two seperated subset. $D:$ firstly, set of accepting states in part of $D$ ($F\\subseteq D$). Then for all transistion starts from $D$ ($T\\cap(D\\times\\Sigma\\times Q)$), their successors are also in $D$ ($pr_2(T\\cap(D\\times\\Sigma\\times Q))\\subseteq D$). Therefore, we can split such automaton into nondeterministic part $N$ and determinstic part $D$, and the accepting run will end up stays in $D$. From Nondeterministic to Semi-deterministic Büchi AutomataThe translation is based on a subset construction, where we collect two sets of states: the states that are reachable on the given input word, and the states that are reachable on some path through an accepting state. A state of the semi-deterministic automaton is accepting if the two sets become equal; when this happens, the second set is reinitialized with the subset of accepting states that appear in the first component. Example The subset construction produces a deterministic automaton that accepts a subset of the words accepted by the original automaton. If the two sets are equal infinitely often, we can construct a run of the original automaton that goes through accepting states infinitely often: intuitively, we can go “backwards” from each position where the two sets have become equal and select a path segment for the original automaton where an accepting state is visited (in the proof below we give a more precise argument using König’s lemma). There is no general guarantee that the set of reachable states from some position of an accepting run and the set of states reachable on a path through some accepting state are the same. This is illustrated by the following example. Let the input word be $\\alpha^\\omega$. From the initial position of some run, which starts in the initial state $p$, all states are reachable, but only $r$ and $s$ are reachable on paths from $s$. Ultimately, however, every accepting run must reach (and remain in) positions where the set of reachable states and the set of states reachable on a path through some accepting state are the same. This is because the set of reachable states can only become smaller finitely often; hence, at some point, the set of reachable states will remain the same from all subsequent positions, including those (future) positions of the accepting run where the run visits an accepting state. In our semi-deterministic automaton, we therefore start by simulating the given nondeterministic automaton. At any point we allow a nondeterministic transition into the (from then on) deterministic subset construction. $\\textbf{Construction 10.5. } \\text{For a Büchi automaton }\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI}\\normalsize (F))\\text{, we construct the semi-}\\newline\\text{deterministic Büchi automaton }\\mathcal{A’} = (\\Sigma,Q’,I’,T’,\\small\\text{BÜCHI}\\normalsize (F’))\\text{ with }\\mathcal{L}(\\mathcal{A’})=\\mathcal{L}(\\mathcal{A})\\text{ as follows:}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ Q’=Q\\uplus (2^Q\\times2^Q)\\newline\\hspace{1cm} \\cdot \\ I’=I’ \\newline\\hspace{1cm} \\cdot \\ T’=T\\cup\\lbrace(q,\\sigma,(\\lbrace q’\\rbrace,\\varnothing))\\mid(q,\\sigma,q’)\\in T\\rbrace \\newline\\hspace{2.85cm}\\cup \\ \\lbrace((L_1,L_2),\\sigma,(L’_1,L’_2))\\mid L_1\\neq L_2\\newline\\hspace{3.8cm}L’_1=pr_2(T\\cap L_1\\times\\lbrace\\sigma\\rbrace\\times Q)\\newline\\hspace{3.8cm}L’_2=pr_2(T\\cap L_1\\times\\lbrace\\sigma\\rbrace\\times F)\\cup pr_2(T\\cap L_2\\times\\lbrace\\sigma\\rbrace\\times Q)\\rbrace\\newline\\hspace{2.85cm}\\cup \\ \\lbrace((L_1,L_2),\\sigma,(L’_1,L’_2))\\mid L_1= L_2\\newline\\hspace{3.8cm}L’_1=pr_2(T\\cap L_1\\times\\lbrace\\sigma\\rbrace\\times Q)\\newline\\hspace{3.8cm}L’_2=pr_2(T\\cap L_1\\times\\lbrace\\sigma\\rbrace\\times F)\\newline\\hspace{1cm} \\cdot \\ F’=\\lbrace(L,L)\\in(2^Q\\times2^Q)\\mid L\\neq\\varnothing\\rbrace\\newline\\end{array}$ $\\textbf{Lemma 10.1. } \\textit{For every Büchi automaton }\\mathcal{A}\\textit{ there exists a semi-deterministic Büchi}\\newline\\textit{automaton }\\mathcal{A’}\\textit{ with }\\mathcal{L}(\\mathcal{A})=\\mathcal{L}(\\mathcal{A’}).$ Proof$\\mathcal{L}(\\mathcal{A’})\\subseteq\\mathcal{L}(\\mathcal{A})$ Let $\\alpha\\in\\mathcal{L}(\\mathcal{A’})$ and let $r’=q_0q_1\\dots q_{n-1}(L_n,L’n)(L{n+1},L’{n+1})\\dots$ be an accepting run of $\\mathcal{A’}$ on $\\alpha$. Since $r’$ is accepting, there is an infinite sequence $i_0i_1\\dots$ of indices such that $i_0=n$, and, for all $j\\geq1$, $L{i_j} = L’{i_j}$ and $L’{i_j}\\neq\\varnothing$. For every $j\\geq1$, and every $q’\\in L_{i_j}$ there exists a state $q\\in L_{i_{j−1}}$ and a sequence $q = q_{i_{j−1}} , q_{i_{j−1}+1},\\dots, q_{i_j} = q’$ such that $(q_k, \\alpha(k), q_{k+1})\\in T$ for all $k\\in \\lbrace i_{j−1},\\dots,i_j − 1\\rbrace$ and $q_k\\in F$ for some $k\\in\\lbrace i_{j−1}+1,\\dots,i_j\\rbrace$. We use the following notation: $\\textit{predecessor}(q’,i_j) := q, \\textit{run}(q’,i_0) = q_0q_1\\dots q_{n−1}q’$ for $L_{i_0} = \\lbrace q’\\rbrace$, and $\\textit{run}(q’,i_j)=(q_{i_{j−1}+1})(q_{i_{j−1}+2})\\dots q_{i_j}$, for $j\\geq1$. Now consider the j∈N Lij × {j} -labeled tree where the root is labeled with (q ′ , 0) for Li0 = {q ′}, and the parent of each node with a label (q ′ , j) is labeled with (predecessor(q ′ , ij ), j − 1). The tree is infinite and finite-branching, and, hence, by K¨onig’s Lemma, has an infinite branch (qi0 , i0),(qi1 , i1), . . ., corresponding to an accepting run of A: run(qi0 , i0) · run(qi1 , i1) · run(qi2 , i2) · . . . L(A) ⊆ L(A′ ): Let α ∈ L(A) and let r = q0, q1, . . . be an accepting run of A on α. Let i ∈ N be an index s.t. qi ∈ F and for all j ≥ i there exists a k &gt; j, such that {q ∈ Q | qi α[i,k] −−−→ q} = {q ∈ Q | qj α[j,k] −−−→ q}. The index i exists: ”⊇” holds for all i, because there is a path through qj . Assume, by way of contradiction, that for all i ∈ N, there is a j ≥ i s.t for all k &gt; j ”⊋” holds. Then there exists an i ′ s.t. {q ∈ Q | qi ′ α[i ′ ,k] −−−−→ q} = ∅ for all k &gt; i′ . Contradiction. We define a run r ′ of A′ : r ′ = q0 . . . qi−1({qi}, ∅)(L1, L′ 1 )(L2, L′ 2 ). . . where Lj and L ′ j are determined by the definition of A′ . To prove that r ′ is accepting, assume otherwise, and let m ∈ N be an index such that Ln ̸= L ′ n for all n ≥ m. Then, let j &gt; m be some index with qj ∈ F; hence qj ∈ L ′ j . There exists a k &gt; j such that L ′ k+1 = {q ∈ Q | qj α[j,k] −−−→ q} = {q ∈ Q | qi α[i,k] −−−→ q} = Lk+1. Contradiction. Next chapter: From semi-deterministic Büchi to deterministic Muller Further Reading:","link":"/AGV/agv10-4/"},{"title":"AGV 11.2 -- Reachability Games","text":"Previous chapter: Infinite Games (Basic Definitions) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Reachability ConditionWe will now analyze infinite games for various types of winning conditions. We start with the simple reachability condition. The reachability condition is given as a set $R$ of positions called the reachability set. The reachability condition is satisfied if the play reaches some position in $R$. Formally, for an infinite word $\\alpha$ over $\\Sigma$, we use $\\text{Occ}(\\alpha) := \\lbrace\\sigma\\in\\Sigma\\mid\\exists n\\in\\mathbb{N}.\\ \\alpha(n)=\\sigma\\rbrace$ to denote the set of all letters occurring in $\\alpha$. $\\textbf{Definition 11.11. } \\text{The }\\textit{reachability condition }\\small\\text{REACH} \\normalsize(R)\\text{ on a set of positions }R\\subseteq V\\text{ is the set}$ $$\\small\\text{REACH} \\normalsize(R) = \\lbrace\\rho\\in V^\\omega\\mid\\text{Occ}(\\rho)\\cap R\\neq\\varnothing\\rbrace\\newline\\newline\\text{A game }\\mathcal{G}=(\\mathcal{A},\\text{Win})\\text{ with Win}=\\small\\text{REACH} \\normalsize(R)\\text{ is called a }\\textit{reachability game}\\text{ with reachability set }R$$ Example$$\\text{Position of Player 0: Circles;}\\ \\ \\text{Positions of Player 1: rectangles.}$$ $$\\mathcal{G}=(\\mathcal{A},\\small\\text{REACH} \\normalsize(R)),\\ R=\\lbrace v_4,v_5\\rbrace$$ The winning region for Player 0: $W_0(\\mathcal{G})=\\lbrace v_3,v_4,v_5,v_6,v_7,v_8\\rbrace$ Strategy: $\\sigma(v_1) = v_2, \\sigma(v_3) = v_4, \\sigma(v_7) = v_8, \\sigma(v_8) = v_5$ Attractor ConstructionReachability games can be solved with a simple fixed point construction called the attractor construction. The attractor construction computes the winning region for Player 0 iteratively by the reachability set: adds all positions owned by Player 0 that have an edge into the winning region, then adds all positions owned by Player 1 where all edges lead into the winning region. (no choices) Repeats until no more positions can be added. In the following, we give a slightly more general definition of the attractor construction that can be applied also to Player 1. We do this in preparation for the constructions for other winning conditions, which will use the attractor construction as a subroutine. $\\textbf{Construction 11.1. } \\text{Let an arena }\\mathcal{A} = (V,V_0,V_1,E)\\text{ be given. The }\\textit{attractor construction}\\text{ on}\\newline\\mathcal{A}\\text{ is defined for each Player }i\\text{, for all }n\\in\\mathbb{N}\\text{ and }R\\subseteq V\\text{ as follows.}$$\\begin{array}{lll}\\hspace{1cm} \\cdot \\ CPre^{i}(R) &amp;=&amp; \\lbrace v\\in V_i\\mid\\exists v’\\in V.(v, v’)\\in E\\wedge v’\\in R\\rbrace\\cup\\ \\newline&amp;&amp;\\lbrace v\\in V_{1-i}\\mid\\forall v’\\in V.(v, v’)\\in E\\rightarrow v’\\in R\\rbrace\\newline\\hspace{1cm} \\cdot \\ Attr_{0}^{i}(R)&amp;=&amp;R \\newline\\hspace{1cm} \\cdot \\ Attr_{n+1}^{i}(R)&amp;=&amp;Attr_{n}^{i}(R)\\cup CPre^{i}(Attr_{n}^{i}(R)) \\newline\\hspace{1cm} \\cdot \\ Attr^{i}(R)&amp;=&amp;\\underset{n\\in\\mathbb{N}}{\\bigcup} Attr_{n}^{i}(R)\\newline\\end{array}$ Example In general, the attractor construction solves a game with winning condition $\\small\\text{REACH} \\normalsize(R)$ as follows: $W_0(\\mathcal{G})=Attr^0(R), W_1(\\mathcal{G})=V\\setminus W_0(\\mathcal{G})$. We can furthermore give a uniform memoryless winning strategy. These results are summarized in the following theorem. $\\textbf{Theorem 11.1. } \\textit{Reachability games are memoryless determined. It holds that}\\newline W_0(\\mathcal{G})=Attr^0(R), W_1(\\mathcal{G})=V\\setminus W_0(\\mathcal{G})\\textit{. Both players have a uniform winning strategy.}$ ProofWe show for all positions $v\\in V$ that If $v\\in Attr^0(R)$, then $v\\in W_0(\\mathcal{G})$, with the following uniform memoryless strategy $\\sigma$: We fix an arbitrary total ordering on $V$ . For $v\\in (Attr^0(R)\\setminus R)\\cap V_0$, let $n = min\\lbrace n\\in\\mathbb{N}\\mid v\\in Attr_{n}^0(R)\\rbrace$. Then, let $\\sigma(v)$ be the smallest $v’\\in Attr_{n-1}^0(R)$ with $(v, v′)\\in E$ For every other position $v\\in V_0\\setminus(Attr^0(R)\\setminus R)$, let $\\sigma(v)$ be the smallest $v’\\in V$ with $(v, v′)\\in E$ We show, by induction on $n\\in\\mathbb{N}$, that any play that starts in $v\\in Attr_{n}^0(R)$and is consistent with $\\sigma$ reaches $R$ within at most $n$ steps. If $v\\in V\\setminus Attr^0(R)$, then $v\\in W_1(\\mathcal{G})$, with the following uniform memoryless strategy $\\tau$: We again fix an arbitrary total ordering on $V$. For $v\\in V_1\\setminus Attr^0(R)$, let $\\tau(v)$ be the smallest $v’\\in V\\setminus Attr^0(R)$ such that $(v, v′)\\in E$. Such a successor $v’$ always exists, because otherwise $v\\in Attr^0(R)$. For every other position $v\\in V_1\\cap Attr^0(R)$ let $\\tau(v)$ be the smallest $v’\\in V\\setminus Attr^0(R)$ with $(v, v′)\\in E$. Now let $\\rho$ be an arbitrary play that is consistent with $\\tau$. We show, by induction on $n$, that $\\rho(n)\\notin Attr^0(R)$ and, hence, $\\rho(n)\\notin R$, for all $n\\in\\mathbb{N}$. ExplainationStrategy for Player 0 A position $v$ belongs to $Attr_{n}^0(R)\\setminus R$, must have edges to $v’\\in Attr_{n-1}^0(R)$, then choose the smallest $v’$ If a position $v$ does not belong to $Attr^0(R)\\setminus R$, simply choose the smallest $v’$ from its edges. Strategy for Player 1 If a position $v$ does not belong to $Attr_{n}^0(R)$, it must have any one edge does not move to $Attr^0(R)$, choose the smallest $v’$ among those. If a position $v$ belongs to $Attr_{n}^0(R)$, since all its edges move to some positions in $Attr_{n}^0(R)$, simply choose the smallest $v’$ from its edges. Next chapter: Büchi Games Further Reading:","link":"/AGV/agv11-2/"},{"title":"AGV 11.1 -- Infinite Games (Basic Definitions)","text":"Previous chapter: From semi-deterministic Büchi to deterministic Muller This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionWe now introduce infinite two-player games on finite graphs. Infinite games are useful to solve the synthesis problem, where we are interested in finding a strategy that guarantees that a given specification is satisfied (cf. Section 1.2). As we will see, games also play a fundamental role in automata theory, in particular for automata over infinite trees. Basic DefinitionsThe game is played on a graph, called the arena. The vertices of the graph are called positions and are partitioned into the positions of Player 0 and the positions of Player 1. A play of the game starts in some initial position In any positions, the player who owns the position chooses the edge on which the play is continued. Player 0 wins if the play is an element of the winning condition. The winner is determined by a winning condition, which, like the acceptance condition of an automaton on infinite words is a subset of the infinite words over the positions. $\\textbf{Definition 11.1. } \\text{A }\\textit{game arena}\\text{ is a tuple }\\mathcal{A} = (V,V_0,V_1,E)\\text{, where}\\newline\\begin{array}{l}\\hspace{0.5cm} \\cdot \\ V_0\\text{ and }V_1=V\\setminus V_0\\text{ are disjoint sets of positions,}\\newline\\hspace{1cm} \\text{called the positions of Player 0 and Player 1.}\\newline\\hspace{0.5cm} \\cdot \\ E\\subseteq V\\times V\\text{ is a set of edges such that every position }v\\in V\\newline\\hspace{1cm} \\text{has at least one outgoing edge }(v,v’)\\in E.\\newline\\end{array}$ $\\textbf{Definition 11.2. } \\text{A }\\textit{play}\\text{ is an infinite sequence } \\rho\\in V^\\omega\\text{ such that}$$$\\forall n\\in\\mathbb{N}.(\\rho(n),\\rho(n+1))\\in E$$ We say a play $\\rho$ starts in a position $v$ iff $v=\\rho(0)$. We denote the set of all possible plays on $\\mathcal{A}$ with $\\text{Plays}(\\mathcal{A})$ and the set of all possible plays starting in position $v$ with $\\text{Plays}(\\mathcal{A},v)$. $\\textbf{Definition 11.3. }\\text{A }\\textit{game }\\mathcal{G}=(\\mathcal{A},\\text{Win})\\text{ consists of an arena }\\mathcal{A}\\text{ and a }\\textit{winning condition}\\newline\\text{Win}\\subseteq V^\\omega\\text{. We call a play }\\rho\\textit{ winning for Player 0}\\text{ iif }\\rho\\in\\text{Win and }\\textit{winning for Player 1}\\text{ otherwise.}$ When it is Player $i$’s turn, the current vertex must be a position of Player $i$ ($V_i$), all the prefix of the play seen so far (including current vertex) is called the history of the play, which is an element of $V^\\ast V_i$. A strategy fixes the decisions of a player based on the history of the play. A strategy for Player $i$ is a function $\\sigma:V^\\ast V_i\\rightarrow V$ that selects for each such history a successor position. $\\textbf{Definition 11.4. }\\text{A }\\textit{strategy}\\text{ for Player }i\\text{ is a function }\\sigma:V^\\ast V_i\\rightarrow V\\text{ such that }(v,v’)\\in E\\newline\\text{whenever }\\sigma(wv)=v’\\text{ for some }w\\in V^\\ast,v\\in V_i$ In the following, we use $\\sigma$ and $\\tau$ to denote strategies for Player $i$ and the opponent Player $(1−i)$, respectively. $\\textbf{Definition 11.5. }\\text{A play }\\rho\\text{ is }\\textit{consistent}\\text{ with a strategy }\\sigma\\text{ iff}$$$\\forall n\\in\\mathbb{N}.\\text{if }\\rho(n)\\in V_i\\text{ then }\\rho(n+1)=\\sigma(\\rho[n])$$ We denote the set of all plays that begin in some position $v$ and are consistent with strategy $\\sigma$ with $\\text{Plays}(\\mathcal{A}, \\sigma, v)$. Note that the strategies $\\sigma$ and $\\tau$ of the two players together uniquely identify a specific play: $\\mid \\text{Plays}(\\mathcal{A}, \\sigma, v)\\cap \\text{Plays}(\\mathcal{A}, \\tau, v)\\mid = 1$. Our definition of a strategy is very general in the sense that the decisions are based on the entire history of the play. Intuitively, this means that the players have infinite memory. It often suffices to work with simpler strategies, such as memoryless strategies. Memoryless strategies are often also called positional. $\\textbf{Definition 11.6. }\\text{A strategy }\\sigma\\text{ for Player }i\\text{ is }\\textit{memoryless}\\text{ iff }\\sigma(wv)=\\sigma(v)\\text{ for all }w\\in V^\\ast,v\\in V_i.$ In a slight abuse of notation, memoryless strategies are often given directly as a function $\\sigma:V_i\\rightarrow V$ that maps the positions owned by Player $i$ to their successor positions. Next, we characterize winning strategies: $\\textbf{Definition 11.7. }\\text{A strategy }\\sigma\\text{ for Player }i\\text{ is }\\textit{winning}\\text{ from a position }v\\text{ if all plays that}\\newline\\text{start in }v\\text{ and that are consistent with }\\sigma\\text{ are winning for Player }i.$ Note that this definition refers to a specific position $v$ in which we start the play. The set of all positions where the player has a winning strategy is called the winning region. $\\textbf{Definition 11.8. }\\text{ The }\\textit{winning region }W_i(\\mathcal{G})\\text{ of Player }i\\text{ in a game }\\mathcal{G}\\text{ is defined as}\\newline\\text{the set of positions }v\\in V\\text{ for which there exists a strategy for Player }i\\text{ that is winning from }v.$ Note that the strategies for different positions in the winning region may be different. If a strategy $\\sigma$ is winning from all positions of the winning region, we call $\\sigma$ a uniform winning strategy. It is easy to see that no position can be in the winning regions of both players. Otherwise there exists a position $v$ and strategies $\\sigma$ and $\\tau$ that are winning from $v$ for Player 0 and 1, respectively. Then the unique play that is consistent with $\\sigma$ and $\\tau$ need to be both in Win, because $\\sigma$ is winning, and not in Win, because $\\tau$ is winning. A more difficult question is whether all positions are in some winning region, i.e., whether the winning regions form a partition of $V$. This property is called the determinacy of a game: $\\textbf{Definition 11.9. }\\text{A }\\textit{game }\\mathcal{G}\\text{ is }\\textit{determined }\\text{if }V=W_0(\\mathcal{G})\\cup W_1(\\mathcal{G})$ If the winning strategies are in fact memoryless, we say the game is memoryless (positionally) determined. $\\textbf{Definition 11.10. }\\text{A game is }\\textit{memoryless determined }\\text{if for every position }v\\in V\\text{, there exists}\\newline\\text{a memoryless stratey that is winning for some player from position }v.$ ConclusionIn this section, we have learned: Arena: the graph of the game, expressed as a tuple $\\mathcal{A}=(V,V_0,V_1,E)$ Position: vertex of Arena $v\\in V$ Positions of Player $i$: a disjoint set of position $V_i$ Play: an infinite sequence of $\\rho\\in V^\\omega$ set of all Plays: $\\text{Plays}(\\mathcal{A})$ set of all Plays in position $v$: $\\text{Plays}(\\mathcal{A},v)$ starting position: $v = \\rho(0)$ the player who owns the position chooses the edge on which the play is continued ($\\rho\\in V_i$) Game: consist of an arena and a winning condition $\\mathcal{G}=(\\mathcal{A},\\text{Win})$ winning condition: $\\text{Win}\\subseteq V^\\omega$ winning play for Player 0: $\\rho\\in\\text{Win}$ winning play for Player 1: $\\rho\\notin\\text{Win}$ history: all previous positions $V^\\ast$ Strategy: decisions of a player based on history of the play Strategy of Player $i$: $\\sigma:V^\\ast V_i\\rightarrow V$ Strategy of Player 0: $\\sigma$ Strategy of Player 1: $\\tau$ Memoryless (positional) Strategy: $\\sigma(wv)=\\sigma(v)\\text{ for all }w\\in V^\\ast,v\\in V_i$ Winning Region: set of all positions that Player $i$ has a winning strategy $W_i(\\mathcal{G})\\in V$ Uniform Winning Strategy: one strategy that can apply to any position in a winning region. Determinacy: $\\mathcal{G}$ is determined if every position is in some player’s winning region $V=W_0(\\mathcal{G})\\cup W_1(\\mathcal{G})$ A determined game have winning strategy in every positions. Memoryless Game: all strategies are memoryless in every position of a determined game Next chapter: Reachability Games Further Reading:","link":"/AGV/agv11-1/"},{"title":"AGV 11.4 -- Parity Games","text":"Previous chapter: Büchi Games This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionPerhaps the most intriguing type of infinite games are parity games. Parity games play a key role in verification (in particular for $\\mu$-calculus model checking) and synthesis, and finding fast algorithms for parity games is an active research topic. The algorithm discussed in the following takes exponential time. There are also several quasi-polynomial-time algorithms for solving parity games (starting with a breakthrough result by Calude, Jain, Khoussainov, Li, and Stephan in 2017). In practice, however, these algorithms do not perform well (yet). Parity Condition $\\textbf{Definition 11.10. }\\text{The }\\textit{parity condition }\\small\\text{PARITY} \\normalsize(C)\\text{ for a coloring function }c:V\\rightarrow\\mathbb{N}\\text{ is the set}$$$\\small\\text{PARITY} \\normalsize(C)=\\lbrace\\alpha\\in V^\\omega\\mid\\text{max}\\lbrace c(q)\\mid q\\in\\text{Inf}(\\alpha)\\rbrace\\text{is even}\\rbrace.$$ The parity condition is satisfied if the biggest number $q$ of the coloring function is even, among all positions that are visited infinitely often. Example The winning region for Player 0: $W_0(\\mathcal{G})=\\lbrace v_3,v_6,v_7\\rbrace$ Strategy: $\\sigma(v_3) = v_6, \\sigma(v_7) = v_4, \\sigma(v_7) = v_6$ ($v_8$ have no choices and $v_1$ is arbitrary) The winning region for Player 1: $W_1(\\mathcal{G})=\\lbrace v_0,v_1,v_2,v_4,v_5,v_8\\rbrace$ Strategy: $\\tau(v_0) = v_1, \\tau(v_2) = v_5, \\tau(v_4) = v_0, \\tau(v_5) = v_1$ We first prove that parity games are memoryless determined, then derive an algorithm for solving parity games. In the following theorem, we emphasize that determinacy holds also for (countably) infinite game arenas. This will be helpful when we use the determinacy to complement tree automata, because the acceptance game of a tree automaton refers to the infinite input tree and is therefore infinite. $\\textbf{Theorem 11.3. } \\textit{Parity games are memoryless determined with uniform winning strategies}\\newline\\textit{for game arenas with a countable set of positions and a finite number of colors.}$ Here, we try to construct a uniform winning strategy for an arbitrary parity game. ProofLet $k=\\text{max}\\lbrace c(v)\\mid v\\in V\\rbrace$ be the highest color in the given parity game. We prove that parity games are memoryless determined by induction on $k$. Case $k=0$: If the highest color is 0, then all plays are winning. $W_0(\\mathcal{G})=V, W_1(\\mathcal{G})=\\varnothing$. For the memoryless winning strategy $\\sigma$, we fix an arbitrary total order on $V$ and choose $\\sigma(v)=\\text{min}\\lbrace v’\\in V\\mid(v,v’)\\in E\\rbrace$. Case $k&gt;0$: If $k$ is even, consider Player $i$, otherwise Player (1-$i$). Let $W_{i-1}$ be the set of positions where Player (1-$i$) has a memoryless winning strategy. We show that Player $i$ has a memoryless winning strategy $\\sigma$ from $V\\setminus W_{1−i}$ . Consider the subgame $\\mathcal{G’}$: $V_0’ = V_0\\setminus W_{1-i},V_i’=V_1\\setminus W_{1-i}, V’=V_0’\\cup V_1’$ $E’=E\\cap(V’\\times V’)$ $c’(v)=c(v)\\text{ for all }v\\in V’$ Note that $\\mathcal{G’}$ is still a game: for $v\\in V_i’$, there is a $v\\in V\\setminus W_{1-i}$ with $(v,v’)\\in E’$, otherwise $v$ would be in $W_{1-i}$ for $v\\in V_{1-i}’$, for all $v’\\in V$ with $(v,v’)\\in E’$, $v’\\in V\\setminus W_{1-i}$, hence there is a $v’\\in V’$ with $(v,v’)\\in E$. Let $c’^{−1}(k)=\\lbrace v\\in V’\\mid c’(v)=k\\rbrace$ (set of highest color positions in $V’$), andLet $Y=Attr_i’(c’^{-1}(k))$. (attractor here means the set of positions that at least visit $c’^{-1}(k)$ once)Let $\\sigma_A$ be the corresponding attractor strategy on $\\mathcal{G’}$ into $c’^{-1}(k)$, as defined in the proof of Theorem 11.1. Now consider the subgame $\\mathcal{G’’}$: $V_0’’ = V_0’\\setminus Y,V_i’’=V_1’\\setminus Y, V’’=V_0’’\\cup V_1’’$ $E’’=E’\\cap(V’’\\times V’’)$ $c’’:V’’\\rightarrow\\lbrace 0,\\dots,k-1\\rbrace;c’’(v)=c’(v)\\text{ for all }v\\in V’’$ Note that $\\mathcal{G’’}$ is still a game, and that the maximal color in $\\mathcal{G’’}$ is at most $k−1$ (we removed position that color as $k$ and positions that can visit it by Player $i$). By induction hypothesis, that $\\mathcal{G’’}$ is memoryless determined. It is also clear that $W_{1-i}’’$, the set of positions in game $\\mathcal{G’’}$ where Player (1−$i$) has a memoryless winning strategy, is empty, because $W_{1-i}’’$ is a subset of $W_{1-i}$: assume Player (1−$i$) had a memoryless winning strategy from some position in $V’’$. Then this strategy would win in $\\mathcal{G}$, too, since Player $i$ has no opportunity to leave $\\mathcal{G’’}$ other than to $W_{1-i}$. Hence, there is a uniform winning memoryless winning strategy $\\sigma_{IH}$ for player $i$ from all positions in $V’’$. We define the following uniform strategy $\\sigma$ for Player $i$ in game $\\mathcal{G}$: $$\\sigma(v)=\\left\\lbrace\\begin{array}{ll}\\sigma_{IH}(v)&amp;\\text{if }\\ v\\in V’’\\newline\\sigma_{A}(v)&amp;\\text{if }\\ v\\in V\\setminus c’^{-1}(k)\\newline\\text{min. successor in }V\\setminus W_{1-i}&amp;\\text{if }\\ v\\in V\\cap c’^{-1}(k)\\newline\\text{min. successor in }V&amp;\\text{otherwise.}\\newline\\end{array}\\right.$$ The strategy $\\sigma$ is winning for Player 0 on $V\\setminus W_{1−\\sigma}$. Consider a play that is consistent with $\\sigma$, it can be either: $Y$ (Set of positions that Player 0 can visit the highest color $k$) is visited infinitely often. Thus, Player $i$ wins. Eventually only positions in $V’’$ are visited. Hence, since Player $i$ follows $\\sigma_{IH}$, Player $i$ wins. Construct $W_{1−i}$ with McNaughton’s algorithmThe proof above is non-constructive in the sense that we begin the argument by considering (rather than computing) the set $W_{1−i}$ of positions where the opponent, Player (1−$i$), has a memoryless winning strategy. McNaughton’s algorithm, one of the classic algorithms for parity games over finite arenas, computes this set iteratively, with repeated recursive calls: $\\textbf{Construction 11.3. } \\text{Let a finite parity game }\\mathcal{G} = (\\mathcal{A},\\small\\text{PARITY} \\normalsize(C))\\text{ be given. We compute the}\\newline\\text{winning regions }W_0(\\mathcal{G})\\text{ and }W_1(\\mathcal{G})\\text{ as follows. (To avoid confusion we indicate in each attractor}\\newline\\text{construction explicitly the game it refers to.)}$$\\newline\\text{Function }\\textit{McNaughton}(\\mathcal{G})=\\newline\\begin{array}{ll} \\hspace{1cm} 1. &amp; k:=\\text{ highest color in }\\mathcal{G}\\newline \\hspace{1cm} 2. &amp; \\textbf{if }k=0\\text{ or }V=\\varnothing\\newline &amp;\\textbf{then return }(V,\\varnothing)\\newline \\hspace{1cm} 3. &amp; i:=k\\text{ mod }2\\newline \\hspace{1cm} 4. &amp; W_{1-i}:=\\varnothing\\newline \\hspace{1cm} 5. &amp; \\textbf{repeat}\\newline &amp;\\begin{array}{ll} \\hspace{0.5cm} (a) &amp; \\mathcal{G}’:=\\mathcal{G}\\setminus Attr^i(c^{-1}(k),\\mathcal{G})\\newline \\hspace{0.5cm} (b) &amp; (W_0’,W_1’):=\\textit{McNaughton}(\\mathcal{G}’)\\newline \\hspace{0.5cm} (c) &amp; \\textbf{if }(W_{1-i}’=\\varnothing)\\textbf{ then}\\newline &amp;\\begin{array}{rl} \\ \\text{i.} &amp; W_i:=V\\setminus W_{1-i}\\newline \\ \\text{ii.} &amp; \\textbf{return }(W_0,W_1)\\newline \\end{array}\\newline \\hspace{0.5cm} (d) &amp; W_{1-i}:=W_{1-i}\\cup Attr^{1-i}(W_{1-i}’,\\mathcal{G})\\newline \\hspace{0.5cm} (e) &amp; \\mathcal{G}’:=\\mathcal{G}\\setminus Attr^{1-i}(c^{-1}(W_{1-i}’,\\mathcal{G})\\newline \\end{array}\\end{array}$ Explaination(Line 1): The construction begins by determining the highest color $k$ that appears in the arena. (Line 2): If this color is 0 (or the arena is empty), then Player 0 wins the game from all positions, function ends. (Line 3): Otherwise we continue by analyzing the game from the perspective of Player $i = 0$ if $k$ is even and from the perspective of Player $i = 1$ if $k$ is odd. (Line 4): We initialize the winning region for the opponent to $\\varnothing$ and repeat the following: (Line 5a): First, we eliminate all positions where Player $i$ can enforce a visit to the highest color $k$ (which, if visited infinitely often, is beneficial for Player $i$). (Line 5b): We recursively solve the resulting subgame $\\mathcal{G}’$. (line 5c): If the oppoent does not have any winning positions in the subgame, then we are done: If the play of $\\mathcal{G}$ stays in the subgame, then Player $i$ wins with the winning strategy of the subgame. If the play leaves the subgame, then Player $i$ can enforce a visit to the highest color. So either the play eventually stays in the subgame forever, and Player $i$ wins there, or it infinitely often leaves the subgame, and Player $i$ wins by visiting the highest color infinitely often. (line 5d): If the opponent wins from some non-empty winning region $W_{1−i}’$ of the subgame, then we add the opponent’s attractor of $W_{1−i}’$ to the winning region of $\\mathcal{G}$. The opponent is sure to win from every position in the attractor by first ensuring a visit to $W_{1−i}’$ and then staying there forever applying its winning strategy inside $\\mathcal{G}’$. Player $i$ cannot force the game out of the subgame, because $\\mathcal{G}’$ was constructed by removing Player $i$’s attractor from $\\mathcal{G}$. (line 5e): We remove the entire attractor from $\\mathcal{G}$ (line 5e) and continue with the resulting subgame. ExampleConsider again the example above. (line 1): The highest color is $k=4$,(line 3): k is even, hence $i=0$.(line 5a): We have that $c^{-1}(4)=\\lbrace v_4, v_8\\rbrace$ and $Attr^0(c^{-1}(4))=Attr^0(\\lbrace v_4,v_8\\rbrace)=\\lbrace v_3,v_4,v_6,v_7,v_8\\rbrace$. Hence, $\\mathcal{G}’$ is the subgame consisting of positions $v_0$, $v_1$, $v_2$, and $v_5$: (line 5b): The recursive call returns $W_0’=\\varnothing, W_1’=\\lbrace v_0,v_1,v_2,v_5\\rbrace$. (We skip over the evaluation of the recursive call here, note that Player 1 wins from every position with a strategy that moves from $v_2$ to $v_5$.)(line 5d): We have that $Attr^1(\\lbrace v_0,v_1,v_2,v_5\\rbrace)=\\lbrace v_0,v_1,v_2,v_4,v_5,v_8\\rbrace$. Hence, $W_1$ is set to $\\lbrace v_0,v_1,v_2,v_4,v_5,v_8\\rbrace$,(line 5e): And $\\mathcal{G}$ is reduced to the subgame consisting of positions $v_3$, $v_6$ and $v_7$: (line 5b): Now, the game does not contain any positions with color 4 anymore, we therefore call the algorithm recursively. It returns $W_0’=\\lbrace v_3,v_6,v_7\\rbrace, W_1’=\\varnothing$. (We again skip over the evaluation of the recursive call, note that all plays are winning for Player 0.)(line 5ci): Since $W_1’=\\varnothing$, we set $W_0$ to $V\\setminus W_1$,(line 5cii): Return the final result $W_0=\\lbrace v_3,v_6,v_7\\rbrace$, $W_1=\\lbrace v_0,v_1,v_2,v_4,v_5,v_8\\rbrace$. Next chapter: Muller Games Further Reading:","link":"/AGV/agv11-4/"},{"title":"AGV 11.3 -- Büchi Games","text":"Previous chapter: Reachability Games This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Recurrence ConstructionIn a Büchi game, the goal of Player 0 is to visit some accepting position infinitely often. The attractor construction checks whether there is a strategy which enforces at least one visit to an accepting position. Reaching an accepting state at least once is indeed a necessary precondition, but we also have to ensure that from this position we can enforce a second visit to some accepting state, then a third, and so forth. The recurrence construction computes the largest subset of the accepting states from which Player 0 can enforce infinitely many subsequent visits to the subset. $\\textbf{Construction 11.2. } \\text{Let an arena }\\mathcal{A} = (V_0,V_1,E)\\text{ with }V=V_0\\cup V_1\\text{ be given. The }\\newline\\textit{recurrence construction}\\text{ on }\\mathcal{A}\\text{ is defined for all }n\\in\\mathbb{N}\\text{ and }F\\subseteq V\\text{ as:}$$\\begin{array}{lll}\\hspace{1cm} \\cdot \\ W_n^1(F) &amp;=&amp; V\\setminus Attr^0(Recur_n(F))\\newline\\hspace{1cm} \\cdot \\ Recur_{0}(F)&amp;=&amp;F \\newline\\hspace{1cm} \\cdot \\ Recur_{n+1}(F)&amp;=&amp;Recur_{n}(F)\\setminus CPre^{1}(W_n^1(F)) \\newline\\hspace{1cm} \\cdot \\ Recur(F)&amp;=&amp;\\underset{n\\in\\mathbb{N}}{\\bigcap} Recur_{n}(R)\\newline\\end{array}$ The set $W_n^1(F)$ contains those positions in $V$ from which Player 1 can enforce that at most $n$ visits to $F$. The set $CPre^{1}(W_n^1(F))$ adds those positions in $V$ from which move to positions in $W_n^1(F)$, meaning that there are at most $n+1$ visits to $F$ in newly added positions. The set $Recur_{n}(F)$ contains the subset of $F$ from which Player 0 can enforce at least $n$ further (i.e., a total of at least $n+1$) visits to $F$. The set $Recur(F)$ contains the subset of $F$ from which Player 0 can enforce infinitely many visits to $F$. The recurrence construction solves a game with winning condition $\\small\\text{BÜCHI} \\normalsize (F)$ as follows: $$W_0(\\mathcal{G})=Attr^0(Recur(F)),\\ W_1(\\mathcal{G})=V\\setminus W_0(\\mathcal{G})$$ $$\\text{Position of Player 0: Circles;}\\ \\ \\text{Positions of Player 1: rectangles.}$$ $\\textbf{Theorem 11.2. } \\textit{Büchi games are memoryless determined. It holds that}\\newline W_0(\\mathcal{G})=Attr^0(Recur(F)), W_1(\\mathcal{G})=V\\setminus W_0(\\mathcal{G})\\textit{. Both players have a uniform winning strategy.}$ ProofWe show for all positions $v\\in V$ that If $v\\in Attr^0(Recur(F))$, then $v\\in W_0(\\mathcal{G})$, with the following uniform memoryless strategy $\\sigma$: We fix some arbitrary total ordering on $V$. For $v\\in (Attr^0(Recur(F))\\setminus Recur(F))\\cap V_0$, we follow the attractor strategy from the proof of Theorem 11.1. For $v\\in Recur(F)\\cap V_0$, we choose the smallest $v’\\in V$ with $(v,v’)\\in E$ and $v’\\in Attr^0(Recur(F))$. Such a successor must exist, because otherwise $v\\in CPre^{1}(W_n^1(F))$ for some $n\\in\\mathbb{N}$, and hence $v\\notin Recur(F)$. Every play that is consistent with $\\sigma$ visits $Recur(F)\\subseteq F$ infinitely often. Hence, $\\sigma$ is winning for Player 0. If $v\\in V\\setminus Attr^0(Recur (F))$, then $v\\in W_1(\\mathcal{G})$, with the following uniform memoryless strategy $\\tau$: We again fix an arbitrary total ordering on $V$. We define the memoryless strategies $\\tau$ such taht, for $n\\in\\mathbb{N}$, if a play starts in $v\\in W_n^1=V\\setminus Attr^0(Recur_n(F))$ and is consistent with $\\tau$, there are at most $n$ visits to $F$. For $n=0$ let $\\tau(v)$ be the smallest $v’\\in V$ such that $(v,v’)\\in E$ and $v’\\in V\\setminus Attr^0(F)$. For $n&gt;0$ let $\\tau(v)$ be the smallest $v’\\in W_{n-1}^1(F)$ with $(v,v’)\\in E$ if $v\\in CPre^1(W_{n-1}^1(F))$, otherwise be the smallest $v’$ in $W_{n}^1(F)$ with $(v,v’)\\in E$. Such a $v’$ always exists as otherwise $v\\in Attr^0(Recur (F))$. ExplainationStrategy for Player 0 If a position $v$ belongs to $(Attr^0(Recur(F))\\setminus Recur(F))\\cap V_0$, it must have edges to $Recur(F)$, then choose the smallest $v’$ among those. If a position $v$ belongs to $Recur(F)\\cap V_0$, we need to ensure it will visit $F$ again, so we need to choose an edge that belongs to $Attr^0(Recur(F))$. Strategy for Player 1 Every position from Player 1 must belong to $W_n^1$, that ensures that only at most n times visits to $F$, otherwise Player 1 has no choice (belongs to $Attr^0(F)$) For such $n = 0$, we can ensure it never visit $F$ by choosing smallest $v’$ that does not move to $Attr^0(F)$. For such $n &gt; 0$, if $v$ belongs to $W_{n-1}^1$, then it must have edge(s) that also belongs to $W_{n-1}^1$, otherwise there exist edge(s) belongs to $W_{n}^1$, so that it can stays in winning region of Player 1 without visiting $Attr^0(F)$. Next chapter: Parity Games Further Reading:","link":"/AGV/agv11-3/"},{"title":"AGV 11.6 -- A Remark on Undetermined Games","text":"Previous chapter: Muller Games This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Since we have shown that reachability, Büchi, parity, and Muller games are all determined, it might seem as if all games were determined. The purpose of this last subsection is to remark that this is not the case. By definition, every play is either won by Player 0 or by Player 1; however, it is entirely possible that neither Player 0 nor Player 1 has a winning strategy. We will construct a game with a winning condition that allows the players to steal strategies in the sense that if Player i has a winning strategy σ, then Player 1−$i$ has a strategy $\\tau$ that mimicks $\\sigma$ so that the play resulting from strategies σ and τ is won by Player 1-$i$: hence, strategy $\\sigma$ is not winning after all! We fix the alphabet $\\mathbb{B}=\\lbrace 0,1\\rbrace$. To define the winning condition, we introduce an infinite $\\text{XOR}$ function $f$. An infinite $\\text{XOR}$ function is a function $f:\\mathbb{B}^\\omega\\rightarrow\\mathbb{B}$ such that $f(\\alpha)\\neq f(\\beta)$ for all α, β ∈ B ω that have the exact same letter in every position except for exactly one position where they have different letters. To see that such a function exists, define an equivalence relation $\\sim$ such that $\\alpha\\sim\\beta$ iff there exists a position $n\\in\\mathbb{N}$ such that $\\alpha(i)=\\beta(i)$ for all $i\\geq n$. Let $S\\subseteq\\mathbb{B}^\\omega$ be a set that contains exactly one element from each $\\sim$-equivalence class, and let $r(\\alpha)$ be the unique $\\beta\\in S$ such that $\\alpha\\sim\\beta$. For every $\\alpha\\in\\mathbb{B}^\\omega$, the two sequences $\\alpha$ and $r(\\alpha)$ differ only in a finite number of positions. We define $f(\\alpha)=0$ if this number is even and $f(\\alpha)=1$ if it is odd. Hence, $f$ is indeed an infinite $\\text{XOR}$ function: if two sequences $\\alpha,\\beta\\in\\mathbb{B}^\\omega$ differ in exactly one position, then $f(\\alpha)\\neq f(\\beta)$. We now use the infinite $\\text{XOR}$ function $f$ to define the game. We’ll describe the game somewhat informally in terms of rounds executed by the players; it is straightforward to translate this into an explicit arena and winning condition. Our game is played in rounds $n = 0, 1, 2,\\dots,$ where in round $n$, first Player 0 picks a finite word $w_{2n}\\in\\mathbb{B}^+$, then Player 1 picks $w_{2n+1}\\in\\mathbb{B}^+$. The resulting play $\\alpha=w_0,w_1,w_2,\\dots$ is winning for Player $f(\\alpha)$. We now use the “strategy stealing” argument to show that no player has a winnig strategy in this game. A strategy for Player $i$ is a mapping $\\sigma:\\cup_{n\\in\\mathbb{N}}(\\mathbb{B}^+)^{2n+i}$, where we denote $(\\mathbb{B}^+)^0=\\varepsilon$. As usual, $\\sigma$ is a winning strategy for Player $i$ if Player $i$ wins every play that is consistent with $\\sigma_i$ . Now fix an arbitrary strategy $\\tau$ for Player 1. From $\\tau$, we construct two different strategies $\\sigma$ and $\\sigma’$ for Player 0. For the first round: $\\sigma(\\varepsilon)=0$ $\\sigma’(\\varepsilon)=1w_1$ where $w_1=\\tau(0)$ and for all subsequent rounds: $\\sigma(0,w_1,w_2,\\dots,w_{2n+1})=\\tau(1w_1,w_2,\\dots,w_{2n+1})$ $\\sigma’(1w_1,w_2,\\dots,w_{2n+1})=\\tau(0,w_1,w_2,\\dots,w_{2n+1})$ i.e., $\\sigma$ continues to mimick $\\tau$. Consider now the plays $\\alpha$, resulting from playing strategies $\\sigma$ and $\\tau$, and $\\alpha’$ resulting from playing strategies $\\sigma’$ and $\\tau$. By construction, $\\alpha= 0w_1w_2w_3\\dots$ and $\\alpha’= 1w_1w_2\\dots$ are the same except for the first position, where $\\alpha$ has a 0 and $\\alpha’$ has a 1. Hence, we have that $f(\\alpha)\\neq f(\\alpha’)$: one of the two plays is won by Player 0, the other by Player 1. Hence, $\\tau$ cannot be a winning strategy for Player 1! The construction of the stealing strategies $\\tau$, $\\tau’$ for Player 1 from a given strategy σ of Player 0 is analogous. For the first round: $\\tau(w_0)=0$ for $W_0=\\sigma(\\varepsilon)$ $\\tau’(w_0)=1w_1$ for $w_1=\\sigma(0,w_0)$ and for all subsequent rounds: $\\tau(w_0,0,w_2,\\dots,w_{2n+1})=\\sigma(w_0,1w_1,w_2,\\dots,w_{2n+1})$ $\\tau’(w_0,1w_1,w_2,\\dots,w_{2n+1})=\\sigma(w_0,0,w_1,w_2,\\dots,w_{2n+1})$ Again, the resulting plays only differ in exactly one position and are, hence, won by different players. Thus, strategy $\\sigma$ cannot be winning for Player 0 either. Next chapter: Tree Automata and Acceptance Game Further Reading:","link":"/AGV/agv11-6/"},{"title":"AGV 11.5 -- Muller Games","text":"Previous chapter: Parity Games This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Muller games differ from the games we have studied so far in that they are not memoryless determined. Consider, for example, a game arena consisting of three positions $v_0,v_1,v_2$, such that there are edges from $v_0$ and $v_2$ to $v_1$ and from $v_1$ to $v_0$ and $v_2$. Hence, the only choice in the game is in $v_1$, where Player 0 gets to choose between moving to $v_0$ or $v_2$: The Muller condition $\\mathcal{F}=\\lbrace\\lbrace v_0,v_1,v_2\\rbrace\\rbrace$ is only satisfied if all three positions are visited infinitely often. Player 0 can therefore not win the game with a memoryless strategy, because such a strategy would either visit $v_0$ and $v_1$ infinitely often, or $v_1$ and $v_2$, but not all three positions. There is, however, a memoryful winning strategy: simply alternate between moving from $v_1$ to $v_0$ and to $v_2$. In the following, we solve Muller games via a reduction to parity games, i.e., we define a parity game such that we can translate the winning strategy in the parity game into a winning strategy in the Muller game. The fact that parity games are memoryless determined and Muller games are not, is not a contradiction: our reduction introduces additional state into the parity game, such that, on the extended state space, there exists a memoryless winning strategy. This augmentation of the state space is known as the latest appearence record: Latest Appearence Record $\\textbf{Definition 11.11. } \\text{Let }V\\text{ be the set of positions of the game arena and let }{$}\\text{ be some fresh}\\newline\\text{symbol. A }\\textit{latest appearence record}\\text{ over }V\\text{ is a finite word over the alphabet }V\\cup\\lbrace{$}\\rbrace\\text{ where every}\\newline\\text{letter from }V\\cup\\lbrace{$}\\rbrace\\text{ appears exactly once and whose first letter is from }V\\text{ . The }\\textit{hit set}\\text{ of a latest}\\newline\\text{appearence record }\\ell=v_0v_1\\dots v_m{$}v_m+1\\dots v_n\\text{ is defined as }hit(\\ell)=\\lbrace v_0,\\dots v_m\\rbrace.$ We denote the set of all latest appearence records by $\\text{LAR}$. Each latest appearence record represents a permutation of $V$ plus a position indicated by ${$}$. We will construct the parity game in such a way that whenever the play visits $v_i$, $v_i$ is moved to the beginning of the word, and the ${$}$-symbol is moved to $v_i$’s previous position. In this way, every play will reach a position such that, from then on, the hit set (i.e., the set of game positions to the left of ${$}$) is always a subset of the infinity set, and infinitely often equal to the infinity set. The winning condition in the parity game thus only needs to ensure that the infinity set is actually one that satisfies the Muller condition. For this purpose, we assign even colors to hit sets (multiply the size of hit sets by 2) that appear in the table of the Muller condition, and odd colors to hit sets (multiply the size of hit sets by 2 then -1) that do not appear in the table. Since the hit set corresponds to subsets of the infinity set, we need to make sure that odd colors that result from strict subsets of entries in the table are ignored. We do this by increasing the colors depending on the position of the ${$}$-symbol. In this way, the color of the appearence records corresponding to the full infinity set is more important than the colors of the subsets that appear in-between occurrences of the full infinity set. $\\textbf{Definition 11.4. } \\text{Let a Muller game }\\mathcal{G}=(\\mathcal{A},\\small\\text{MULLER} \\normalsize(\\mathcal{F}))\\text{ with arena }\\mathcal{A} = (V,V_0,V_1,E)\\newline\\text{be given. We compute a parity game }\\mathcal{G}’=(\\mathcal{A}’,\\small\\text{PARITY}\\normalsize(\\mathcal{c}) )\\text{ with arena }\\mathcal{A}’ = (V’,V_0’,V_1’,E’)\\newline\\text{ as follows.}$$\\begin{array}{l} \\hspace{0.5cm} \\cdot \\ V’=V\\times\\text{LAR},V_0’=V_0\\times\\text{LAR},V_1’=V_1\\times\\text{LAR} \\newline \\hspace{0.5cm} \\cdot \\ E’=\\lbrace((v,v_0v_1\\dots v_m{$}v_{m+1}\\dots v_n),(v’,v_0’v_1’\\dots v_{j-1}{$}v_{j+1}\\dots v_{m+1}\\dots v_n))\\mid(v,v’)\\in E,v’=v_j\\rbrace\\newline \\hspace{0.5cm} \\cdot \\ c(v,\\ell)= \\left\\lbrace\\begin{array}{ll} 2\\cdot |hit(\\ell)| &amp; \\text{if }hit(\\ell)\\in\\mathcal{F}\\newline 2\\cdot |hit(\\ell)|-1 &amp; \\text{if }hit(\\ell)\\notin\\mathcal{F}\\newline \\end{array}\\right. \\newline\\end{array}$ In order to solve a given Muller game, we solve the constructed parity game. To determine if a player has a winning strategy from some position $v$ of the Muller game, we then simply check if the same player wins from the corresponding position of the parity game. In principle, we could use any position of the parity game where $v$ appears in the first component of the position of the parity game. In the following theorem, we arbitrarily fix the position $(v,v_0v_1\\dots v_n{$})$. $\\textbf{Theorem 11.4. } \\textit{For every Muller game }\\mathcal{G}=(\\mathcal{A},\\small\\text{MULLER} \\normalsize(\\mathcal{F}))\\textit{ with arena }\\mathcal{A} = (V,V_0,V_1,E),\\newline\\textit{there is a parity game }\\mathcal{G}’=(\\mathcal{A}’,\\small\\text{PARITY}\\normalsize(\\mathcal{c}) )\\text{ with arena }\\mathcal{A}’ = (V’,V_0’,V_1’,E’)\\textit{ where}\\newline V’=V\\times\\text{LAR}\\textit{ such that each player has a winning strategy from a position }v\\in V\\textit{ of the Muller}\\newline\\textit{game iff the same player has a winning strategy from position }(v,v_0v_1\\dots v_n{$})\\textit{ of the parity game.}$ ExampleConsider the following game with winning condition $\\small\\text{MULLER} \\normalsize(\\lbrace\\lbrace v_0,v_1\\rbrace\\rbrace)$: We construct the following parity game: (Shown is only the reachable part of the arena from positions $(v_0, v_0v_1{$})$ and $(v_1, v_0v_1{$})$.) Since Player 0 wins from every position of the parity game, we conclude that the same is true for every position of the Muller game. Next chapter: A Remark on Undetermined Games Further Reading:","link":"/AGV/agv11-5/"},{"title":"AGV 12.1 -- Tree Automata and Acceptance Game","text":"Previous chapter: A Remark on Undetermined Games This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionInfinite games allow us to reason elegantly about infinite trees. A famous example of an argument that became significantly simpler with the introduction of game-theoretic ideas is the proof of Rabin’s theorem. Rabin’s theorem states that the satisfiability of monadic second-order logic with two successors (S2S) is decidable. Like in Section 6, where we showed that S1S formulas can be translated to automata, we will show that S2S formulas can be translated to automata, this time, in order to accommodate more than one successor function, to automata over infinite trees. The most difficult part of the proof of Rabin’s theorem is to show that tree automata are closed under complement. The original proof was purely combinatorial (and very difficult to understand), but the game-theoretic argument is simply based on the determinacy of the acceptance game of the tree automaton: the acceptance of a tree by a tree automaton = the existence of a winning strategy for Player 0, the non-acceptance = the absence of such a strategy = the existence of a winning strategy for Player 1. We can therefore complement the language of a given tree automaton by constructing a new automaton that verifies the existence of a winning strategy for Player 1. We begin this section with a discussion of tree automata. The logic S2S and the translation to tree automata will be introduced later in the section. Tree AutomataWe consider tree automata over infinite binary trees. We use the notation for trees introduced in Section 7.1. The (full) binary tree is the language $\\mathcal{T} =\\lbrace 0, 1\\rbrace^\\ast$. For an alphabet $\\Sigma, \\mathcal{T}_\\Sigma = \\lbrace(\\mathcal{T}, t)\\mid t:\\mathcal{T}\\rightarrow\\Sigma\\rbrace$ is the set of all binary $\\Sigma$-labeled trees. $\\textbf{Definition 12.1. } \\text{An }\\textit{automaton over infinite binary trees }\\mathcal{A}\\text{ is a tuple }(\\Sigma,Q,q_0,T,Acc)\\text{, where}\\newline\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\Sigma \\text{ is a finite alphabet,} \\newline\\hspace{1cm} \\cdot \\ Q \\text{ is a finite set of states,} \\newline\\hspace{1cm} \\cdot \\ q_0 \\in Q \\text{ is an initial states,} \\newline\\hspace{1cm} \\cdot \\ T \\subseteq Q \\times \\Sigma \\times Q \\times Q, \\newline\\hspace{1cm} \\cdot \\ Acc \\subseteq Q^\\omega \\text{ is the accepting condition}\\end{array}$ In the following, we will refer to automata over infinite binary trees simply as tree automata. Note that a transition of a tree automaton has two successor states, rather than a successor state as for word automata. The two states correspond to the two directions 0 and 1 of the input tree, the automaton may transition into different states for the different directions. $\\textbf{Definition 12.2. } \\text{A }\\textit{run }\\text{of a tree automata }\\mathcal{A}\\text{ on an infinite }\\Sigma\\text{-labeled binary tree }(\\mathcal{T},t)\\newline\\text{ is a }Q\\text{-labeled binary tree }(\\mathcal{T},r)\\text{ such that the following hold:}\\newline\\begin{array}{l}\\hspace{0.5cm} \\cdot \\ r(\\varepsilon)=q_0 \\hspace{2.5cm} \\cdot \\ (r(n),t(n),r(n0),r(n1))\\in T\\text{ for all }n\\in\\lbrace 0,1\\rbrace^\\ast\\end{array}$ Note that $n0$ and $n1$ are the children of node $n$ in direction 0 and 1, respectively. The accepting runs are defined as for alternating automata in Section 7.1: we apply the acceptance condition to the branches of the run tree. (Note that a run of an alternating automaton may have finite and infinite branches. The acceptance condition is only checked on infinite branches. Here, all branches are infinite.) $\\textbf{Definition 12.3. } \\text{A }\\textit{run }(\\mathcal{T},r)\\text{ is }\\textit{accepting}\\text{ iff, for every infinite branch }n_0n_1n_2\\dots,$$$r(n_0)r(n_1)r(n_2)\\dots\\in Acc$$ The language of the tree automaton $\\mathcal{A}$ consists of the set of accepted $\\Sigma$-labeled trees. ExampleThe following Büchi tree automaton $\\mathcal{A}=(\\Sigma,Q,q_0,T,\\small\\text{BÜCHI}\\normalsize(F))$ accepts all $\\lbrace a,b\\rbrace$-labeled trees with infinitely many $b$’s on each branch. $\\Sigma = \\lbrace a,b\\rbrace$ $Q=\\lbrace q_a,q_b\\rbrace; q_0=q_a$ $T=\\lbrace(q_a,a,q_a,q_a),(q_b,a,q_a,q_a),(q_a,b,q_b,q_b),(q_b,b,q_b,q_b)\\rbrace$ $F = \\lbrace q_b\\rbrace$ Consider the $\\Sigma$-labeled input tree $(\\mathcal{T},t)$: The following $Q$-labeled tree is a run $(\\mathcal{T},r)$ of $\\mathcal{A}$ on $(\\mathcal{T},t)$: Acceptance GameThe acceptance mechanism of a tree automaton is also characterized via its acceptance game. Player 0 can choose the alphabet $t(w)$ and Player 1 can choose the next state $q’$: $\\textbf{Definition 12.4. } \\text{Let }\\mathcal{A}=(\\Sigma,Q,q_0,T,Acc)\\text{ be a tree automata and let }(\\mathcal{T},t)\\text{ be a }\\Sigma\\text{-labeled}\\newline\\text{binary tree. Then the }\\textit{acceptance game}\\text{ of }\\mathcal{A}\\text{ on }(\\mathcal{T},t)\\text{ is the game }\\mathcal{G}_{\\mathcal{A},t}=(\\mathcal{A}’,\\text{Win}’)\\text{ with the}\\newline\\text{infinite game arena }\\mathcal{A}’=(V’,V_0’,V_1’,E’)\\text{ and the winning condition Win’ defined as follows:}\\newline\\begin{array}{lll}\\hspace{1cm} \\cdot \\ V_0’&amp;=&amp;\\lbrace(w,q)\\mid w\\in\\lbrace0,1\\rbrace^\\ast,q\\in Q\\rbrace\\newline\\hspace{1cm} \\cdot \\ V_1’&amp;=&amp;\\lbrace(w,\\tau)\\mid w\\in\\lbrace0,1\\rbrace^\\ast,\\tau\\in T\\rbrace\\newline\\hspace{1cm} \\cdot \\ E’&amp;=&amp;\\lbrace((w,q),(w,\\tau))\\mid\\tau=(q,t(w),q^0,q^1),\\tau\\in T\\rbrace\\ \\cup \\newline&amp;&amp;\\lbrace((w,\\tau),(w,q’))\\mid\\tau=(q,\\sigma,q^0,q^1)\\text{ and}\\newline&amp;&amp;\\hspace{3.65cm}w’=w0\\text{ and }q’=q^0\\text{ or }w’=w1\\text{ and }q’=q^1\\rbrace\\newline\\hspace{1cm} \\cdot \\ \\text{Win}’ &amp;=&amp; \\lbrace(w[0,0],q(0))(w[0,0],\\tau(0))(w[0,1],q(1))(w[0,1],\\tau(1))\\dots\\mid\\newline&amp;&amp; \\hspace{1cm} q(0)q(1)\\dots\\in Acc, w(0)w(1)\\dots\\in\\lbrace 0,1\\rbrace^\\omega,\\tau(0)\\tau(1)\\dots\\in T^\\omega\\rbrace\\end{array}$ ExampleThe following is a part of the acceptance game of automaton $\\mathcal{A}$ from the example above: $\\textbf{Theorem 12.1. } \\textit{A tree automaton }\\mathcal{A}=(\\Sigma,Q,q_0,T,Acc)\\textit{ accepts an input tree }(\\mathcal{T},r)\\newline\\textit{ if and only if Player 0 wins the acceptance game }\\mathcal{G}_{\\mathcal{A},t}=(\\mathcal{A}’,\\text{Win}’)\\textit{ from position }(\\varepsilon,q_0).$ ProofIf the input tree is accepted, there is a winning strategy for Player 0 Given an accepting run $(\\mathcal{T},r)$ of $\\mathcal{A}$, we construct a memoryless winning strategy $\\sigma:V_0’\\rightarrow V’$ for Player 0: $$\\sigma(w,q)=(w,(r(w),t(w),r(w0),r(w1)))$$ If there is a winning strategy for Player 0, the input tree is accepted Given a winning strategy $\\sigma:V’^\\ast V_0’\\rightarrow V’$, we construct an accepting run $(\\mathcal{T},r)$ where $r(\\varepsilon)=q_0$ and for all $w\\in\\lbrace 0,1 \\rbrace^\\ast$ $$r(w0)=q\\text{ for }\\sigma(\\Delta(w))=(w,(\\rule{0.5em}{0.4pt},\\rule{0.5em}{0.4pt},q,\\rule{0.5em}{0.4pt}))\\ \\text{ and }\\ r(w1)=q\\text{ for }\\sigma(\\Delta(w))=(w,(\\rule{0.5em}{0.4pt},\\rule{0.5em}{0.4pt},\\rule{0.5em}{0.4pt}q))$$ where $\\Delta(\\varepsilon)=(\\varepsilon,q_0)\\ \\text{ and }\\ \\Delta(wd)=\\Delta(w)\\cdot\\sigma(\\Delta(w))\\cdot(wd,r(wd))\\text{ for }d\\in\\lbrace0,1\\rbrace.$ Next chapter: Emptiness Game Further Reading:","link":"/AGV/agv12-1/"},{"title":"AGV 12.2 -- Emptiness Game","text":"Previous chapter: Tree Automata and Acceptance Game This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Emptiness GameThe acceptance game can also be used to test if the language of a given tree automaton is non-empty. For this purpose, we first translate the given automaton into an automaton with singleton alphabet. $\\textbf{Construction 12.1. } \\text{For a given tree automaton }\\mathcal{A}\\text{ over }\\Sigma\\text{-labeled trees, we consider the}\\newline\\text{following tree automaton }\\mathcal{A}’\\text{ over }\\lbrace1\\rbrace\\text{-labeled trees, such that }\\mathcal{L(A)}=\\varnothing\\text{ iff }\\mathcal{L(A’)}=\\varnothing:\\newline\\begin{array}{l}\\hspace{1cm} \\cdot \\ Q’=Q \\newline\\hspace{1cm} \\cdot \\ q_0’=q_0\\newline\\hspace{1cm} \\cdot \\ T’=\\lbrace(q,1,q’,q’’)\\mid(q,\\sigma,q’,q’’)\\in T,\\sigma\\in\\Sigma\\rbrace\\newline\\hspace{1cm} \\cdot \\ Acc’ = Acc\\end{array}$ Because the subtrees of a $\\lbrace 1\\rbrace$-labeled binary tree are the same from all nodes, we can simplify its acceptance game such that only finitely many positions are needed. We call this game the emptiness game. $\\textbf{Definition 12.5. } \\text{Let }\\mathcal{A}=(\\Sigma,Q,q_0,T,Acc)\\text{ be a tree automaton. The }\\textit{emptiness game}\\text{ of }\\mathcal{A}\\newline \\text{ is the game }\\mathcal{G}_{\\mathcal{A}}=(\\mathcal{A}’,\\text{Win}’)\\text{ with the finite game arena }\\mathcal{A}’=(Q\\cup T, Q,T,E)\\text{, where}\\newline E=\\lbrace(q,\\tau)\\mid\\tau=(q,\\sigma,q^0,q^1),\\tau\\in T\\rbrace\\cup\\lbrace(\\tau,q’)\\mid\\tau=(\\rule{0.5em}{0.4pt},\\sigma,q^0,q^1)\\text{ and }(q’=q^0\\text{ or }q’=q^1)\\rbrace\\newline\\text{and Win’}=\\lbrace q(0)\\tau(0)q(1)\\tau(1)\\dots\\mid q(0)q(1)\\dots\\in Acc,\\tau(0)\\tau(1)\\dots\\in T^\\omega\\rbrace$ $\\textbf{Theorem 12.2. } \\textit{The language of a tree automaton }\\mathcal{A}\\textit{ is non-empty iff Player 0 wins the}\\newline\\textit{emptiness game }\\mathcal{G}_{\\mathcal{A}}\\textit{ from position }q_0.$ ProofThe emptiness game corresponds to the acceptance game of the automaton from Construction 12.1 on the $\\lbrace 1\\rbrace$-labeled binary tree. Next chapter: Complementation of Parity Tree Automata Further Reading:","link":"/AGV/agv12-2/"},{"title":"AGV 12.3 -- Complementation of Parity Tree Automata","text":"Previous chapter: Emptiness Game This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionWe now prove that parity tree automata are closed under complementation. As discussed at the beginning of the section, our proof makes heavy use of the determinacy of parity games (with infinite game arenas) established in Theorem 11.3. $\\textbf{Theorem 12.3. } \\textit{For every parity tree automaton }\\mathcal{A}\\textit{ over }\\Sigma\\textit{ there is a parity tree automaton }\\mathcal{A}’\\newline\\textit{ with }\\mathcal{L(A’)}=\\mathcal{T}_\\Sigma\\setminus\\mathcal{L(A)}.$ ProofLet $\\mathcal{A}=(\\Sigma,Q,q_0,T,\\small\\text{PARITY} \\normalsize(C))$. By Theorem 12.1, a tree $(\\mathcal{T},t)$ is accepted by $\\mathcal{A}$ iff Player 0 has a winning strategy from position $(\\varepsilon,q_0)$ of the acceptance game $\\mathcal{G}_{\\mathcal{A},t}$. Since $\\mathcal{A}$ is a parity tree automaton, $\\mathcal{G}{\\mathcal{A},t}$ is a parity game and therefore, by Theorem 11.3, memoryless determined. Hence, $\\mathcal{A}$ does not accept some tree $t$ iff Player 1 has a winning memoryless strategy $\\sigma$ in $\\mathcal{G}{\\mathcal{A},t}$ from $(\\varepsilon,q_0)$. The strategy $\\sigma:\\lbrace 0,1\\rbrace^\\ast\\times T\\rightarrow\\lbrace 0,1\\rbrace^\\ast\\times Q$ can be represented as a function $\\sigma’:\\lbrace 0,1\\rbrace^\\ast\\times T\\rightarrow\\lbrace 0,1\\rbrace$ where $\\sigma(w,(q,\\sigma,q^0,q^1))=(wi,q^i)$ iff $\\sigma’(w,(q,\\sigma,q^0,q^1))=i$. Yet another representation of the same strategy is $\\sigma’:\\lbrace 0,1\\rbrace^\\ast\\rightarrow\\lbrace T\\rightarrow\\lbrace 0,1\\rbrace\\rbrace$, which can be understood as a labeling of $\\mathcal{T}$ with finite “local strategies”. Hence, A does not accept $(\\mathcal{T},t)$ iff there is a $(T\\rightarrow\\lbrace 0,1\\rbrace)$-labeled tree $(\\mathcal{T},v)$ such that for all i0i1i2 . . . ∈ {0, 1} Next chapter: Further Reading:","link":"/AGV/agv12-3/"},{"title":"AGV 2.1 -- Büchi automata (Preliminaries)","text":"Previous chapter: The Logic-Automata Connection This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner 2.1 Preliminaries Basic math notations Example Natural numbers $\\mathbb{N} = \\lbrace 0,1,2,3\\dots\\rbrace$ Positive Natural numbers $\\mathbb{N}^+ = \\lbrace 1,2,3\\dots\\rbrace$ Numbers from $n$ to $m$ $[n,m] = \\lbrace n,n+1,\\dots,m\\rbrace$ For $n,m\\in \\mathbb{N}$ with $n\\leq m$ Numbers from $0$ to $m$ $[m] = \\lbrace 0,1,\\dots,m\\rbrace$ $$$$ Alphabet and Letter Example Alphabet a nonempty, finite set of symbols, usually denoted by $\\Sigma$ Letter elements of an alphabets, denoted by $\\sigma$ $$$$ Finite and Infinite Word Example Finite Words concatenation $w=w(0)w(1)\\dots w(n-1)$ of finitely many letters of $\\Sigma$ Infinite Words $\\alpha$ (will be explained in chapter 2.3 $n$-th letter of word $w$ $w(n)$ for each $n\\in [|w| - 1]$ Length of words $w$ $|w|$ set of all finite words \\Sigma^*$ Infinite Words $\\alpha$ (will be explained in chapter 2.3 set of all non-empty finite words $\\Sigma^+ = \\Sigma^* \\backslash \\lbrace \\epsilon \\rbrace$ set of all infinite words $\\Sigma^\\omega$ $$$$ Language Example language over finite words ($\\omega$-language) each subset of $\\Sigma^*$ language over infinite words ($\\omega$-language) each subset of $\\Sigma^\\omega$ Next chapter: Automata over Infinite Words Further Reading: Büchi automaton, Omega language","link":"/AGV/agv2-1/"},{"title":"AGV 2.2 -- Automata over Infinite Words","text":"Previous chapter: Büchi automata (Preliminaries) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner 2.2 Automata over Infinite WordsThe operational behavior of an automaton over infinite words is very similar to automaton over finite words: Start with an initial state Automaton constructs a run by reading one letter of the input alphabet at a time, andtransitioning to a successor state permitted by its transitions. The major difference between automata over finite and infinite words is the acceptance condition. $\\textbf{Definition 2.1. } \\text{ An } \\textit{automaton over infinite words } \\text{is a tuple }\\mathcal{A} = (\\Sigma,Q,I,T,Acc)\\text{, where}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\Sigma \\text{ is a finite } \\textit{alphabet} \\newline\\hspace{1cm} \\cdot \\ Q \\text{ is a finite set of } \\textit{states} \\newline\\hspace{1cm} \\cdot \\ I \\subseteq Q \\text{ is a subset of } \\textit{initial states} \\newline\\hspace{1cm} \\cdot \\ T \\subseteq Q \\times \\Sigma \\times Q \\text{ is a set of } \\textit{transitions} \\text{, and} \\newline\\hspace{1cm} \\cdot \\ Acc \\subseteq Q^\\omega \\text{ is the } \\textit{accepting condition}\\end{array}$ In the following, we refer an automaton over infinite words simply as an automaton. Runs of Büchi automataA run in a Büchi automaton has to transition infinitely many times, and starts with the initial state. $\\textbf{Definition 2.2. }\\text{ An }\\textit{run }\\text{of an automaton }\\mathcal{A}\\text{ on an infinite input word }\\alpha\\text{ is an }\\newline\\text{infinite sequence of states }r=r(0)r(1)r(2)\\dots\\text{ such that the following hold:}\\newline\\begin{array}{l}\\hspace{1cm} \\cdot \\ r(0)\\in I\\newline\\hspace{1cm} \\cdot \\ \\text{for all }i\\in\\mathbb{N},(r(i),\\alpha(i),r(i+1))\\in T\\end{array}$ ExampleBelow is a graphical representation of an automation over: alphabet $\\Sigma = \\lbrace a,b\\rbrace$, and with set of states $Q =\\lbrace A,B,C,D\\rbrace$, initial set of states $I =\\lbrace A\\rbrace$, and set of transitions $T =\\lbrace (A,a,B),(B,a,C),(C,b,D),(D,b,A)\\rbrace$ On the infinite input word $aabbaabb\\dots$, the (only) run of the automaton is $ABCDABCDABCD\\dots$ Deterministic and Complete AutomatonAn automaton is deterministic if $|I|\\leq 1$ and $\\mid\\lbrace (q,\\sigma,q’)\\in T \\mid q’\\in Q\\rbrace|\\leq 1$ for every $q\\in Q$ and $\\sigma\\in\\Sigma$.Meaning that the automaton has exactly 1 initial state, and exactly 1 transition from $q$ to $q’$ An automaton is complete if $\\mid\\lbrace (q,\\sigma,q’)\\in T \\mid q’\\in Q\\rbrace|\\geq 1$ for every $q\\in Q$.Meaning that any two states $(q,q’)$ in the automaton should have at least 1 transitions from $q$ to $q’$. Therefore, the automaton in the example above is deterministic but not complete. The transitions of an deterministic and complete automata are usually given as a function $\\delta:Q \\times\\Sigma\\rightarrow Q$.Meaning that the set of all transitions $\\delta$ contains any states with any letter. Accepted Run $\\textbf{Definition 2.3. } \\text{ An automaton } \\mathcal{A} \\textit{ accepts}\\text{ an infinite word }\\alpha\\text{ if: }$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\text{there is a run }r\\text{ of }\\mathcal{A}\\text{ on }\\alpha\\text{, and}\\newline\\hspace{1cm} \\cdot \\ r \\text{ is } \\textit{accepting}:r\\in Acc\\end{array}\\newline\\text{The } \\textit{language recognized }\\text{by }\\mathcal{A}\\text{ is defined as follows: }\\mathcal{L}(\\mathcal{A}) = \\lbrace \\alpha\\in\\Sigma^\\omega\\mid\\mathcal{A}\\text{ accepts }\\alpha\\rbrace$ We say two automata are equivalent if they have the same language. Next chapter: Further Reading: Büchi automaton, Omega language","link":"/AGV/agv2-2/"},{"title":"AGV 2.3 -- The Büchi Acceptance Condition","text":"Previous chapter: Automata over Infinite Words This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner 2.3 The Büchi Acceptance ConditionThe Büchi Acceptance Condition is given as a set of accepting states $F$.A run of a Büchi automaton is accepting if some state from this set occurs infinitely often. Infinite wordFor an infinite word $\\alpha$ over $\\Sigma$, an Infinity Set of $\\alpha$ is denoted as:$$\\text{Inf}(\\alpha) = \\lbrace\\sigma\\in\\Sigma\\mid\\forall m\\in\\mathbb{N}.\\exists n\\in\\mathbb{N}.n\\geq m \\text{ and } \\alpha(n)=\\sigma\\rbrace$$ Meaning that $\\text{Inf}(\\alpha)$ is a set of all letters $\\sigma$ from the alphabet $\\Sigma$, so thatfor all $m$, you can always find $\\sigma$ as the n-th letter of $\\alpha$ when there exists an $n$ where $n\\geq m$. This denotes the set of all letters of $\\Sigma$ that occur infinitely often in $\\alpha$. Büchi ConditionTo express the meaning of “some state from the set of accepting states is reached infinitely often“, we can rewrite our accpeing condition $Acc$ into Büchi Condition $\\small\\text{BÜCHI} \\normalsize(F)$. Here, $\\small\\text{BÜCHI} \\normalsize(F)$ is a set of infinite word: $\\textbf{Definition 2.4. } \\text{ The } \\textit{Büchi Condition } \\small\\text{BÜCHI} \\normalsize(F)\\text{ on a set of states }F\\subseteq Q\\text{ is the set}$ $$\\small\\text{BÜCHI} \\normalsize(F) = \\lbrace\\alpha\\in Q^\\omega \\mid \\text{Inf}(\\alpha) \\cap F \\neq \\varnothing\\rbrace$$ $\\text{An automaton }\\mathcal{A}=(\\Sigma,Q,I,T,Acc)\\text{ with }Acc=\\small\\text{BÜCHI} \\normalsize(F)\\text{ is called a }\\textit{Büchi automaton.}\\newline \\text{The set }F\\text{ is called the }\\textit{set of accepting states.}$ Constrcution of Complete Büchi Automaton $\\textbf{Theorem 2.1. } \\textit{For every Büchi Automaton }\\mathcal{A},\\text{ there is a complete Büchi Automaton }\\mathcal{A}’ \\newline \\textit{such that }\\mathcal{L}(\\mathcal{A}) = \\mathcal{L}(\\mathcal{A}’)$ ExampleLet $\\mathcal{A}$ be the automaton from Example 2.1 with Büchi acceptance condition $\\small\\text{BÜCHI}\\normalsize (\\lbrace D\\rbrace)$. The language of the automaton consists of a single word:$$\\mathcal{L}(\\mathcal{A})=\\lbrace aabbaabbaabb\\dots\\rbrace$$ For a Büchi Automaton $\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))$,we define the complete Büchi Automaton $\\mathcal{A’} = (\\Sigma,Q’,I’,T’,\\small\\text{BÜCHI} \\normalsize (F’))$ using: $Q’$ = $Q \\cup \\lbrace q_f\\rbrace$ where $q_f$ is a fresh state $I’$ = $I$ $T’$ = $T \\cup \\lbrace (q,\\sigma,q_f) \\mid \\nexists q’ . (q,\\sigma,q’)\\in T\\rbrace\\cup\\lbrace (q_f,\\sigma,q_f)\\mid\\sigma\\in\\Sigma\\rbrace$ $F’$ = $F$ Explained in Human languageFirst, we add a fresh state $q_f$ the set of state $Q$ without changing the initial and accepting states. A complete Büchi Automaton requires every states must have transitions for every letters in the alphabet.So for each states orginally in $Q$, we construct new transitions that the letters are not yet used by the state.(which is $\\nexists q’ . (q,\\sigma,q’)\\in T$) Finally, for all transitions from the fresh state are self transitioning, and they include all letters.(which is $(q_f,\\sigma,q_f)\\mid\\sigma\\in\\Sigma$) Formal Proof The runs of $\\mathcal{A’}$ are a superset of those of $\\mathcal{A}$. During the construction of $\\mathcal{A’}$, we have not removed any original transitions, but adding new fresh states. Furthermore, on any infinite input word, the accepting runs of $\\mathcal{A}$ and $\\mathcal{A’}$ are the same. If $\\mathcal{A}$ accepts, $\\mathcal{A’}$ also accepts because $\\mathcal{A’}$ is a superset. If a run reaches new fresh states $q_f$ stays in $q_f$ forever, and since $q_f\\notin F’$, such a run is not accepting. SummaryA complete deterministic automaton may be viewed as a total function from $\\Sigma^\\omega$ to $Q^\\omega$.A complete (possibly nondeterministic) automaton produces at least one run for every input word. Next chapter: Kleene’s Theorem Further Reading: Büchi automaton, Omega language","link":"/AGV/agv2-3/"},{"title":"AGV 3.2 -- $\\omega$-regular language","text":"Previous chapter: Kleene’s Theorem This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Introduction$\\omega$-regular expression denotes languages over infinite words.In addition to language union on languages over infinite words, we have 2 operations that convert languages over finite words into languages over infinite words. $\\omega$-regular expressionThe collection of $\\omega$-regular languages over an alphabet $\\Sigma^\\omega$ is $\\mathcal{L}(W)$, where $W$ is a $\\omega$-regular expression.$\\omega$-regular expression can be defined as $W := E^\\omega \\mid E\\cdot W\\mid W_1+W_2 $, where: infinite concatenation of a non-empty regular language $E^\\omega$, union of $\\omega$-regular languages, $W_1+W_2$, and concatenation of regular languages and $\\omega$-regular language $E\\cdot W$ $\\textbf{Definition 3.3. } \\textit{$\\omega$-Regular expressions } \\text{are defined as follows:}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\text{If $E$ is a regular expression where $\\varepsilon\\notin\\mathcal{L}(E)$, then $E^\\omega$ is an $\\omega$-regular expression.}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(E^\\omega) = \\mathcal{L}(E)^\\omega\\newline\\hspace{1cm} \\ \\ \\text{where } L^\\omega =\\lbrace \\omega_0\\omega_1\\dots\\mid\\omega_i\\in L,|\\omega_i|&gt;0\\rbrace \\text{ for } L\\subseteq\\Sigma^* \\newline\\hspace{1cm} \\cdot \\ \\text{If $E$ is a regular expression and $W$ is an $\\omega$-regular expression,}\\newline\\hspace{1cm} \\ \\ \\text{then $E\\cdot W$ is an $\\omega$-regular expression:}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(E\\cdot W) = \\mathcal{L}(E)\\cdot\\mathcal{L}(W)\\newline\\hspace{1cm} \\ \\ \\text{where } L_1 \\cdot L_2=\\lbrace \\omega\\alpha\\mid\\omega\\in L_1,\\alpha\\in L_2\\rbrace \\text{ for } L_1 \\subseteq\\Sigma^*, L_2 \\subseteq\\Sigma^\\omega\\newline\\hspace{1cm} \\cdot \\ \\text{If $W_1$ and $W_2$ are $\\omega$-regular expressions, then $W_1+W_2$ is an $\\omega$-regular expression:}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(W_1+W_2) = \\mathcal{L}(W_1)\\cup\\mathcal{L}(W_2) .\\newline\\end{array}$ A language over infinite words is $\\omega$-regular if it is definable by a $\\omega$-regular expression. Next chapter: Closure Properties of the Büchi-recognizable languages (Intersection and Union) Further Reading: Omega language, Omega Regular language, Büchi automaton","link":"/AGV/agv3-2/"},{"title":"AGV 3.1 -- Kleene&#39;s Theorem","text":"Previous chapter: The Büchi Acceptance Condition This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionKleene’s theorem states that the set of languages over finite words that can be defined by regular expressions is exactly the set of languages that can be recognized by automata over finite words. Based on this, we define $\\omega$-regular expressions, and finally prove the corresponding theorem for $\\omega$-regular languages: Büchi’s characterization theorem. Regular ExpressionRegular expression consist of constants that denote languages of finite words, andconsist of operator symbols that denote operations over these languages. The collection of regular languages over an alphabet $\\Sigma$ is $\\mathcal{L}(E)$, where $E$ is a regular expression.Regular expression can be defined as $E := \\varepsilon\\mid\\varnothing\\mid a\\in\\Sigma\\mid E+F\\mid E\\cdot F\\mid E^* $, where: empty language: $\\varepsilon$, empty string language: $\\varnothing$, singleton language: (single letter from alphabet) $a\\in\\Sigma$, union of regular languages: $E+F$, concatenation of regular languages: $E\\cdot F$, and language with kleene star: $E^*$. $\\textbf{Definition 3.1. } \\textit{Regular expressions } \\text{are defined as follows:}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\text{The constants }\\varepsilon\\text{ and }\\varnothing\\text{ are regular expressions.}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(\\varepsilon) = \\lbrace\\varepsilon\\rbrace ,\\mathcal{L}(\\varnothing)=\\varnothing.\\newline\\hspace{1cm} \\cdot \\ \\text{If }a\\in\\Sigma\\text{ is a symbol, then }a\\text{ is a regular expressions.}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(a) = \\lbrace a\\rbrace .\\newline\\hspace{1cm} \\cdot \\ \\text{If }E\\text{ and }F\\text{ are regular expressions, then }E+F\\text{ is a regular expression:}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(E+F) = \\mathcal{L}(E)\\cup\\mathcal{L}(F) .\\newline\\hspace{1cm} \\cdot \\ \\text{If }E\\text{ and }F\\text{ are regular expressions, then }E\\cdot F\\text{ is a regular expression:}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(E\\cdot F) = \\lbrace uv\\mid u\\in\\mathcal{L}(E),v\\in\\mathcal{L}(F)\\rbrace.\\newline\\hspace{1cm} \\cdot \\ \\text{If }E\\text{ is a regular expression, then }E^*\\text{ is a regular expression.}\\newline\\hspace{1cm} \\ \\ \\mathcal{L}(E^{\\ast}) = \\lbrace u_1u_2\\dots u_n\\mid n\\in\\mathbb{N},u_i\\in\\mathcal{L}(E)\\text{ for all }0\\leq i\\leq n\\rbrace.\\end{array}$ Kleene’s TheoremA language over finite words is regular if it is definable by a regular expression, orif it is recognized by automata over finite words. $\\textbf{Definition 3.2. } \\text{An }\\textit{automaton on finite words } \\mathcal{A}\\text{ is a tuple } (\\Sigma,Q,I,T,F), \\text{ where }\\Sigma\\text{ is an input} \\newline \\text{alphabet, }Q\\text{ is a nonempty finite set of states, }I\\in Q\\text{ is a set of initial states, }\\Delta\\subseteq Q\\times\\Sigma\\times Q\\text{ is}\\newline\\text{a set of transitions, and }F\\subseteq Q\\text{ are a set of finite states.}$ Mathematical DefinitionAn automaton $\\mathcal{A}$ accepts a finite word $w \\in\\Sigma^*$ if there is a finite sequence of states $q(0)q(1) . . . q(|w|)$ such that $q(0)\\in I$ and $(q(i),w(i), q(i + 1))\\in\\Delta$ for all $i &lt; |w|$ and with $q(|w|)\\in F$ Explained in Human languageIn a nutshell, a finite word $w$ is accepted by an automaton $\\mathcal{A} = (\\Sigma,Q,I,T,F)$ if: Notation Explaination $\\Sigma$ $w$ is a finite word in $\\Sigma$ $Q$ There exists a finite sequence of states $q(0)q(1) . . . q(|w|)$ $I$ In such sequence, $q(0)$ is one of the initial state $I$ $T$ In such sequence, there is transition from $q(i)$ to $q(i+1)$ for each letter $w(i)$, where $i&lt;|w|$ $F$ In such sequence, $q(|w|)$ is one of the final state $F$, and it will be the last state of the run since there is no more letters From accepted word to accepted languageThe set of all words accepted by $\\mathcal{A}$ is called the language of $\\mathcal{A}$, denoted by $\\mathcal{L}(\\mathcal{A})$. $\\textbf{Theorem 3.1. } \\text{(Kleene’s Theorem)}\\newline\\textit{A language is regular iff it is recognized by some finite word automaton.}$ Next post: $\\omega$-regular language Further Reading: Regular expression,Regular language Kleene star","link":"/AGV/agv3-1/"},{"title":"AGV 3.3 -- Closure Properties of the Büchi-recognizable languages (Intersection and Union)","text":"Previous chapter: $\\omega$-regular language This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner In the following we refer the languages recognized by Büchi automata simply as Büchi-recognizable languages. Büchi’s characterization theorem states that the $\\omega$-regular languages are exactly Büchi-recognizable languages. To prepare for the proof of Büchi’s theorem, we establish several closure properties of the Büchi-recognizable languages. Closure Properties of Language Union $\\textbf{Construction 3.1. } \\text{Let }L_1\\text{ and }L_2\\text{ be }\\omega\\text{-languages recognized by the Büchi automata}\\newline\\mathcal{A}_1=(\\Sigma,Q_1,I_1,T_1,\\small\\text{BÜCHI}\\normalsize (F_1))\\text{ and }\\mathcal{A}_2 = (\\Sigma,Q_2,I_2,T_2,\\small\\text{BÜCHI}\\normalsize (F_2))\\text{, respectively.}\\newline\\text{We construct}\\mathcal{A}_\\cup=(\\Sigma,Q_\\cup,I_\\cup,T_\\cup,\\small\\text{BÜCHI}\\normalsize (F_\\cup))\\text{ with }\\mathcal{L}(\\mathcal{A}_\\cup)=L_1\\cup L_2\\text{ as follows:}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ Q_\\cup=Q_1\\cup Q_2 \\hspace{1cm}(\\text{w.l.o.g we assume }Q_1\\cap Q_2=\\varnothing)\\newline\\hspace{1cm} \\cdot \\ I_\\cup=I_1\\cup I_2 \\ \\hspace{1cm} \\cdot \\ T_\\cup=T_1\\cup T_2 \\ \\hspace{1cm} \\cdot \\ F_\\cup=F_1\\cup F_2 \\newline\\end{array}$ The correctness of this construction is proven by the following theorem. $\\textbf{Theorem 3.2. } \\textit{If $L_1$ and $L_2$ are Büchi-recognizable, then so is $L_1\\cup L_2$.}$ Formal ProofWe prove that the Büchi automaton $\\mathcal{A}_\\cup$ built by $\\mathcal{A}_1$ and $\\mathcal{A}_2$ indeed recognizes the union of the languages of the two automata. $\\mathcal{L}(\\mathcal{A}_\\cup)\\subseteq\\mathcal{L}(\\mathcal{A}_1)\\cup\\mathcal{L}(\\mathcal{A}_2):$ For $\\alpha\\in\\mathcal{L}(\\mathcal{A}_\\cup)$, we have an accepting run $r=r(0)r(1)r(2)\\dots$ of $\\mathcal{A}_\\cup$ on $\\alpha$.If $r(0)\\in I_1$, then $r$ is an accepting run of $\\mathcal{A}_1$, otherwise $r(0)\\in I_2$ and $r$ is an accepting run of $\\mathcal{A}_2$. $\\mathcal{L}(\\mathcal{A}_\\cup)\\supseteq\\mathcal{L}(\\mathcal{A}_1)\\cup\\mathcal{L}(\\mathcal{A}_2):$ For $i\\in\\lbrace 1,2\\rbrace$ and $\\alpha\\in\\mathcal{L}(\\mathcal{A}_i)$, there is an accepting run $r$ of $\\mathcal{A}_i$. The run $r$ is also an accepting run of $\\mathcal{A}_\\cup$. Closure Properties of Language IntersectionAs we will see later, the Büchi-recognizable languages are closed under complement.The closure under complement and union implies the closure under intersection $(\\mathcal{A}_\\cap = (\\mathcal{A’_1}\\cup \\mathcal{A’_2})’)$ Below, the automaton for the intersection is essentially the product of the two automata. $\\textbf{Construction 3.2. } \\text{Let }L_1\\text{ and }L_2\\text{ be }\\omega\\text{-languages recognized by the Büchi automata}\\newline\\mathcal{A}_1=(\\Sigma,Q_1,I_1,T_1,\\small\\text{BÜCHI}\\normalsize(F_1))\\text{ and }\\mathcal{A}_2 = (\\Sigma,Q_2,I_2,T_2,\\small\\text{BÜCHI}\\normalsize (F_2))\\text{, respectively.}\\newline\\text{We construct}\\mathcal{A}_\\cap=(\\Sigma,Q_\\cap,I_\\cap,T_\\cap,\\small\\text{BÜCHI}\\normalsize (F_\\cap))\\text{ with }\\mathcal{L}(\\mathcal{A}_\\cap)=L_1\\cap L_2\\text{ as follows:}$$\\begin{array}{rll}\\hspace{1cm} \\cdot \\ Q_\\cap&amp;=&amp;Q_1\\times Q_2\\times\\lbrace 1,2\\rbrace\\newline\\hspace{1cm} \\cdot \\ I_\\cap&amp;=&amp;I_1\\times I_2\\times\\lbrace 1\\rbrace\\newline\\hspace{1cm} \\cdot \\ T_\\cap&amp;=&amp;\\lbrace (q_1,q_2,i),\\sigma ,(q’_1,q’_2,j)\\mid (q_1,\\sigma,q’_1)\\in T_1,(q_2,\\sigma,q’_2)\\in T_2,i,j\\in\\lbrace 1,2\\rbrace,\\newline\\hspace{1cm} \\ &amp;&amp;\\hspace{5.3cm}q_i \\notin F_i\\rightarrow i=j \\wedge q_i\\in F_i\\wedge i\\neq j\\rbrace\\newline\\hspace{1cm} \\cdot \\ F_\\cap&amp;=&amp;\\lbrace (q_1,q_2,2)\\mid q_1\\in F_1, q_2\\in F_2\\rbrace\\newline\\end{array}$ Explaination $Q_\\cap$ States in the new automaton is tuples that contain the states of two automata plus the third component that used to combine acceptance conditions, e.g. $(q_1,q_2,1)$ is a state. $I_\\cap$ New automaton is constructed with only one initial state, so we assign the value to the acceptance condition {1} for simplicity $T_\\cap$ The switch from $\\lbrace 0\\rbrace$ and $\\lbrace 1\\rbrace$ happens nondeterministically. And once you enter the second copy $\\lbrace 1\\rbrace$, it stays forever. $F_\\cap$ For each transition, it has to be possible by both automata and if such transition reach the one of the automata’s accepting state, the acceptance condition changed. This creates an alternation between the accepting state of the 1st and 2nd automaton. Since we start our acceptance condition as {1}, our accepting states of the new automaton will be the accepting states of the 2nd automaton when acceptance condition becomes {2}. i.e. Whenever the run reach to the accepting states of the 2nd automaton, it always has to first reach the accepting states of the 1st automaton. $\\textbf{Theorem 3.3. } \\textit{If $L_1$ and $L_2$ are Büchi-recognizable, then so is $L_1\\cap L_2$.}$ Formal ProofThe run $r’$ is accepting iff $r_1$ is accepting and $r_2$ is accepting. i.e. $r’=(q_1^0,q_2^0,t^0)(q_1^1,q_2^1,t^1)\\dots$ is a run of $\\mathcal{A}_\\cap$ on an input word $\\alpha$,iff $r_1 = q^0_1 q^1_1\\dots$ is a run of $\\mathcal{A}_1$ on $\\alpha$ and $r_1 = q^0_2 q^1_2\\dots$ is a run of $\\mathcal{A}_2$ on $\\alpha$ Therefore, an accepting run will be: (1) go to states with {1} → (2) reach accepting state of 1st automaton → (3) go to states with {2} → (4) reach accepting state of 2nd automaton → goes back to (1)… Next chapter: Closure Properties of the Büchi-recognizable languages (Concatenations) Further Reading: Closure (mathematics), Intersection (set theory), Union (set theory)","link":"/AGV/agv3-3/"},{"title":"AGV 3.4 -- Closure Properties of the Büchi-recognizable languages (Concatenations)","text":"Previous chapter: Closure Properties of the Büchi-recognizable languages (Intersection and Union) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner In this section, we continue the proof of the closure properties of the Büchi-recognizable languages. Concatenation of regular Langauge and Büchi-recognizable Language $\\textbf{Construction 3.3. } \\text{Let }\\mathcal{A}_1=(\\Sigma,Q_1,I_1,T_1,F_1)\\text{ be an automaton over finite words that}\\newline\\text{recognizes the languge }L_1\\text{, and let }\\mathcal{A}_2 = (\\Sigma,Q_2,I_2,T_2,\\small\\text{BÜCHI}\\normalsize (F_2))\\text{ be a Büchi automaton}\\newline\\text{over the same alphabet that recognizes }L_2.\\text{ We construct a Büchi autotmaton }\\newline\\mathcal{A’}=(\\Sigma,Q’,I’,T’,\\small\\text{BÜCHI}\\normalsize (F_2))\\text{ with }\\mathcal{L}(\\mathcal{A’})=L_1\\cdot L_2\\text{ as follows:}$$\\begin{array}{lrll}\\hspace{1cm} \\cdot &amp;Q’&amp;=&amp;Q_1\\cup Q_2 \\hspace{0.7cm}(\\text{w.l.o.g we assume }Q_1\\cap Q_2=\\varnothing)\\newline\\hspace{1cm} \\cdot &amp;I’&amp;=&amp;\\biggl\\lbrace \\begin{array}{ll}I_1 &amp; \\ \\text{if }I_1\\cap F_1=\\varnothing\\newlineI_1\\cup I_2 &amp; \\ \\text{otherwise}\\end{array} \\newline\\hspace{1cm} \\cdot &amp;T’&amp;=&amp;T_1\\cup T_2\\cup\\lbrace (q,\\sigma,q’)\\mid(q,\\sigma,f)\\in T_1,f\\in F_1,q’\\in I_2\\rbrace \\newline\\end{array}$ Notation Explaination $Q’$ Same as Union, we include both automata for concatenation $I’$ We normaly start the the automaton from $I_1$ If $I_1$ is non-empty, otherwise we start at $I_2$ $T’$ For any states that can reach the accepting states of $T_1$ with one transition, we create a new transitions that reach the initial states of $T_2$ $(I_2)$ The correctness of this construction is proven by the following theorem. $\\textbf{Theorem 3.4. }\\newline\\textit{If $L_1$ is a regular language and $L_2$ is Büchi-recognizable, then $L_1\\cdot L_2$ is Büchi-recognizable.}$ Explained in Human languageIf $I_1$ is non-empty, then we can have a run that starts from $I_1$ to $r(n)$, that is one transition before $\\mathcal{A_1}$’s accepting states of $f$. Then for next transition we either move to $f$, or we move on to $\\mathcal{A_2}$ starting from $r(n+1)$. If $I_1$ is empty, then any word accepted by $\\mathcal{A‘}$ is accepted by $\\mathcal{A_2}$ On the other way, we can always construct a word $w\\alpha$. If $w$ is accepted by $\\mathcal{A_1}$ and $\\alpha$ is accepted by $\\mathcal{A_2}$, then $w\\alpha$ is always accepted by $\\mathcal{A’}$ Formal ProofWe prove that the Büchi automaton $\\mathcal{A’}$ built from the automaton on finite words $\\mathcal{A}_1$ and the Büchi automaton $\\mathcal{A}_2$ indeed recognizes the concatenation of the languages of the two automata. $\\mathcal{L}(\\mathcal{A’})\\subseteq\\mathcal{L}(\\mathcal{A}_1)\\cdot\\mathcal{L}(\\mathcal{A}_2):$ For $\\alpha\\in\\mathcal{L}(\\mathcal{A’})$, we have an accepting run $r=r(0)r(1)r(2)\\dots$ of $\\mathcal{A’}$ on $\\alpha$.If $r(0)\\in I_1$, then there is an $n\\in\\mathbb{N}$ such that $(r(n),\\alpha(n),r(n+1))\\in Q_1\\times\\Sigma\\times I_2$ and therefore, there is a final state $f\\in F_1$ such that $r(0)r(1)r(2)\\dots r(n)f$ is an accepting run of $\\mathcal{A_1}$ on $\\alpha(0)\\alpha(1)\\alpha(2)\\dots \\alpha(n)$ and $r(n+1)r(n+2)\\dots$ is an accepting run of $\\mathcal{A_2}$ on $\\alpha(n+1)\\alpha(n+2)\\dots$If $r(0)\\in I_2$ then $I_1\\cup F_1 \\neq\\varnothing$ and therefore, $\\varepsilon\\in\\mathcal{L}(\\mathcal{A_1}),\\alpha\\in\\mathcal{L}(\\mathcal{A_2})$ $\\mathcal{L}(\\mathcal{A’})\\supseteq\\mathcal{L}(\\mathcal{A}_1)\\cdot\\mathcal{L}(\\mathcal{A}_2):$ For $w\\in\\mathcal{A_1}$, let $r=r(0)r(1)\\dots r(n)$ be an accepting run $\\mathcal{A_1}$ on $w$.For $\\alpha\\in\\mathcal{A_2}$, let $s=s(0)s(1)\\dots$ be an accepting run of $\\mathcal{A_2}$ on $\\alpha$.Then, $r(n)\\in F_1$ and, by construction, $r(0)r(1)\\dots r(n-1)s(0)s(1)\\dots$ is an accepting run of $\\mathcal{A’}$ on $w\\alpha$. Infinite concatenation of Regular languageFinally, we show that the infinite concatenation of words of a regular language forms a Büchi-recognizable language. We construct a Büchi automaton for this language from an automaton over finite words in two steps. From automaton over finite words to a single initial stateFirstly, we modify a given automaton over finite words into an equivalent automaton with one single initial state that has no incoming transitions. We do this by create a new fresh state as the new initial state, with all transitions identical as the original. And the original initial state now has no incoming transitions. $\\textbf{Construction 3.4. } \\text{Let }\\mathcal{A}_1=(\\Sigma,Q_1,I_1,T_1,F_1)\\text{ be an automaton over finite words. We assume}\\newline\\text{that }\\varepsilon\\notin\\mathcal{L}(\\mathcal{A})\\text{. We construct an automaton }\\mathcal{A’} = (\\Sigma,Q’,I’,T’,F)\\text{ over finite words such that }\\newline\\mathcal{L}(\\mathcal{A})=\\mathcal{L}(\\mathcal{A’})\\text{ and }\\mathcal{A’}\\text{ has a single initial state that has no incoming transitions.}$$\\begin{array}{lrll}\\hspace{1cm} \\cdot &amp;Q’&amp;=&amp;Q_1\\cup\\lbrace q_f\\rbrace \\text{where }q_f\\text{ is a fresh state}\\newline\\hspace{1cm} \\cdot &amp;I’&amp;=&amp;\\lbrace q_f\\rbrace\\newline\\hspace{1cm} \\cdot &amp;T’&amp;=&amp;T\\cup\\lbrace (q_f,\\sigma,q’)\\mid(q,\\sigma,q’)\\in T \\text{ for some }q\\in I\\rbrace\\newline\\end{array}$ The second construction builds the Büchi automaton that recognizes the infinite concatenations of words from the regular language by adding a loop to the modified automaton. $\\textbf{Construction 3.5. } \\text{Let }\\mathcal{A}\\text{ be an automaton over finite words. We assume that }\\varepsilon\\notin\\mathcal{L}(\\mathcal{A}).\\newline\\text{We construct a Büchi automaton }\\mathcal{A’’}=(\\Sigma,Q’’,I’’,T’’,\\small\\text{BÜCHI}\\normalsize (F’’))\\text{ such that }\\mathcal{L}(\\mathcal{A’’})=\\mathcal{L}(\\mathcal{A})^\\omega.\\newline\\text{Let }\\mathcal{A’}=(\\Sigma,Q’,I’,T’,F’)\\text{ be the automaton of Construction 3.4. Then }A’’\\text{ is defined as follows:}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ Q’’=Q’ \\hspace{1cm} \\cdot \\ I’’=I’ \\hspace{1cm} \\cdot \\ F’’=I’\\newline\\hspace{1cm} \\cdot \\ T’’=T’\\cup\\lbrace (q,\\sigma,q_f)\\mid(q,\\sigma,q’)\\in T’ \\text{ and }q’\\in F’\\rbrace\\newline\\end{array}$ This construction change the final state into the initial state, with adding new transition from final state to initial state. Therefore the automaton becomes a loop. The correctness of this construction is proven by the following theorem. $\\textbf{Theorem 3.5. }\\textit{If $L$ is a regular language such that $\\varepsilon\\notin L$, then $L^\\omega$ is Büchi-recognizable}$ Explained in Human languageIf there is accepting run for $\\mathcal{L}(\\mathcal{A’’})$, it reaches the accepting state infinitely often.We can split the word in every accepting states and it will be an accepting run for $\\mathcal{L}(\\mathcal{A’})$, since the transition to accepting state $F’’$ and $F’$ are identical. If there is accepting run(s) for $\\mathcal{L}(\\mathcal{A’})$, we can infinitely repeat them so that is also accepted for $\\mathcal{L}(\\mathcal{A’’})$. Formal ProofConstruction 3.4 does not affect the language of $\\mathcal{A}$. We show that the Büchi automaton $\\mathcal{A’’}$ built in Construction 3.5 from the resulting automaton $\\mathcal{A’}$ on finite words indeed recognizes $\\mathcal{L}(\\mathcal{A’})^\\omega$. $\\mathcal{L}(\\mathcal{A’’})\\subseteq\\mathcal{L}(\\mathcal{A’})^\\omega:$ Assume that $\\alpha\\in\\mathcal{L}(\\mathcal{A’’})$ and that $r(0)r(1)r(2)\\dots$ is an accpeting run of $\\mathcal{A’’}$ on $\\alpha$. Hence, we have that $r(i)=q_f\\in F’’=I’$ for inifinitely many indices $i:i_0,i_1,i_2,\\dots$. This provides a sequence of runs of $\\mathcal{A’}$: run $r(0)r(1)\\dots r(i_0-1)q$ on $w_0= \\alpha(0)\\alpha(1)\\dots\\alpha(i_0-1)$ for some $q\\in F’$ run $r(i_0)r(i_0+1)\\dots r(i_i-1)q$ on $w_1= \\alpha(i_0)\\alpha(i_1)\\dots\\alpha(i_1-1)$ for some $q\\in F’$ and so forth. We thus have that $w_k\\in\\mathcal{L}(\\mathcal{A’})$ for every $k\\geq 0$. Hence, $\\alpha\\in\\mathcal{L}(\\mathcal{A’})^\\omega$. $\\mathcal{L}(\\mathcal{A’’})\\supseteq\\mathcal{L}(\\mathcal{A’})^\\omega:$ Assume that $\\alpha =w_0w_1w_2\\dots\\in\\Sigma^\\omega$ such that $w_k\\in\\mathcal{L}(\\mathcal{A’})$ for all $k\\geq 0$. For each $k$, we choose an accepting run $r_k(0)r_k(1)r_k(2)\\dots r_k(n_k)$ of $\\mathcal{A’}$ on $w_k$. Hence, $r_k(0)\\in I’$ and $r_k(n_k)\\in F’$ for all $k&gt;1$. Thus, $$r_0(0)\\dots r_0(n_0-1)r_1(0)\\dots r_1(n_1-1)r_2(0)\\dots r_2(n_2-1)\\dots$$ is an accepting run of $\\mathcal{A’’}$ on $\\alpha$. Hence, $\\alpha\\in\\mathcal{L}(\\mathcal{A’’})$. SummaryNow we proved the closure property holds for Union $(W_1\\cup W_2)$, Intersection $(W_1\\cap W_2)$, Concatenation of regular Langauge and Büchi-recognizable Language $(E+W)$, and Infinite concatenation of Regular language $(E^\\omega)$. In the next section we can start to prove Büchi’s Characterization Theorem. Next chapter: Büchi’s Characterization Theorem Further Reading: Closure (mathematics), Concatenation, Absolute infinite","link":"/AGV/agv3-4/"},{"title":"AGV 3.5 -- Büchi&#39;s Characterization Theorem","text":"Previous chapter: Closure Properties of the Büchi-recognizable languages (Concatenations) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner We are now ready to prove Büchi’s Characterization Theorem, a result from 1962. $\\textbf{Theorem 3.6. }\\text{(Büchi’s Characterization Theorem) }\\newline\\textit{An $\\omega$-language is Büchi-recognizable iff it is $\\omega$-regular.}$ Explained in Human language p.s. Büchi-recognizable means the language can be described by a Büchi automata. $”\\Leftarrow”$We have to prove if the language is $\\omega$-regular, then it is Büchi-recognizable.In section 3.2, we learn that an $\\omega$-regular language contains 3 operators: union of $\\omega$-regular languages, $W_1+W_2$ (proved by Theorem 3.2), infinite concatenation of a non-empty regular language $E^\\omega$ (proved by Theorem 3.4), and concatenation of regular languages and $\\omega$-regular language $E\\cdot W$ (proved by Theorem 3.5) $”\\Rightarrow”$We have to prove if the language is Büchi-recognizable, then it is $\\omega$-regular.Here, we try to construct a Büchi-recognizable using all $\\omega$-regular operators. We begin by constructing a regular language $W_{q,q’}$. Words of the language $w$ is accepted by some finite-word automata, with a pair of state $q,q’\\in Q$ being the initial and accepting states respectively. Then we try to prove that a Büchi-recognizable language $\\mathcal{L}(\\mathcal{A})$ equals to the regular language concatenates (3.5) with the infinite concatenated, non-empty (3.4) starting from the previous accepting state. That is $W_{q,q’}\\cdot(W_{q’,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$. On top of that, we do a set union (3.2) for each pair of initial and accepting states $q,q’\\in Q$, that is $\\bigcup_{q\\in I,q’\\in F}$ Therefore, now we only need to prove that $\\mathcal{L}(\\mathcal{A})=\\bigcup_{q\\in I,q’\\in F}W_{q,q’}\\cdot(W_{q,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$. For the word $\\alpha$ from L.H.S, it reaches $q’\\in F$ infinitely often. We can thus seperate into $r:q\\xrightarrow{w_0}q’\\xrightarrow{w_1}q’\\xrightarrow{w_2}\\dots$The $w$ here refers the word to reach $q’$. We know that Büchi automata is non-empty so $|w_i|&gt;0$ when $i&gt;0$.Therefore, $w_0$ will be a word from $W_{q,q’}$ and the rest will be words for $(W_{q’,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$ because they reach the accepting state $q’\\in F$. For the word $w_0w_1w_2\\dots$ from R.H.S, where $w_0$ refers to $W_{q,q’}$ and the rest refers to $(W_{q’,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$.This is accepted by $\\mathcal{L}(\\mathcal{A})$ because it reaches the accepting state $q’\\in F$ infinitely often. Formal Proof$”\\Leftarrow”$Follows from the closure properties of the Büchi-recognizable languages established by Theorems 3.2, 3.4, 3.5. $”\\Rightarrow”$Given a Büchi automaton $\\mathcal{A}$, we consider for each pair $q,q’\\in Q$ the regular language$$W_{q,q’} = \\lbrace w\\in\\Sigma^*\\mid \\text{finite-word automaton } (\\Sigma,Q,\\lbrace q\\rbrace,T,{q’}) \\text{ accepts }w\\rbrace.$$We claim that $\\mathcal{L}(\\mathcal{A})=\\bigcup_{q\\in I,q’\\in F}W_{q,q’}\\cdot(W_{q,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$. The claim is proven as follows. $\\mathcal{L}(\\mathcal{A})\\subseteq\\bigcup_{q\\in I,q’\\in F}W_{q,q’}\\cdot(W_{q,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$: Let $\\alpha\\in\\mathcal{L}(\\mathcal{A})$. Then there is an accpeting run $r$ of $\\mathcal{A}$ on $\\alpha$, which begins at some $q=r(0)\\in I$ and visits some $q’\\in F$ infinitely often: $$r:q\\xrightarrow{w_0}q’\\xrightarrow{w_1}q’\\xrightarrow{w_2}\\dots$$ where $w_i\\in\\Sigma^*$ for all $i\\ge 0$, $|w_i|&gt;0$ for all $i&gt;0$ and $\\alpha=w_0w_1w_2\\dots$ The notation $q_0\\xrightarrow{w}q_{k+1}$ for some finite word $w=w(0)w(1)\\dots w(k)$ means that there exist states $q_1,\\dots,q_k\\in Q$ s.t. $(q_i,w(i),q_{i+1})\\in T$ for all $0\\leq i \\leq k$. Since $w_0\\in W_{q,q’}$ and $w_k\\in W_{q’,q’}$ for $k&gt;0$, we have that $\\alpha \\in W_{q,q’}\\cdot W_{q’,q’}^\\omega$ for some $q\\in I,q’\\in F$. $\\mathcal{L}(\\mathcal{A})\\supseteq\\bigcup_{q\\in I,q’\\in F}W_{q,q’}\\cdot(W_{q,q’}\\setminus\\lbrace\\varepsilon\\rbrace)^\\omega$: Let $\\alpha=w_0w_1w_2\\dots$ with $w_0\\in W_{q,q’}$ and $w_k\\in W_{q’,q’}$ for some $q\\in I,q’\\in F$ and for all $k&gt;0$. Then the run: $$r:q\\xrightarrow{w_0}q’\\xrightarrow{w_1}q’\\xrightarrow{w_2}\\dots$$ exists and is aceepting because $q’\\in F$. It follows that $\\alpha\\in\\mathcal{L}(\\mathcal{A})$. Next chapter: Deterministic vs. Nondeterministic Büchi Automata Further Reading: Büchi automaton","link":"/AGV/agv3-5/"},{"title":"AGV 4.1 -- Deterministic vs. Nondeterministic Büchi Automata","text":"Previous chapter: Büchi’s Characterization Theorem This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn the theory of automata over finite words, we have Rabin-Scott powerset construction, which converts a nondeterministic automaton over finite words into a deterministic automaton that recognizes the same language. This shows that nondeterminism does not make automata over finite words more expressive (but makes it more concise as the construction produces an exponential number of states). The situation is different for Büchi automata: even though the language $L = (a+b)^*b^\\omega$ is clearly Büchi-recognizable, there is, as the following theorem shows, no deterministic Büchi automaton that recognizes L. Deterministic vs. nondeterministic Büchi automata $\\textbf{Theorem 4.1. }\\textit{Language $L=(a+b)^*b^\\omega$ is not recognizable by deterministic Büchi Automata}$ Starting a base case $b^\\omega$, we add $(a+b)$ as the prefix one by one. Can we express the kleene star?That is, is the automaton only accept finitely many $(a+b)$? ProofAssume, by way of contradiction, that $L$ is recognizable by the deterministic Büchi automaton $\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI}\\normalsize(F))$. Since $\\alpha_0=b^\\omega$ is in $L$, there is an unique run $$r_0=r_0(0)r_0(1)r_0(2)\\dots$$ of $\\mathcal{A}$ on $\\alpha_0$ with $r_0(n_0)\\in F$ for some $n_0\\in\\mathbb{N}$. Similarly, $\\alpha_1=b^{n_0}ab^\\omega$ in $L$ and there is a unique run $$r_1=r_0(0)r_0(1)r_0(2)\\dots r_0(n_0)r_1(n_0+1)r_1(n_0+2)\\dots$$ of $\\mathcal{A}$ on $\\alpha_1$ with $r_1(n_1)\\in F$ for some $n_1&gt;n_0$. Since $\\mathcal{A}$ is deterministic, $r_0$ &amp; $r_1$ are identical up to position $n_0$. By repeating this argument infinitely often, we obtain a word $\\alpha=b^{n_0}ab^{n_1}ab^{n_2}a\\dots$ and a run $r$ with infinitely many visits to $F$. Hence, $\\alpha$ is accepted by $\\mathcal{A}$. However, $\\alpha$ is not an element of $L$. This contradicts $L = \\mathcal{L}(\\mathcal{A})$. Limit OperatorWe start by defining a Limit Operator to generate $\\omega$-language using regular language. For the regular language $W$, $\\overrightarrow{W}$ contains all the words that they all contain words in $W$ as substrings. $\\textbf{Definition 4.1. }\\text{(Limit). The}\\textit{ limit }\\overrightarrow{W}\\text{ of a language }W\\subseteq\\Sigma^* \\text{ over finite words}\\newline\\text{is the following language over infinite words:}$ $$\\overrightarrow{W}=\\lbrace\\alpha\\in\\Sigma^\\omega\\mid\\text{there exist infinitely many }n\\in\\mathbb{N}\\text{ s.t. }\\alpha[0,n]\\in W\\rbrace.$$ And From regular language, now we can define an $\\omega$-regular language that is recognizable by deterministic Büchi Automata: $\\textbf{Theorem 4.2. }\\textit{An }\\omega\\textit{-language }L\\subseteq\\Sigma^\\omega\\textit{ is recognizable by a deterministic Büchi Automata}\\newline\\textit{iff there is a regular langauge }W\\subseteq\\Sigma^*\\textit{ s.t. }L=\\overrightarrow{W}.$ Explained in Human languageBasically, we are trying to prove $\\mathcal{L}(\\mathcal{A_B})=\\overrightarrow{\\mathcal{L}(\\mathcal{A_F})}$, where $\\mathcal{L}(\\mathcal{A_B})$ is a deterministic Büchi Automata and $\\mathcal{L}(\\mathcal{A_F})$ is a a deterministic automaton over finite words. As we can see, an accepted word $\\alpha\\in\\mathcal{L}(\\mathcal{A_B})$ with have substring $\\alpha[0,n]$ that is accepted by $\\mathcal{L}(\\mathcal{A_F})$ with infinitely many $n\\in\\mathbb{N}$, by the definition of the limit operator. We can prove regular language $W$ exists if $\\alpha$ exists using $\\overrightarrow{W}$. Automata that accepts $W$ and $\\overrightarrow{W}$ should exist, namely $\\mathcal{L}(\\mathcal{A_F})$ and $\\overrightarrow{\\mathcal{L}(\\mathcal{A_F})}$ respectively. Formal ProofWe claim that the languages of a deterministic Büchi automaton $\\mathcal{A}_B$ and of a deterministicautomaton over finite words $\\mathcal{A}_F$, where the automata $\\mathcal{A}_B=(\\Sigma,Q,I,T,\\small\\text{BÜCHI}\\normalsize(F))$ and $\\mathcal{A}_F=(\\Sigma,Q,I,T,F)$, constructed from the same components, are related as follows: $$\\mathcal{L}(\\mathcal{A_B})=\\overrightarrow{\\mathcal{L}(\\mathcal{A_F})}.$$ Since every regular language is recognized by a deterministic automaton over finite words,the theorem follows. To prove the claim, we consider an infinite word $\\alpha\\in\\Sigma^\\omega$. $\\hspace{1cm}\\alpha\\in\\mathcal{L}(\\mathcal{A_B})\\newline\\text{iff} \\hspace{0.5cm}\\text{for the unique run $r$ of $\\mathcal{A_B}$ on $\\alpha$, Inf($r$) $\\cap$ $F\\neq\\varnothing$}\\newline \\text{iff} \\hspace{0.5cm} \\alpha[0,n]\\in\\mathcal{L}(\\mathcal{A_F})\\text{ for infinitely many }n\\in\\mathbb{N}\\newline\\text{iff} \\hspace{0.5cm} \\alpha\\in\\mathcal{L}(\\mathcal{A_F})$ Next chapter: Complementation of deterministic Büchi Automata Further Reading: powerset construction","link":"/AGV/agv4-1/"},{"title":"AGV 4.2 -- Complementation of deterministic Büchi Automata","text":"Previous chapter: Deterministic vs. Nondeterministic Büchi Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionFor regular languages, Complementation is very simple: one translates a given complete deterministic automaton $\\mathcal{A}$ that recognizes some language $L\\subseteq\\Sigma^*$ into an automaton $\\mathcal{A’}$ that recognizes the complement $\\Sigma^*\\setminus L$ by complementing the set of final states, i.e., $F’=Q\\setminus F$. For deterministic Büchi automata, the construction is tricky, because it introduces nondeterminism $\\textbf{Construction 4.1. }\\text{Let }\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))\\text{ be a complete deterministic Büchi}\\newline\\text{automaton, where we assume w.l.o.g. that } Q\\neq\\varnothing. \\text{ We construct a Büchi automaton}\\newline\\mathcal{A’}=(\\Sigma,Q’,I’,T’,\\small\\text{BÜCHI} \\normalsize (F’))\\text{ with }\\mathcal{L}(\\mathcal{A’})=\\Sigma^\\omega\\setminus\\mathcal{L}(\\mathcal{A})\\text{ as follows:}$$\\begin{array}{lrll}\\hspace{1cm} \\cdot &amp;Q’&amp;=&amp;(Q\\times\\lbrace 0\\rbrace)\\cup((Q\\setminus F)\\times\\lbrace 1\\rbrace)\\newline\\hspace{1cm} \\cdot &amp;I’&amp;=&amp;I\\times\\lbrace 0\\rbrace\\newline\\hspace{1cm} \\cdot &amp;T’&amp;=&amp;\\lbrace ((q,0),\\sigma,(q’,i))\\mid(q,\\sigma,q’)\\in T,i\\in\\lbrace 0,1\\rbrace,(q’,i)\\in Q’\\rbrace \\cup \\newline\\hspace{1cm} \\ &amp;&amp;&amp;\\lbrace ((q,1),\\sigma,(q’,1))\\mid(q,\\sigma,q’)\\in T,q’,q’\\in Q\\setminus F\\rbrace\\newline\\hspace{1cm} \\cdot &amp;F’&amp;=&amp;(Q\\setminus F)\\times\\lbrace 1\\rbrace\\end{array}$ Explaination $Q’$ Uses two copies of the given automaton $\\mathcal{A}$, mark them seperately using $\\lbrace 0\\rbrace$ and $\\lbrace 1\\rbrace$, notice that all accpeting states from $\\lbrace 1\\rbrace$ are eliminated $I’$ The complemented automaton starts from initial states from $\\lbrace 0\\rbrace$. $T’$ The switch from $\\lbrace 0\\rbrace$ and $\\lbrace 1\\rbrace$ happens nondeterministically. And once you enter the second copy $\\lbrace 1\\rbrace$, it stays forever. $F’$ The automaton $\\mathcal{A’}$ accepts if the run ends up in the second copy, which means that, on the unique run of $\\mathcal{A}$ on the input word, the accepting states of $\\mathcal{A}$ are only visited finitely often. Note that the resulting automaton is nondeterministic. This is, in general, unavoidable. This is because there are languages, such as $L=(b^*a)^\\omega$ where the language itself is recognizable by a deterministic Buchi automaton, while its complement $\\Sigma^\\omega\\setminus L$ can only be recognized by a nondeterministic Büchi automaton. $\\textbf{Theorem 4.3. }\\textit{For every deterministic Büchi automaton }\\mathcal{A}\\textit{, there exists a Büchi automaton }\\mathcal{A’}\\newline\\textit{ such that }\\mathcal{L}(\\mathcal{A’})=\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A}).$ Explained in Human Language$\\mathcal{L}(\\mathcal{A’})\\subseteq\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A}):$ To prove that “If $\\alpha$ is accepted by $\\mathcal{L}(\\mathcal{A’})$, then it is also accepted by the complement of $\\mathcal{L}(\\mathcal{A})$”, we can show that every state in $\\mathcal{A’}$ is same as $\\mathcal{A}$, but as long as it switched to $\\lbrace 1\\rbrace$, it stays forever and there is no accepting states of $\\mathcal{A}$. Therefore, every accepted word in $\\mathcal{L}(\\mathcal{A’})$ is also accepted in $\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A})$. $\\mathcal{L}(\\mathcal{A’})\\supseteq\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A}):$ To prove that “If $\\alpha$ is accepted by the complement of $\\mathcal{L}(\\mathcal{A})$, then it is also accepted by $\\mathcal{L}(\\mathcal{A’})$”, we assume there’s a word $\\alpha$ that is not accepted by $\\mathcal{L}(\\mathcal{A})$. Since $\\mathcal{A}$ is complete and deterministic, so the run on $\\alpha$ is still infinite, yet never visit the accepted states infinitely often. i.e. starting from certain position, it will never visit the accepting states anymore. We can treat that position as the switch from $\\lbrace 0\\rbrace$ and $\\lbrace 1\\rbrace$ in $\\mathcal{A’}$ occurs. And therefore $\\alpha$ is an accepted word in $\\mathcal{L}(\\mathcal{A’})$. Formal ProofLet $\\mathcal{A’}$ be constructed from the given deterministic Büchi automaton $\\mathcal{A}$ (which weassume, w.l.o.g., to be complete) by Construction 4.1. We prove that $\\mathcal{L}(\\mathcal{A’})=\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A}).$ $\\mathcal{L}(\\mathcal{A’})\\subseteq\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A}):$ For every word $\\alpha\\in\\mathcal{L}(\\mathcal{A’})$ we have an accepting run: $r’:(q_0,0)(q_1,0)\\dots(q_j,0)(q’_0,1)(q_1,1)\\dots$ on $\\mathcal{A}$. Hence, $r:q_0q_1\\dots q_jq_0’q_1’\\dots$ is the unique run of $\\mathcal{A}$ on $\\alpha$. Since $q_0’,q_1’,\\dots\\in Q\\setminus F$, we have that $\\text{Inf}(r)\\subseteq Q\\setminus F$. Hence, $r$ is not accepting and $\\alpha\\in\\Sigma^\\omega\\setminus\\mathcal{L}(\\mathcal{A})$. $\\mathcal{L}(\\mathcal{A’})\\supseteq\\Sigma^\\omega \\setminus\\mathcal{L}(\\mathcal{A}):$ Let $\\alpha\\notin\\mathcal{L}(\\mathcal{A’})$ be some word that is not in the language of $\\mathcal{A}$. Since $\\mathcal{A}$ is complete and deterministic, there exists a unique run $r:q_0q_1q_2\\dots$ of $\\mathcal{A}$ on $\\alpha$ and $\\text{Inf}(r)\\cap F=\\varnothing$. Thus, there exists a $k\\in\\mathbb{N}$ such that $q_j\\notin F$ for all $j &gt; k$. This gives us the run: $r’:(q_0,0)(q_1,0)\\dots(q_j,0)(q’_0,1)(q_1,1)\\dots$ of $\\mathcal{A’}$ on $\\alpha$ with $\\text{Inf}(r)\\subseteq ((Q\\setminus F)\\times\\lbrace 1\\rbrace)=F’$. Hence, $r’$ is accepting and therefore $\\alpha\\in\\mathcal{L}(\\mathcal{A’})$. Example: $\\mathcal{L}(\\mathcal{A})=(b^\\ast a)^\\omega$ and $\\mathcal{L}(\\mathcal{A’})=(a+b)^\\ast b^\\omega$ Note that, since not all $\\omega$-regular languages can be recognized by deterministic Büchi automata, Construction 4.1 does not provide us with a complementation construction for all $\\omega$-regular languages. Such a general construction is the subject of the following section. Next chapter: Infinite Directed Acyclic Graph (DAG) Further Reading:","link":"/AGV/agv4-2/"},{"title":"AGV 5.1 -- Infinite Directed Acyclic Graph (DAG)","text":"Previous chapter: Complementation of deterministic Büchi Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Reasoning about all runs of an automatonSince complementation inevitably introduce nondeterminism, we need to check whether all runs of the automaton on the word are rejecting to determine whether a word is in the complement of the language recognized by this Büchi automaton. ExampleConsider the following nondeterministic Büchi automaton $\\mathcal{A}$. Its language consists of all infinite words over $\\lbrace a,b\\rbrace$ with infinitely many bs, i.e., $(a^*b)^\\omega$: $\\textbf{Definition 5.1. }\\text{Let }\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))\\text{ be a Büchi automaton. The run DAG of }\\mathcal{A}\\newline\\text{on a word }\\alpha\\in\\Sigma^\\omega\\text{ is the directed acylic graph }G=(V,E)\\text{, where}$$\\begin{array}{lll}\\hspace{1cm} \\cdot \\ V&amp;=&amp;\\cup_{i\\geq0}(Q_i\\times\\lbrace i\\rbrace)\\text{ with }Q_0=I\\text{ and}\\newline\\hspace{1.1cm} \\ Q_{i+1}&amp;=&amp;\\lbrace q’\\mid(q,\\alpha(i),q’)\\in T\\text{ for some }q\\in Q\\rbrace\\newline\\hspace{1cm} \\cdot \\ E&amp;=&amp;\\lbrace((q,i),(q’,i+1))\\mid i\\geq 0,(q,\\alpha(i),q’)\\in T\\rbrace.\\end{array}$$\\text{For a natural number }i\\text{, we refer to the set }Q_i\\text{ as the }\\textit{level }i\\text{ of the DAG.}$ Using infinite directed acyclic graph (DAG) to represent the set of all runs on a particular word, e.g., $ababa^\\omega$. In the above example: the word $\\alpha=ababa^\\omega$ has only two b‘s and hence must be rejected by $\\mathcal{A}$. Since the automaton is nondeterministic, a single word of $\\mathcal{A}$ will have multiple runs on $\\alpha$, represented by each path in DAG. The path is called accepting if and only if its corresponding run is accepting, i.e., the path visits $F\\times\\mathcal{A}$ infinitely often. Graph PruningTo show the non-acceptance of $\\alpha$ by $\\mathcal{A}$, we can systematically identifying and pruning away vertices of the run DAG that only lead to rejecting paths until the graph is empty using the following definitions: Vertices in DAG Defintiion Endangered when they only have finitely many descendants Safe when they are not in $F\\times\\mathcal{A}$, and none of its descendants are in $F\\times\\mathcal{A}$ either We start with a graph $G_0=G$. Each iteration $j\\geq 0$ of our pruning will consist of two steps: in step $2j$, the graph = $G_{2j}$, remove all endangered vertices, graph after removal = $G_{2j+1}$ in step $2j+1$, the graph = $G_{2j+1}$, remove all safe vertices, graph after removal = $G_{2j+2}$ Step 0: Remove endangered vertices Input: $G_0$ ↑ (using example above), Output: $G_1$ ↓ Since there’s no endangered verticies, $G_0$ = $G_1$ Step 1: Remove safe vertices (marked as blue) Input: $G_1$ ↑, Output: $G_2$ ↓ As we can see all vertices that in state r stay in r forever (accepting state is q). Therefore those path can never be accepted and they are safe. Step 2: Remove endangered vertices (marked as red) Input: $G_2$ ↑, Output: $G_3$ ↓ After safe verticies in r are removed, paths towards r from q become dead end. Therefore they are now endangered. Step 3: Remove safe vertices (marked as blue) Input: $G_3$ ↑, Output: $G_4$ ↓ Now all the vertices starting from (p,3) stay in p forever. Therefore they are safe as well. Step 4: Remove endangered vertices (marked as red) Input: $G_4$ ↑, Output: $G_5$ ↓ Now the graph is finite, meaning all the vertices in the graph are endangered. Therefore, graph $G_5$ will be an empty graph, which means there’s no accepting path and thus the word $ababa^\\omega$ is rejected. Explained in Human languageWe start from removing endangered verticies in step 0 because they can never infinitely reach the accepting states. Then we can remove all Safe verticies in step 1 because they never visit accepting states anymore. For the subsequent pruning, if a vertex v of the original run DAG is pruned away in step $2j$ because it is endangered in $G_{2j}$, it means that all paths from v in the original run DAG can reach were pruned away in the earlier steps. Similarly, if a vertex v of the original run DAG is pruned away in step $2j+1$ because it is safe in $G_{2j+1}$, it means that v corresponds to a non-accepting state. And furthermore, all paths from v in the original run DAG either avoid accepting vertices, or eventually reach a vertex that was pruned away in the earlier steps. So we can see that if a vertex is pruned away, all paths from that vertex are rejecting. Hence, if our scheme eventually obtains the empty graph, the pruning is a proof of the non-acceptance of the word by the automaton. We formalize this type of reasoning as a ranking, which labels the vertices with numbers. We will define ranking in the next chapter. Next chapter: Ranking of DAG Further Reading:","link":"/AGV/agv5-1/"},{"title":"AGV 5.2 -- Ranking of DAG","text":"Previous chapter: Infinite Directed Acyclic Graph (DAG) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Rankinglast section, we have introduced the DAG and the pruning method to reason about the run of the word on a non-deterministic Büchi automaton. We know that pruning can be use to prove the non-acceptance of the word by automaton is we obtains an empty graph eventually. In this section, we will introduce ranking, which can formalize our pruning construction. $\\textbf{Definition 5.2. }\\text{(Ranking). A }\\textit{ranking}\\text{ on a run DAG } G=(V,E)\\text{ is a function }f:V\\rightarrow\\newline\\lbrace 0,\\dots,2|Q|\\rbrace\\text{ such that:}$$\\begin{array}{l}\\hspace{1cm} 1. \\ \\text{ If }q\\in F\\text{, then }f((q,i))\\text{ is even for all }i.\\newline\\hspace{1cm} 2. \\ \\text{ If }(v,v’)\\in E\\text{, we have that }f(v’)\\leq f(v).\\end{array}\\newline$$\\text{A ranking } f\\text{ is }\\textit{odd}\\text{ if, for each even }j\\text{, there is no infinite path in }G\\text{ that consists only }\\newline\\text{of verticies }v\\text{ such that }f(v)=j.$ By the above definition, we can see: All verticies that contains state q (the accepting state) are marked as even rank. Verticies will never have lower rank than their successors. Most importantly, it introduced a new concept: Odd Ranking. Meaning that the run of the DAG is rejected. From Ranking to RankNow, let’s consider the function $rank:V\\rightarrow\\lbrace 0,\\dots,2|Q|\\rbrace$, where $rank(q,i)=2j$ iff $(q,i)$ is endangered in $G_{2j}$, and $rank(q,i)=2j+1$ iff $(q,i)$ is safe in $G_{2j+1}$. By tracking our pruning procedure, we can easily identify their rank now.Blue verticies that remove in $G_1$ are safe, thus their rank are 1, and so as orange (2), green (3) and red (4). From Rank to Odd RankingNow back to odd ranking, by our new function $rank$, we can prove the following lemma. $\\textbf{Lemma 5.1. }\\newline\\textit{A Büchi automaton }\\mathcal{A}\\textit{ reject the word }\\alpha\\textit{ iff the run DAG of }\\mathcal{A}\\textit{ on }\\alpha\\textit{ has an odd ranking}$ Proof If the run DAG of $\\mathcal{A}$ on $\\alpha$ has an odd ranking, then $\\alpha$ is rejected By the definition above, we know that graph with odd ranking has no infinite path that settles on only even rank $f(v)=j$. Which means: every run of $\\mathcal{A}$ on $\\alpha$ gets stuck, or, eventually settles on vertices with a constant odd rank, since ranks are not increasing. In which case the run eventually only visits states that are not accepting. If $\\alpha$ is rejected, then the run DAG of $\\mathcal{A}$ on $\\alpha$ has an odd ranking Since we know pruning leads to rejecting with function $rank$. Therefore, instead of using odd ranking directly, we try to show the function $rank$ is the odd ranking. To prove this, we need to show that: All vertices are indeed pruned away, latest by step $2|Q|$. Accepting vertices must have an even rank The rank cannot increase upon traversing an edge. There is no infinite path that consists only of vertices of even rank. Requirement 1: All vertices are indeed pruned away, latest by step $2|Q|$Here, we define the width of a level $j$ in a graph $G_i$ as the number of vertices of the form $(q,j)$ in $G_i$. Assume we are in step $2j$, it removes all endangered vertices from $G_{2j}$. Then how would $G_{2j+1}$ become? become empty (all vertices in $G_{2j}$ are endangered) still has infinitely many vertices(by definition, vertices that not endangered always have some infinite path, also because $\\infty-n=\\infty$) In scenario 2, since our premise is that the word is rejected, there must exist a safe vertex in $G_{2j+1}$. If there’s no safe vertex, then those vertices are either accepting, or has a path to accepting vertex, which contradicts to our premise. a) Each iteration there must be at least one safe vertex being removed Among all the safe vertices in $G_{2j+1}$, consider the vertex $(q,m)$ which has the smallest $m$ (closest to the beginning). All descendants of this vertex in $G_{2j+1}$ are also safe (because safe refers to the whole path). Therefore, these safe vertices will be pruned away in step $2j+1$. Observe the affect towards the width after these deletions: At the beginning of iteration $0$, each level has width at most $|Q|$ (max. no. of states). In every iteration $j$, the widths of all levels $i\\geq m$ decrease by at least 1.(because vertex $(q,m)$ and all its descendants in $i\\geq m$ are removed) b) If one safe vertex is removed, all its descendants will also be removed By a) and b), we can ensure that after $|Q|$ iterations of safe vertices removal there must be some level $m$ contains no more vertices, which means now the graph terminate in $m$ and it is a finite graph. By including the iteration of endangered vertices, we can conclude that in step $2|Q|$, $G_{2|Q|}$ must be a finite graph, all vertices are endangered, thus it will prune all vertices away. Requirement 2: Accepting vertices must have an even rankSince accepting vertices are not safe, and our premise it that the word is rejected. That means the path contains accepting verticeis can only be finite, which means they are always endangered. In our rank function, endangered vertices have even rank and thus accepting vertices always have even rank. Requirement 3: The rank cannot increase upon traversing an edgeSuppose there is a $(q’,i+1)$ that is pruned later than $(q,i)$. Consider the step $j$ at which $(q,i)$ is pruned. The graph $G_j$ would have contained both vertices, along with their edge. If $j$ is even, it means that $(q,i)$ has even rank, and endangered in $G_j$ , and hence so must be its successor $(q’,i+1)$, meaning that it is pruned at step $j$. If $j$ is odd, then $(q,i)$ was established to be safe in Gj , implying the safety of its successor too. In both cases, the pruning of $(q’,i+1)$ in the same step is inevitable. Requirement 4: There is no infinite path of only even-ranked verticesIf such a path exists, since ranks do not increase along a path, and there are only finitely many ranks, this path will eventually consist only of vertices of rank $2j$. However, only endangered verticies will have even rank in $G_{2j}$, and endangered vertices only have finite paths. ConclusionWith the above properties and proofs, we know that: the run DAG of $\\mathcal{A}$ on $\\alpha$ has an odd ranking = $\\alpha$ is rejected Pruning method with $rank$ function = odd ranking So if a word is rejected, its run DAG always becomes an empty graph after pruning, and if a run DAG eventually becomes an empty graph after pruning, it must be rejected by $\\mathcal{A}$. With these definition, we can now construct complementation of Büchi automaton $\\mathcal{A}$ using the run DAG of $\\mathcal{A}$ and the odd ranking. Let see how is it done in the next section. Next chapter: Complement Büchi Automaton with Odd Ranking Further Reading:","link":"/AGV/agv5-2/"},{"title":"AGV 6.1 -- Linear-Time Temporal Logic (LTL)","text":"Previous chapter: Complement Büchi Automaton with Odd Ranking This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn the following sections, we introduce several logics for the specification of sets of infinite sequences: LTL, QPTL, and S1S, which can be used to describe $\\omega$-regular languages: we can translate a given formula into an automaton that recongnizes the models of the formula. As a result, we can use the automata-theoretic machinery to answer logical questions like satisfiability or validity. We can also use the logics as a much more convenient starting point, compared to a direct specification using automata, for verification and synthesis. Linear-Time Temporal Logic (LTL)Linear-time temporal logic (LTL) is a popular logic for the specification of reactive systems. For a given set of atomic propositions$(AP)$, the formulas of LTL define sets of infinite words over the alphabet $2^{AP}$. Atomic Propositions:the interface of a system or a component, such as (a boolean representation of) input and output variables Words defined by the Formula:the executions of the system that are considered correct (see example in section 1.1). Syntax of LTLφ has to be true until and including the point where ψ first becomes true; if ψ never becomes true, φ must remain true forever. LTL formulars are constructed from proposition logic with extra temporal operators. $\\begin{array}{lrl} \\ \\textit{Propositional logic}\\newline\\hspace{1cm} \\cdot \\ p,\\varphi,\\dots\\in AP&amp;&amp; \\ \\text{(Atomic Propositions)}\\newline\\hspace{1cm} \\cdot \\ \\neg\\varphi,\\varphi\\wedge\\psi,\\dots&amp;&amp; \\ \\text{(Logical Operators)}\\newline \\ \\textit{Temporal Operator}\\newline\\hspace{1cm} \\cdot \\bigcirc\\varphi \\ /\\ \\mathcal{X}\\ \\varphi&amp; \\ \\text{Ne}\\textbf{X}\\text{t: }&amp;\\varphi\\text{ has to be }\\textit{true}\\text{ in next state}\\newline\\hspace{1cm} \\cdot \\ \\Diamond \\ \\varphi \\ /\\ \\mathcal{F}\\ \\varphi&amp; \\ \\textbf{F}\\text{inally: }&amp;\\varphi\\text{ has to be }\\textit{true}\\text{ eventually}\\newline\\hspace{1cm} \\cdot \\ \\square \\ \\varphi \\ /\\ \\mathcal{G}\\ \\varphi&amp; \\ \\textbf{G}\\text{lobally: }&amp;\\varphi\\text{ has to be }\\textit{true}\\text{ for now and so on}\\newline\\hspace{1cm} \\cdot \\ \\varphi \\ \\mathcal{U} \\ \\psi&amp; \\ \\textbf{U}\\text{ntil: }&amp;\\varphi\\text{ has to be }\\textit{true}\\textit{ at least }\\text{until }\\psi\\text{ becomes }\\textit{true}\\newline\\hspace{1cm} \\cdot \\ \\varphi \\ \\mathcal{R} \\ \\psi&amp; \\ \\textbf{R}\\text{elease: }&amp;\\psi\\text{ has to be }\\textit{true}\\text{ until }\\varphi\\text{ becomes true (inclusive)}\\newline&amp;&amp;\\psi\\text{ remains }\\textit{true}\\textit{ forever}\\text{ if } \\varphi\\text{ never becomes }\\textit{true}\\newline\\hspace{1cm} \\cdot \\ \\varphi \\ \\mathcal{W} \\ \\psi&amp; \\ \\textbf{W}\\text{eak until: }&amp;\\varphi\\text{ has to be }\\textit{true}\\textit{ at least }\\text{until }\\psi\\text{ becomes }\\textit{true}\\text{, or}\\newline&amp;&amp;\\varphi\\text{ remains }\\textit{true}\\textit{ forever}\\text{ if }\\psi\\text{ never becomes }\\textit{true}\\newline\\hspace{1cm} \\cdot \\ \\varphi \\ \\mathcal{M} \\ \\psi&amp; \\ \\textbf{M}\\text{ighty Release: }&amp;\\psi\\text{ has to be }\\textit{true}\\text{ until }\\varphi\\text{ becomes true (inclusive)}\\end{array}$ RemarksWhen we determine whether a LTL formula is $\\textit{true}$ or $\\textit{false}$, it only depends on current and future states. For example, the formula $\\varphi\\ \\mathcal{U}\\ \\psi$ states that $\\varphi$ must remain $\\textit{true}$ UNTIL $\\psi$ become $\\textit{true}$. If the current state we have only $\\psi=\\textit{true}$, this formula is still evaluated as $\\textit{true}$, even if $\\varphi$ never become $\\textit{true}$. We can conclude that, $\\varphi\\ \\mathcal{U}\\ \\psi = \\textit{true}$ if: In current state $\\psi$ becomes $\\textit{true}$ (regardless of $\\varphi$’s condition) Eventually $\\psi$ becomes $\\textit{true}$ in certain state (not now), and from now Until that state, $\\varphi$ must remain $\\textit{true}$ Sematics of LTLAdditional operators$\\mathcal{R}$, $\\Diamond\\ (\\mathcal{F})$, $\\square\\ (\\mathcal{G})$ are considered as additional operators, which can be defined by other LTL operators: $\\varphi\\ \\mathcal{R}\\ \\psi\\equiv\\neg(\\neg\\varphi\\ \\mathcal{U}\\ \\neg\\psi)$ $\\square\\ \\varphi \\equiv\\neg\\ \\Diamond\\ \\neg\\ \\varphi\\equiv\\ \\textit{false}\\ \\mathcal{R}\\ \\varphi$ $\\Diamond\\ \\varphi\\equiv\\textit{true} \\ \\mathcal{U}\\ \\varphi$ Weak until $\\mathcal{W}$ and Mighty release $\\mathcal{M}$Weak Until is Until that accepts no stop conditions Forever, or Release that include itself as the stop conditions: $\\varphi\\ \\mathcal{W}\\ \\psi\\equiv(\\varphi\\ \\mathcal{U}\\ \\psi) \\vee \\square\\ \\psi\\equiv\\psi\\ \\mathcal{R}\\ (\\psi\\vee\\varphi)$ Mighty release is forced to Release Finally, or simply Until both propositions becomes $\\textit{true}$: $\\varphi\\ \\mathcal{W}\\ \\psi\\equiv(\\varphi\\ \\mathcal{R}\\ \\psi) \\wedge \\Diamond\\ \\varphi\\equiv\\psi\\ \\mathcal{U}\\ (\\psi\\wedge\\varphi)$ Distributivity$\\bigcirc\\ (\\mathcal{X})$, $\\Diamond\\ (\\mathcal{F})$, $\\square\\ (\\mathcal{G})$, $\\mathcal{U}$ satisfied distributivity: $\\bigcirc(\\varphi\\vee\\psi)\\equiv(\\bigcirc\\varphi)\\vee(\\bigcirc\\psi)$ $\\bigcirc(\\varphi\\wedge\\psi)\\equiv(\\bigcirc\\varphi)\\wedge(\\bigcirc\\psi)$ $\\Diamond(\\varphi\\vee\\psi)\\equiv(\\Diamond\\varphi)\\vee(\\Diamond\\psi)$ $\\square(\\varphi\\wedge\\psi)\\equiv(\\square\\varphi)\\wedge(\\square\\psi)$ $\\rho\\ \\mathcal{U}\\ (\\varphi\\vee\\psi)\\equiv(\\rho\\ \\mathcal{U}\\ \\varphi)\\vee(\\rho\\ \\mathcal{U}\\ \\psi)$ $(\\varphi\\wedge\\psi)\\ \\mathcal{U}\\ \\rho\\equiv(\\varphi\\ \\mathcal{U}\\ \\rho)\\wedge(\\psi\\ \\mathcal{U}\\ \\rho)$ $\\bigcirc(\\varphi\\ \\mathcal{U}\\ \\psi)\\equiv(\\bigcirc\\varphi)\\ \\mathcal{U}\\ (\\bigcirc\\psi)$ Negation Dual First of all, NeXt is a self dual:$\\neg\\bigcirc\\varphi\\equiv\\bigcirc\\neg\\varphi$ (Not in next step = Next step won’t happened) Finally and Globally are dual:$\\neg\\Diamond\\varphi\\equiv\\square\\neg\\varphi$ (Never happened eventually = Forever never happened)$\\neg\\square\\varphi\\equiv\\Diamond\\neg\\varphi$ (Won’t happen forever = eventually won’t happen anymore) Until and Release are dual:$\\neg(\\varphi\\ \\mathcal{U}\\ \\psi)\\equiv\\neg\\varphi\\ \\mathcal{R}\\ \\neg\\psi$ ($\\psi$ won’t happen until $\\varphi$ stops = $\\psi$ can’t happen unless $\\varphi$ stops)$\\neg(\\varphi\\ \\mathcal{R}\\ \\psi)\\equiv\\neg\\varphi\\ \\mathcal{U}\\ \\neg\\psi$ (Never stop $\\psi$ with $\\varphi$ = $\\varphi$ never happens, if it does, that means $\\psi$ has been stopped) Similarly, Weak Until and Mighty Release are also dual:$\\neg(\\varphi\\ \\mathcal{W}\\ \\psi)\\equiv\\neg\\varphi\\ \\mathcal{M}\\ \\neg\\psi$$\\neg(\\varphi\\ \\mathcal{M}\\ \\psi)\\equiv\\neg\\varphi\\ \\mathcal{W}\\ \\neg\\psi$ Special temporal properties $\\Diamond\\varphi\\equiv\\Diamond\\Diamond\\varphi$(Finally Finally $\\varphi$ is $\\textit{true}$) $\\square\\varphi\\equiv\\square\\square\\varphi$($\\varphi$ is always always $\\textit{true}$) $\\varphi\\ \\mathcal{U}\\ \\psi\\equiv\\varphi\\ \\mathcal{U}\\ (\\varphi\\ \\mathcal{U}\\ \\psi)$($\\varphi$ is $\\textit{true}$ Until $\\psi$ become $\\textit{true}$ = $\\varphi$ is $\\textit{true}$ Until, if “$\\varphi$ is $\\textit{true}$ Until $\\psi$ become $\\textit{true}$” is $\\textit{true}$) $\\Diamond\\varphi\\equiv\\varphi\\vee \\bigcirc(\\Diamond\\varphi)$( $\\varphi$ Finally becomes $\\textit{true}$ = either now $\\varphi$ is $\\textit{true}$ or in neXt step $\\varphi$ Finally becomes $\\textit{true}$) $\\square\\varphi\\equiv\\varphi\\wedge \\bigcirc(\\square\\varphi)$( $\\varphi$ is always $\\textit{true}$ = now $\\varphi$ is $\\textit{true}$ and in neXt step $\\varphi$ is also always $\\textit{true}$) SummaryIn this section, we have go through basic sytanx and properties of LTL. In next section we will try to apply the formula to some systems and $\\omega$-languages. Next chapter: Expressing Program Properties using LTL Further Reading:","link":"/AGV/agv6-1/"},{"title":"AGV 6.2 -- Expressing Program Properties using LTL","text":"Previous chapter: Linear-Time Temporal Logic (LTL) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn this section, we consider again our concurrent program $\\small{\\text{TURN}}$ introduced in section 1.1. As discussed before, a major property of interest is mutual exclusion,i.e., at any given point of time, at most one process is in the $\\text{critical}$ region. We can express mutual exclusion, as well as other properties of turn, in LTL. $\\textbf{Example 1.1. }\\small{\\text{TURN}}:$$$\\text{local $t$: boolean where initially $t$ = $false$}\\newlineP_0::\\left[ \\begin{array}{l}\\text{loop forever do}\\newline\\hspace{1cm}\\left[ \\begin{array}{l}\\ell_0: \\text{await }\\neg t; \\newline\\ell_1: \\text{critical;} \\newline\\ell_2: t := true; \\newline\\end{array} \\right]\\end{array} \\right]\\mid\\mid P_1::\\left[ \\begin{array}{l}\\text{loop forever do}\\newline\\hspace{1cm}\\left[ \\begin{array}{l}m_0: \\text{await } t; \\newlinem_1: \\text{critical;} \\newlinem_2: t := false; \\newline\\end{array} \\right]\\end{array} \\right]$$ Properties of the Concurrent Program $\\small{\\text{TURN}}$Mutual exclusion : $\\square\\neg(\\ell_1\\wedge m_1)$ $\\ell_{1}$ and $m_1$ cannot ($\\neg$) be true at the same time ($\\wedge$) at every point in time ($\\square$) The property can equivalently be formulated as $\\neg(\\textit{true}\\ \\mathcal{U}\\ (\\ell_1\\wedge m_1))$, which means that the system remains true, until the a violation of mutual exclusion, i.e., $\\ell_1\\wedge m_1$, occurs Finite waiting : $\\square((\\ell_0\\rightarrow\\Diamond\\ell_1)\\wedge(m_0\\rightarrow\\Diamond m_1))$ If ($\\rightarrow$) $\\ell_0$ is reached, eventually ($\\Diamond$) it moves to $\\ell_1$ and so as $m_0$ and $m_1$, this happens forever ($\\square$) In other words, each process only waits a finite amount of time (in locations $\\ell_0$ and $m_0$, respectively) until it enters the $\\text{critical}$ region (in locations $\\ell_1$ and $m_1$, respectively): This property is a Liveness conditions, usually involve infinite number of steps. Usually it is only meaningful when it is held under additional assumptions on the fairness of the scheduler. Finite waiting under Fairness : $(\\square\\Diamond P_0\\wedge\\square\\Diamond P_1)\\rightarrow\\square((\\ell_0\\rightarrow\\Diamond\\ell_1)\\wedge(m_0\\rightarrow\\Diamond m_1))$ Finite waiting is required if both $P_0$ and $P_1$ will always happens (taking turns) Let the atomic propositions $P_0$ and $P_1$ denote that the scheduler allows the respective process to advance in the current step. The subformula $\\square\\Diamond P_0$ states that process $P_0$ is scheduled infinitely often (always eventually). The finite waiting property is thus only required to hold if both processes are scheduled infinitely often. Bounded overtaking : $\\square(\\ell_0\\rightarrow(\\neg m_1\\ \\mathcal{U}\\ (m_1\\ \\mathcal{U}\\ (\\neg m_1\\ \\mathcal{U}\\ \\ell_1))))$ If $\\ell_0$ is reached, it must leave the location $m_1$, and wait until $\\ell_1$ is reached; when it leaves $\\ell_1$, then it is allowed to move back to location $m_1$ again. This repeats infinitely. In program $\\small{\\text{TURN}}$, the shared variable $t$ ensures that the processes take turns in entering their respective $\\text{critical}$ regions. Bounded Overtaking refers to once process $P_0$ reaches location $\\ell_0$, process $P_1$ can enter location $m_1$ at most once before Process $P_0$ enters location $\\ell_1$. This formulization of bounded overtaking requires that Process $P_1$ will eventually reach $\\ell_1$. Sometimes we may also prefer a weaker requirement, where waiting forever is fine, as long as Process $P_1$ does not get to enter $m_1$ more than once in the meantime. We can simply change the operator into weak until$(\\mathcal{W})$ to allow the processes to wait forever: $\\square(\\ell_0\\rightarrow(\\neg m_1\\ \\mathcal{W}\\ (m_1\\ \\mathcal{W}\\ (\\neg m_1\\ \\mathcal{W}\\ \\ell_1))))$ SummaryIn the next section, we will define the semantics of the logic for our automaton. Next chapter: LTL and Counting Languages Further Reading: Safety and liveness properties. Linear time property","link":"/AGV/agv6-2/"},{"title":"AGV 5.3 -- Complement Büchi Automaton with Odd Ranking","text":"Previous chapter: Ranking of DAG This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn section 4.2, we have shown the complementation construction for deterministic Büchi Automata. Now, by using DAG we can construct complement automaton of any Büchi Automata, with the help of the definition of odd ranking and the new function Level Ranking. Level Ranking $\\textbf{Definition 5.3. }\\text{(Level Ranking). Consider a Büchi Automaton }\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI}\\normalsize(F)).\\newline\\text{A level ranking }\\ell\\text{ is a pair }(S,g)\\text{ such that:}$$\\begin{array}{l}\\hspace{1cm} \\cdot \\ S\\subseteq Q,\\newline\\hspace{1cm} \\cdot \\ g:S\\rightarrow\\lbrace 0,\\dots,2|Q|\\rbrace\\text{ with }g(q)\\text{ necessarily even if }q\\in F.\\end{array}\\newline \\ \\newline\\text{We also denote: }$$\\begin{array}{lll}\\hspace{1cm} \\cdot \\ \\textsf{Lvlrks}&amp;=&amp;\\text{the (finite) set of level ranks, and }\\newline\\hspace{1cm} \\cdot \\ \\textsf{Initrks}&amp;=&amp;\\text{the set of level ranks s.t. }S=I\\end{array}\\newline \\ \\newline\\hspace{1cm}\\text{We say that a level ranking }\\ell’\\text{, given by }(S’,g’)\\text{ cover }\\ell\\text{, given by }(S,g)\\text{ for }\\sigma\\in\\Sigma,\\newline\\text{if }S’=\\lbrace q’\\mid(q,\\sigma,q’)\\in T\\rbrace\\text{ and for all }q\\in S,q’\\in S’\\text{ with }(q,\\sigma,q’)\\in T\\text{, it holds that } g’(q’)\\leq g(q).$ Here, the level ranking refers to a pair that contains set of states that share the same ranking. $\\ell’$ covers $\\ell$ ?The last part of the definition states that one level ranking is covering the other if the following satisfied: Let $\\ell’=(S’,g’)$ and $\\ell=(S,g)$ for $\\sigma\\in\\Sigma$ $q$ are the states that in the set $S$, so as $q’$ are in $S’$ All states $q’$ are some successors of some states $q, \\ (S’=\\lbrace q’\\mid(q,\\sigma,q’)\\in T\\rbrace)$ for all $q\\in S,q’\\in S’, (q,\\sigma,q’)\\in T$, the rank of $q’$ is lower than $q, \\ (g’(q’)\\leq g(q))$ That means the whole level’s successors will never have higher rank than itself. Complement Automaton ConstructionBy lemma 5.1, we know that a word $\\alpha$ that is rejected by $\\mathcal{A}$ has an odd ranking on the run DAG of $\\mathcal{A}$ on $\\alpha$.Now our complement will do the opposite: it only accepts the word that has odd ranking. To achieve this, we construct the odd ranking, level by level, by assigning ranks to vertices.The definition of level ranks and covering for a letter ensure that the requirements of a ranking are satisfied. Through the acceptance condition, we ensure that the ranking is odd, i.e., there is no infinite path that consists only of even-ranked vertices. $\\textbf{Construction 5.1. }\\text{Given a Büchi Automaton }\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))\\text{ that recognizes}\\newline\\text{the language }L\\text{, we construct a Büchi Automaton}$ $$\\newline \\mathcal{A’}=(\\Sigma,\\textsf{Lvlrks}\\times\\textsf{OnEvenPath},I’,T’,\\small\\text{BÜCHI}\\normalsize(F))$$ $\\text{that recognizes the language }\\Sigma^\\omega\\setminus L\\text{ as follows.}$$\\begin{array}{lrl}\\hspace{0.5cm} \\cdot &amp;\\textsf{OnEvenPath} = &amp; 2^Q, \\text{tracking states in the current level,} \\newline\\hspace{0.5cm} \\cdot &amp;I’= &amp; \\lbrace (\\ell,R) \\mid \\ell \\in \\textsf{InitRks, } \\ell \\text{ is given by } (S,g) \\text{, and } R = \\lbrace q \\mid g(q) \\text{ is even} \\rbrace \\rbrace \\newline\\hspace{0.5cm} \\cdot &amp;T’= &amp; \\textsf{NewEvenPaths} \\cup \\textsf{ContinueEvenPaths}, \\text{ where} \\newline\\hspace{0.5cm} &amp;&amp; \\hspace{0.9cm}\\textsf{NewEvenPaths}=\\lbrace ((\\ell,\\varnothing),\\sigma,(\\ell’,R’)) \\mid \\ell’ \\text{ covers } \\ell \\text{ for } \\sigma, \\newline\\hspace{0.5cm} &amp;&amp; \\hspace{4.8cm} \\ell’ \\text{ is given by } (S’,g’), \\newline\\hspace{0.5cm} &amp;&amp; \\hspace{4.8cm} \\text{and } R’ = \\lbrace q’ \\mid g’(q’) \\text{ is even} \\rbrace \\rbrace \\newline\\hspace{0.5cm} &amp;&amp; \\textsf{ContinueEvenPaths} = \\lbrace ((\\ell,R),\\sigma,(\\ell’,R’)) \\mid R \\neq \\varnothing, \\ell’ \\text{ covers } \\ell \\text{ for } \\sigma,\\newline\\hspace{0.5cm} &amp;&amp; \\hspace{4.8cm} \\ell’ \\text{ is given by } (S’,g’)\\text{, and}\\newline\\hspace{0.5cm} &amp;&amp; \\hspace{4.8cm} R’ = \\lbrace q’ \\mid (q,\\sigma,q’) \\in T, q \\in R, \\text{ and } g’(q’) \\text{ is even} \\rbrace \\rbrace \\newline\\hspace{0.5cm} \\cdot &amp;F’ = &amp; \\textsf{Lvlrks} \\times \\lbrace \\varnothing \\rbrace.\\end{array}$ Explanation$Q’ = \\textsf{Lvlrks}\\times\\textsf{OnEvenPath}:$ all states contain their level ranking and set of all states that is tracked currently (states would be dropped if it leads to odd-ranked vertex). $I’:$ The initial state contains the initial level ranking ($\\textsf{Lvlrks}$), and the set of all even ranking states ($R$). $T’:$ We have transitions $\\textsf{NewEvenPaths}$ and $\\textsf{ContinueEvenPaths}$. Both share some properties in common. all the states are covered by their successors, their successors only keep track on even ranking states, $\\textsf{ContinueEvenPaths}$ requires that the current states must keep track on even ranking states already $F’:$ Accepting states are those has no infinite path that consists only of even-ranked vertices.(odd ranking). ProofNow we can try to verify the Lemma with our construction: $\\textbf{Lemma 5.2. }\\textit{The automaton of Construction 5.1 has an accepting run on input }\\alpha\\newline\\textit{if and only if the run DAG of }\\mathcal{A}\\textit{ on }\\alpha\\textit{ has an odd ranking.}$ The run of the automaton on the input word builds a potential ranking, level by level. Note that the level-by-level guessing of the ranking is the only source of non-determinism. But if the guess is indeed an odd ranking, then the automaton has a unique run. In the construction, we used level rankings to ensure the definition of ranking is satisified, so instead of verify whether it is odd ranking, we only need to verify that in the guessed ranking, there is no infinite path of even-ranked vertices. At any given point, the second component ($\\textsf{OnEvenPath}/R$) in the state of the constructed automaton tracks a “batch” of paths that traverse solely through even-ranked vertices. Paths that hit an odd-ranked vertex are dropped from the batch. The acceptance condition enforces that every batch that is followed must be eventually be emptied. Odd ranking = no infinite path of even-ranked vertices = eventually every batch are emptied So we can rephrase the lemma as “The run is accepting if and only if eventually every batch are emptied“. If the run is accepting, then eventually every batch are emptiedThis condition is clearly necessary for the guessed ranking to be odd: if there is eventually a batch that is never emptied, this corresponds to an infinite path of even-ranked vertices in the run DAG, violating requirement 3 (The rank cannot increase upon traversing an edge). Why? Because as long as we have one empty batch, the ranking of that batch will definitely be hihger than the those which are not empty (consider the definition of rank function and pruning), and any states will never have high rank than their successors. If eventually every batch are emptied, then the run is acceptingAssume we have non accepting run that have emptied batch, which means there is an infinite path of even-ranked vertices in the run DAG. Now suppose this run has the batch-tracking set $\\textsf{OnEvenPath}$ is emptied infinitely often. That means in the infinite path of even-ranked vertices there is an level $n$ so that $\\textsf{OnEvenPath}$ is emptied at step $n$. Because this is an infinite path of even-ranked vertices, once $\\textsf{OnEvenPath}$ is emptied at step $n$, the next transition will be $\\textsf{NewEvenPath}$. Which means there is a successor in level $n+1$ that has an even rank. Since the path is infinite, subsequent transitions will be $\\textsf{ContinueEvenPaths}$, and the even-rank vertices will be remained in the set of $\\textsf{OnEvenPath}$. Which contradicts that the run have emptied batch. Therefore this non accepting run that have emptied batch do not exist. Next chapter: Linear-Time Temporal Logic (LTL) Further Reading:","link":"/AGV/agv5-3/"},{"title":"AGV 6.3 -- LTL and Counting Languages","text":"Previous chapter: Expressing Program Properties using LTL This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner SemanticsAn LTL formula $\\varphi$ over $AP$ defines the following language over the alphabet $2^{AP}$: $\\hspace{1cm} \\mathcal{L}(\\varphi)=\\lbrace\\alpha\\in(2^{AP})^\\omega\\mid\\alpha\\models\\varphi\\rbrace$ where $\\models$ is the smallest relation satisfying: $\\begin{array}{lllll}\\hspace{1cm} \\alpha &amp; \\models &amp; p &amp; \\text{iff} &amp; p\\in\\alpha(0) \\ \\ \\ \\ (\\text{i.e., }\\alpha(0)\\models p)\\newline\\hspace{1cm} \\alpha &amp; \\models &amp; \\varphi_1\\wedge\\varphi_2 &amp; \\text{iff} &amp; \\alpha\\models\\varphi_1\\ \\text{ and }\\ \\alpha\\models\\varphi_1\\newline\\hspace{1cm} \\alpha &amp; \\models &amp; \\neg\\varphi &amp; \\text{iff} &amp; \\alpha\\not\\models\\varphi\\newline\\hspace{1cm} \\alpha &amp; \\models &amp; \\bigcirc\\varphi &amp; \\text{iff} &amp; \\alpha[1,\\infty]=\\alpha(1)\\alpha(2)\\alpha(3)\\dots\\models\\varphi\\newline\\hspace{1cm} \\alpha &amp; \\models &amp; \\varphi_1\\ \\mathcal{U}\\ \\varphi_2 &amp; \\text{iff} &amp; \\exists j\\geq0.\\ \\alpha[j,\\infty]\\models\\varphi_2\\ \\text{ and }\\ \\alpha[i,\\infty]\\models\\varphi_1\\text{ for all }0\\leq i\\leq j\\newline\\end{array}$ For the temporal operators, the semantics can be visualized as below: From LTL to $\\omega$-regular languages All LTL-definable properties are $\\omega$-regular, but NOT ALL $\\omega$-regular languages can be defined in LTL. Why can’t we? What is the limitation of LTL formula? Counting and Non-counting LanguagesLet say we want to express: An arbitrary even sequence of $\\varnothing$ symbols, followed by an infinite sequence of $p$ symbols In $\\omega$-language it is $(\\varnothing\\varnothing)^\\ast\\lbrace p\\rbrace^\\omega$. However, this cannot be defined in LTL, because it is a counting language. $\\textbf{Definition 6.1. }\\text{A Language }L\\subseteq\\Sigma^\\omega\\text{ is }\\textit{non-counting}\\text{ iff}$$$\\exists n_0\\in\\mathbb{N}.\\ \\forall n\\geq n_0.\\ \\forall v,w\\in\\Sigma^\\ast, \\alpha\\in\\Sigma^\\omega.\\ vw^n\\alpha\\in L\\Leftrightarrow vw^{n+1}\\alpha\\in L$$ For some threshold $n_0$, and a word with prefix $w$ which repeated $n$ times and $n\\geq n_0$. For every $n$ we picked, if we can always find a pair of words $vw^{n}\\alpha,vw^{n+1}$ that are both accepted by language $L$, then $L$ is non-counting. For example, in $L_1=(\\varnothing\\varnothing)^\\ast\\lbrace p\\rbrace^\\omega$, if $(\\varnothing)^n\\lbrace p\\rbrace^\\omega\\in L_1$, then $(\\varnothing)^{n+1}\\lbrace p\\rbrace^\\omega\\not\\in L_1$, so $L_1$ is counting with $n_0=1$. $\\textbf{Theorem 6.1. }\\textit{ For every LTL-formula }\\varphi,\\mathcal{L}(\\varphi)\\textit{ is non-counting.}$ ProofWe prove the theorem by structural induction on $\\varphi$, going through its sematics: $\\textbf{Case }\\varphi=p:$$\\hspace{1cm}$ Simply choose the length $n_0=1$, (because $\\alpha\\models p\\text{ iff }p\\in\\alpha(0)$) $\\textbf{Case }\\varphi=\\varphi_1\\wedge\\varphi_2:$$\\hspace{1cm}$ By induction hypothesis, If $\\varphi_1$ and $\\varphi_2$ are non-counting, then $\\mathcal{L}(\\varphi_1)$ and $\\mathcal{L}(\\varphi_2)$ have threshold $n_0’$ and$\\hspace{1cm}$ $n’’_0\\in\\mathbb{N}$ respectively. We can choose $n_0=\\text{max}\\lbrace n’_0,n’’_0\\rbrace$ to ensure $\\varphi$ is also non-counting. (max. because$\\hspace{1cm}$ the larger the $n_0$, less $n$ is required to be non-counting) $\\textbf{Case }\\varphi=\\neg\\varphi_1:$$\\hspace{1cm}$ If $\\varphi_1$ is non-counting, then $\\mathcal{L}(\\varphi_1)$ has threshold $n_0’\\in\\mathbb{N}$. We choose $n_0=n’_0$ $\\textbf{Case }\\varphi=\\bigcirc\\varphi_1:$$\\hspace{1cm}$ If $\\varphi_1$ is non-counting, then $\\mathcal{L}(\\varphi_1)$ has threshold $n_0’\\in\\mathbb{N}$. We choose $n_0=n’_0+1$ and try to show$$\\text{“For }n\\geq n_0,\\ vw^n\\alpha\\models\\bigcirc\\varphi\\ \\text{ if and only if }\\ n\\geq n_0,\\ vw^{n+1}\\alpha\\models\\bigcirc\\varphi”$$ $\\hspace{1cm}$ We try to perform operations on the fix length prefix $v$ as it doesn’t affect the finite loop prefix $w$. So we$\\hspace{1cm}$ need to consider whether the fix length prefix $v$ is an empty string $(\\varepsilon)$, if not, we try to “peel” one cycle of$\\hspace{1cm}$ $w$ and make it so-called “fix-length” prefix. $\\hspace{1cm}$ A simple example for the operation: if there’s logic requires $b$ to be reached in the next step, then the same$\\hspace{1cm}$ logic without neXt operator $\\bigcirc$ can be satisfied by removing the first letter. (if $ab\\models\\bigcirc\\varphi$, then $b\\models\\varphi$) $\\hspace{1cm}\\textbf{Case }\\ v\\neq\\varepsilon:$ Thus $v=av’$ for some $a\\in\\Sigma,v’\\in\\Sigma^\\ast$. We have that $\\begin{array}{lrl}\\hspace{2cm}&amp;av’w^n\\alpha&amp;\\models\\bigcirc\\varphi\\newline\\hspace{2cm}\\text{iff}&amp;v’w^n\\alpha&amp;\\models\\varphi\\newline\\hspace{2cm}\\text{iff}&amp;v’w^{n+1}\\alpha&amp;\\models\\varphi&amp;\\text{(induction hypothesis)}\\newline\\hspace{2cm}\\text{iff}&amp;av’w^{n+1}\\alpha&amp;\\models\\bigcirc\\varphi\\newline\\end{array}$ $\\hspace{1cm}\\textbf{Case }\\ v=\\varepsilon:$ Thus either $w=\\varepsilon$ (proved trivially), or $w=aw’$ for some $a\\in\\Sigma,w’\\in\\Sigma^\\ast$. It follows that $\\begin{array}{lrl}\\hspace{2cm}&amp;(aw’)^n\\alpha&amp;\\models\\bigcirc\\varphi\\newline\\hspace{2cm}\\text{iff}&amp;(aw’)(aw’)^{n-1}\\alpha&amp;\\models\\bigcirc\\varphi\\newline\\hspace{2cm}\\text{iff}&amp;w’(aw’)^{n-1}\\alpha&amp;\\models\\varphi\\newline\\hspace{2cm}\\text{iff}&amp;w’(aw’)^{n}\\alpha&amp;\\models\\varphi&amp;\\text{(induction hypothesis)}\\newline\\hspace{2cm}\\text{iff}&amp;(aw’)^{n+1}\\alpha&amp;\\models\\bigcirc\\varphi\\newline\\end{array}$ $\\textbf{Case }\\varphi=\\varphi_1\\ \\mathcal{U}\\ \\varphi_2:$$\\hspace{1cm}$ If $\\varphi_1$ and $\\varphi_2$ are non-counting, then $\\mathcal{L}(\\varphi_1)$ and $\\mathcal{L}(\\varphi_2)$ have threshold $n_0’$ and $n’’_0\\in\\mathbb{N}$ respectively. We$\\hspace{1cm}$ choose $n_0=\\text{max}\\lbrace n’_0,n’’_0\\rbrace+1$ and try to show$$”\\text{For }n\\geq n_0,\\ vw^n\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2\\ \\text{ if and only if }\\ vw^{n+1}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2”$$ $\\hspace{1cm}$ By the semantics of $\\mathcal{U}$, $\\varphi_1$ keeps holding until $\\varphi_2$ is satisfied. Let $j$ be the least index when $\\varphi_2$ is satisfied,$\\hspace{1cm}$ then we have $vw^n\\alpha[j,\\infty]\\models\\varphi_2$ and for all $i&lt;j,\\ vw^n\\alpha[i,\\infty]\\models\\varphi_1$, respectively applies on $vw^{n+1}\\alpha$. $\\hspace{1cm}$ $\\textbf{1. }\\textit{If }\\ vw^n\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2\\textit{, then }\\ vw^{n+1}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2:$$\\hspace{2cm}$ Similar to $\\bigcirc\\varphi$, we try to do the operation with respect to $j$’s position. So we need to consider whether$\\hspace{2cm}$ index $j$ lies within the first cycle of the prefix $|v|+|w|$, or lies in the finite loop of the prefix $|w|$. $\\hspace{2cm}\\textbf{Case }j\\leq|v|+|w|:$$\\hspace{3cm}$ By induction hypothesis, we assumed $\\varphi_1$ and $\\varphi_2$ are non-counting. So$\\hspace{3cm}$ if $vww^{n−1}\\alpha[j,\\infty]\\models\\varphi_2$, then $vww^{n}\\alpha[j,\\infty]\\models\\varphi_2$ and analogously,$\\hspace{3cm}$ if $vww^{n−1}\\alpha[i,\\infty]\\models\\varphi_1$, then $vww^{n}\\alpha[i,\\infty]\\models\\varphi_1$ for $i &lt; j$.$\\hspace{3cm}$ Hence, $vw^{n+1}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$. $\\hspace{2cm}\\textbf{Case }j&gt;|v|+|w|:$$\\hspace{3cm}$ By adding one more cycle, we can essentially get the same suffix if $j$ is somewhere in the cycle:$\\hspace{3cm}$ if $vw^{n}\\alpha[j,\\infty]\\models\\varphi_2$, then $vw^{n+1}\\alpha[j+|w|,\\infty]\\models\\varphi_2$, and$\\hspace{3cm}$ if $vw^{n}\\alpha[i,\\infty]\\models\\varphi_1$, then $vw^{n+1}\\alpha[i,\\infty]\\models\\varphi_1$, for each position $|v|+|w|\\leq i&lt;j+|w|$.$\\hspace{3cm}$ Additionally, by induction hypothesis from the above case, we have that $\\begin{array}{lrcccll}\\hspace{4cm}&amp;|v|+|w|&amp;\\leq&amp;i&amp;&lt;&amp;j+|w|\\newline\\hspace{4cm}\\text{iff}&amp;|v|&amp;\\leq&amp;i&amp;&lt;&amp;j\\newline\\hspace{4cm}\\text{iff}&amp;&amp;&amp;i&amp;&lt;&amp;j&amp;&amp;(\\text{induction hypothesis: }i&lt;|v|+|w|)\\newline\\end{array}$ $\\hspace{3cm}$ Hence, $vw^{n}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$. $\\hspace{1cm}$ $\\textbf{2. }\\textit{If }\\ vw^{n+1}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2\\textit{, then }\\ vw^{n}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2:$$\\hspace{2cm}$ Again, we need consider both cases seperately: $\\hspace{2cm}\\textbf{Case }j\\leq|v|+|w|:$$\\hspace{3cm}$ By induction hypothesis, we assumed $\\varphi_1$ and $\\varphi_2$ are non-counting. So$\\hspace{3cm}$ if $vww^{n}\\alpha[j,\\infty]\\models\\varphi_2$, then $vww^{n-1}\\alpha[j,\\infty]\\models\\varphi_2$ and analogously,$\\hspace{3cm}$ if $vww^{n}\\alpha[i,\\infty]\\models\\varphi_1$, then $vww^{n-1}\\alpha[i,\\infty]\\models\\varphi_1$ for $i &lt; j$.$\\hspace{3cm}$ Hence, $vw^{n}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$. $\\hspace{2cm}\\textbf{Case }j&gt;|v|+|w|:$$\\hspace{3cm}$ Simliar to above, by subtracting one more cycle, we can essentially get the same suffix:$\\hspace{3cm}$ if $vw^{n+1}\\alpha[j,\\infty]\\models\\varphi_2$, then $vw^{n}\\alpha[j-|w|,\\infty]\\models\\varphi_2$, and$\\hspace{3cm}$ if $vw^{n+1}\\alpha[i,\\infty]\\models\\varphi_1$, then $vw^{n}\\alpha[i,\\infty]\\models\\varphi_1$, for each position $|v|+|w|\\leq i&lt;j-|w|$.$\\hspace{3cm}$ Again, by induction hypothesis from the above case, we have that $\\begin{array}{lrcccll}\\hspace{4cm}&amp;|v|+|w|&amp;\\leq&amp;i&amp;&lt;&amp;j-|w|\\newline\\hspace{4cm}\\text{iff}&amp;|v|+2|w|&amp;\\leq&amp;i&amp;&lt;&amp;j\\newline\\hspace{4cm}\\text{iff}&amp;&amp;&amp;i&amp;&lt;&amp;j&amp;&amp;(\\text{induction hypothesis: }i&lt;|v|+|w|)\\newline\\end{array}$ $\\hspace{3cm}$ Hence, $vw^{n}\\alpha\\models\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$. SummaryUnfortunately, LTL is not expressive enough to include all $\\omega$-regular languages. However, by extending the syntax of LTL, we can also extend the languages it covers.In the next section, we will Quantified Propositional Temporal Logic (QPTL), and see whether it suffices to solve our problem. Next chapter: Quantified Propositional Temporal Logic (QPTL) Further Reading:","link":"/AGV/agv6-3/"},{"title":"AGV 6.5 -- Monadic Second-Order Logic of One Successor (S1S)","text":"Previous chapter: Quantified Propositional Temporal Logic (QPTL) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionTemporal logics like LTL and QPTL refer to the positions of the input word implicitly through the temporal operators. With S1S, we now introduce a logic that allows us to manipulate positions explicitly. For example, the mutual exclusion property in LTL is $\\square\\neg(\\ell_1\\wedge m_1)$, where the Always operator $\\square$ implicitly quantifies over all positions. In S1S, we use explicit universal quantifiers instead: $\\forall x.\\neg(x\\in P_{\\ell_1}\\wedge x\\in P_{m_1})$. Definition Monadic: the second-order quantification is restricted to unary relations, i.e., sets, One successor: only have a single successor operation. Later in the course, we will study monadic second-order logics of two or more successors (S2S, WS1S, etc.), which allow us to describe trees rather than words. S1S syntaxIn automaton, we use states; in LTL and QPTL, we use propositions, in S1S, we use positions.Propositions in QPTL can be interpreted as set of positions that holds $\\textit{true}$. basic variables Defintiion Example first-order variables store positions $0,1,x,y,\\dots$ second-order variables store sets of positions (refer to $p$ as $\\exists p.\\varphi$ in QPTL) $X,Y,\\dots$ successor operation $S$, allows us to navigate to the next position (same as $\\bigcirc$ in LTL) $S(t), S(2),\\dots$ Terms and FormulasLet $V_1=\\lbrace x, y,\\dots\\rbrace$ be a set of first-order variables and $V_2=\\lbrace X, Y,\\dots\\rbrace$ a set of second-order variables. Then the terms of S1S are defined by the following grammar: $$t::=0\\mid x\\mid S(t)$$ The formulas`of S1S are defined by the following grammar: $$\\varphi::=t\\in X\\mid t=t\\mid\\neg\\varphi\\mid\\varphi\\wedge\\varphi\\mid\\exists x.\\varphi\\mid\\exists X.\\varphi$$ The precedence order of the operators goes from left (highest precedence) to right (lowest precedence), as denoted by the grammar above. We still allow usual boolean connectives with the following abbreviations: $\\begin{array}{ll}\\hspace{1cm}\\cdot\\hspace{0.5cm} \\forall X.\\varphi := \\neg\\exists X. \\neg\\varphi&amp; \\hspace{3cm}\\cdot\\hspace{0.5cm} \\forall x.\\varphi := \\neg\\exists x. \\neg\\varphi\\newline\\hspace{1cm}\\cdot\\hspace{0.5cm} x\\notin Y := \\neg(x\\in Y)&amp; \\hspace{3cm}\\cdot\\hspace{0.5cm} x\\neq y:= \\neg(x=y)\\end{array}$ Variable ValuationsThe semantics of an S1S formula is given relative to a valuation of the variables. First-order Valuation: $\\sigma_1:V_1\\rightarrow\\mathbb{N}$ assigns to each first-order variable a natural number. Second-order Valuation: $\\sigma_2:V_2\\rightarrow 2^\\mathbb{N}$ assigns to each second-order variable a set of natural numbers. The value of a term is then defined as follows: $\\begin{array}{lll}\\hspace{1cm}\\cdot\\hspace{0.5cm} \\lbrack 0\\rbrack_{\\sigma_1} = 0 &amp;\\hspace{4cm}\\cdot\\hspace{0.5cm} \\lbrack x\\rbrack_{\\sigma_1} = \\sigma_1(x) &amp;\\hspace{3cm}\\cdot\\hspace{0.5cm} \\lbrack S(t)\\rbrack_{\\sigma_1} =[t]_{\\sigma_1}+1\\newline\\end{array}$ Free, Bound and the Language of S1SAgain, we define the subsets of free first-order and free second-order variables as $V’_1\\subseteq V_1$ and $V’_2\\subseteq V_2$ respectively. An S1S formula $\\varphi$ then defines the following language over the alphabet $2^{V’_1\\cup V’_2}$: $$\\mathcal{L}(\\varphi)=\\lbrace \\alpha_{\\sigma_1,\\sigma_2}\\in(2^{V’_1\\cup V’_2})^\\omega\\mid\\sigma_1,\\sigma_2\\models\\varphi\\rbrace$$ where $x\\in\\alpha_{\\sigma_1,\\sigma_2}(j)\\text{ iff }=\\sigma_1(x)$, and $X\\in\\alpha_{\\sigma_1,\\sigma_2}(j)\\text{ iff }j=\\sigma_2(X)$, and $\\models$ is the smallest relation that satisfies the following: $\\begin{array}{llllll}\\hspace{1cm}\\cdot &amp; \\sigma_1,\\sigma_2 &amp; \\models &amp; t\\in X &amp; \\text{iff} &amp; \\lbrack t\\rbrack_{\\sigma_1}\\in\\sigma_2(X)\\newline\\hspace{1cm}\\cdot &amp; \\sigma_1,\\sigma_2 &amp; \\models &amp; t_1=t_2 &amp; \\text{iff} &amp;\\lbrack t_1\\rbrack_{\\sigma_1}=\\lbrack t_2\\rbrack_{\\sigma_1}\\newline\\hspace{1cm}\\cdot &amp; \\sigma_1,\\sigma_2 &amp; \\models &amp; \\neg\\psi &amp; \\text{iff} &amp;\\sigma_1,\\sigma_2\\not\\models\\psi\\newline\\hspace{1cm}\\cdot &amp; \\sigma_1,\\sigma_2 &amp; \\models &amp; \\varphi_0\\wedge \\varphi_1 &amp; \\text{iff} &amp;\\sigma_1,\\sigma_2\\models\\varphi_0\\text{ and }\\sigma_1,\\sigma_2\\models\\varphi_1\\newline\\hspace{1cm}\\cdot &amp; \\sigma_1,\\sigma_2 &amp; \\models &amp; \\exists x.\\varphi &amp; \\text{iff} &amp;\\text{there is an }\\alpha\\in\\mathbb{N}\\text{ s.t. }\\sigma’_1,\\sigma_2\\models\\varphi\\text{ and }\\sigma’_1(y)=\\left\\lbrace\\begin{array}{ll}\\sigma_1(y)&amp;\\text{if }y\\neq x \\newline a&amp;\\text{if }y=x\\end{array}\\right.\\newline\\hspace{1cm}\\cdot &amp; \\sigma_1,\\sigma_2 &amp; \\models &amp; \\exists X.\\varphi &amp; \\text{iff} &amp;\\text{there is an }A\\in\\mathbb{N}\\text{ s.t. }\\sigma_1,\\sigma’_2\\models\\varphi\\text{ and }\\sigma’_2(Y)=\\left\\lbrace\\begin{array}{ll}\\sigma_2(Y)&amp;\\text{if }Y\\neq X\\newline A&amp;\\text{if }Y=X\\end{array}\\right.\\end{array}$ For Existence Operator $(\\exists)$, the definition is similar to QPTL: We have $\\sigma’_1$ that behave exactly the same for every first-order variable $y$. Except for some $x$, there is a value $a$ that $\\sigma’_1$ can assign to $x$ so that $\\varphi$ holds. Same definition apply on second-order variable $X$ resepctively. Example Statement Formula $X$ is a subset of $Y$ $X\\subseteq Y\\equiv\\forall z.\\ (z\\in X\\rightarrow z\\in Y)$ $X$ and $Y$ are equal $X = Y\\equiv X\\subseteq Y \\wedge Y\\subseteq X$ $X$ is upward closed $\\textit{Upwardclosed}(X)\\equiv\\forall y.\\ (y\\in X\\rightarrow S(y)\\in X)$ $x$ is less than or equals to $y$ $x\\leq y\\equiv\\forall Z.\\ (x\\in Z\\wedge\\textit{Upwardclosed}(Z))\\rightarrow y\\in Z$ $X$ is a finite set $\\textit{Fin}(X)\\equiv\\exists Y.\\ (X\\subseteq Y\\wedge(\\exists z.\\ z\\notin Y)\\wedge(\\forall z.\\ (z\\notin Y\\rightarrow S(z)\\notin Y)))$ $X$ is the set of even numbers $\\textit{Even}(X)\\equiv0\\in X\\wedge\\neg S(0)\\in X \\wedge \\forall y.\\ (y\\in X\\leftrightarrow S(S(y))\\in X)$ Every even number in $X$ is in $Y$ $\\textit{EvenCount}(X,Y)\\equiv\\forall w.\\ (\\exists Z.\\ \\textit{Even}(Z)\\wedge w\\in Z)\\rightarrow(w\\in X\\rightarrow w\\in Y)$ SummaryIn the next section, we will try to compare S1S with QPTL and see their expressiveness. Next chapter: Express QPTL using S1S Further Reading:","link":"/AGV/agv6-5/"},{"title":"AGV 6.4 -- Quantified Propositional Temporal Logic (QPTL)","text":"Previous chapter: LTL and Counting Languages This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn the last section, we knew that LTL cannot express counting-languages. QPTL, which extendsLTL with quantification over propositions, repairs this deficiency. ExampleWe knew $L=(\\varnothing\\varnothing)^\\ast\\lbrace p\\rbrace^\\omega$ is not LTL-definable. However, similar language $L’=(\\varnothing\\lbrace q\\rbrace)^\\ast\\lbrace p\\rbrace^\\omega$ is LTL-definable: $$\\varphi=\\neg q\\wedge(\\neg p\\wedge(\\neg q\\leftrightarrow\\bigcirc q))\\ \\mathcal{U}\\ (\\square(p\\wedge\\neg q))$$ Intuitively, $L’$ is the same language as $L$, except that there is an additional proposition $q$ that keeps track of odd and even positions. LTL has no means of introducing such “helpful” propositions that are not already present in the language we wish to define. In QPTL, we can introduce the proposition $q$ using a quantifier. The existential quantification $\\exists q$. $\\varphi$ expresses that there is a way to evaluate the new proposition $q$ such that, in the such extended word, $\\varphi$ is true. In the example, the language of $\\exists q.\\ \\varphi$ is thus precisely $L$. SyntaxQPTL formulas over a set $AP$ of atomic propositions are generated by the following grammar, where $p\\in AP$: $$\\psi ::= p\\ \\mid\\ \\neg\\psi\\ \\mid\\ \\psi\\wedge\\varphi\\ \\mid\\ \\bigcirc\\psi\\ \\mid\\ \\Diamond\\psi\\ \\mid\\ \\exists p.\\ \\psi$$ The QPTL connectives have the same semantics and precedence as in LTL, except for propositional quantification, which has lowest precedence and the following semantics: $$\\begin{array}{lcr}\\alpha\\models\\exists p.\\ \\varphi&amp;\\text{ iff }&amp;\\text{ there exists }\\alpha’\\in(2^{AP})^\\omega\\text{ such that }\\alpha=_{AP\\setminus\\lbrace p\\rbrace}\\alpha’\\text{ and }\\alpha’\\models\\varphi\\end{array}$$ where $\\alpha=_P\\alpha’$ for some $P\\subseteq AP$ iff, for all $i\\in\\mathbb{N},\\alpha(i)\\cap P=\\alpha’(i)\\cap P$. Every $AP$ in $\\alpha$ is same as $\\alpha’$, except $\\alpha’$ can modify $p$ so that $\\varphi$ holds for $\\alpha’$ Until operator $\\mathcal{U}$ in QPTLWe can express the Until operator $\\mathcal{U}$ in the syntax of QPTL with quantifier and the meaning is equivalent: $\\varphi\\ \\mathcal{U}\\ \\psi:$ $\\varphi$ must hold $\\textit{true}$ until $\\psi$ becomes $\\textit{true}$. $$\\exists t.\\ t\\wedge\\square(t\\rightarrow(\\psi\\vee(\\varphi\\wedge\\bigcirc t)))\\wedge\\Diamond\\neg t:$$ $\\exists t.\\ t$: For some proposition $t$, $t$ holds at the beginning; $\\square(t\\rightarrow(\\psi\\vee(\\varphi\\wedge\\bigcirc t)))$: if $t$ holds, we repeatly check the following either $\\psi$ becomes $\\textit{true}$, or $\\varphi$ must remain $\\textit{true}$ and so as $t$ in the next step(so that this if-clause check again in the next step);\\ $\\Diamond\\neg t$ : Eventually, $t$ will no longer hold. It enforces $\\psi$ must become $\\textit{true}$ at some point and then $t$ doesn’t need to hold anymore If it is weak until $\\mathcal{W}$ then this part is not necessarily because we don’t enforce $\\psi$ to be $\\textit{true}$ Free and Bound Atomic Propositions Atomic Propositions Defintiion Free NOT in the scope of a quantifier over the proposition Bound In the scope of a quantifier over the proposition Here, Bounded propositions are internal helpers to help us construct some logical conditions of the formula that do not directly appear in the language. (e.g. we need a counter for counting-language, but it doesn’t belongs to any proposition that refers to the alphabet or language.) From QPTL formula to $\\omega$-regular languageLet’s defined the set of free atomic propositions as $AP’\\subseteq AP$. For every letter in the alphabet, we can describe them in $AP’$. For exmaple, if $\\lbrace a,b,c\\rbrace=\\Sigma$, the word $\\lbrace ab\\rbrace\\in\\Sigma$ will be $\\lbrace a,b,\\neg c\\rbrace\\in AP’$ Now, the language of QPTL is essentially the language over alphabet $2^{AP’}:\\mathcal{L}(\\varphi)=\\lbrace\\alpha\\in{(2^{AP’})}^{\\omega}\\mid\\alpha\\models\\varphi\\rbrace$ In fact, it is exactly the $\\omega$-regular languages that can be expressed in QPTL. And therefore, we can translate a given Büchi automaton with alphabet $2^{AP}$ into a QPTL formula: $\\textbf{Theorem 6.2. } \\textit{For every Büchi automaton }\\mathcal{A}\\textit{ over }\\Sigma=2^{AP}\\textit{ there exists a QPTL formula }\\varphi_{\\mathcal{A}}\\newline\\textit{such that }\\mathcal{L}(\\varphi)=\\mathcal{L}(\\mathcal{A}).$ ProofReminder: here the automaton using atomic propositions $2^{AP}$, not the alphabet $\\Sigma$. To translate states in automata into atomic proposition in QPTL formula, we introduce auxiliary proposition $at_q$ for each state $q\\in Q$, where $Q=\\lbrace q_1, q_2,\\dots,q_n\\rbrace$ Then QPTL formula $\\varphi_\\mathcal{A}$ for the Büchi automaton $\\mathcal{A}$ so that $\\mathcal{L}(\\varphi)=\\mathcal{L}(\\mathcal{A})$ is defined as follows: $$\\begin{array}{rll}\\varphi_\\mathcal{A}:=\\exists at_q,\\dots,at_{q_n}.&amp;&amp;\\underset{q\\in I}{\\bigvee}at_q\\newline&amp;\\wedge&amp;\\square\\left(\\underset{(q,A,q’)\\in T}{\\bigvee}at_q\\wedge\\bigcirc at_{q’}\\wedge\\left(\\underset{p\\in A}{\\bigwedge}p\\right)\\wedge\\left(\\underset{p\\in AP\\setminus A}{\\bigwedge}\\neg p\\right)\\right)\\newline&amp;\\wedge&amp;\\square\\left(\\overset{n}{\\underset{i=1}{\\bigwedge}}\\underset{j\\neq i}{\\bigwedge}\\neg(at_{q_i}\\wedge at_{q_j})\\right)\\newline&amp;\\wedge&amp;\\square\\Diamond\\underset{q\\in F}{\\bigvee}at_q\\end{array}$$ Explained in Human languageTo express a Büchi automaton in QPTL, we first need to know what features/characteristics we need express. Begins with Initial states and reaches the Accepting states infinitely often: $$\\textsf{Initial states: }\\underset{q\\in I}{\\bigvee}at_q \\hspace{3cm} \\textsf{Accepting states: }\\square\\Diamond\\underset{q\\in F}{\\bigvee}at_q$$ There’s always transitions for any states, only using the letters available in the current state defined by set $A$: $$\\square\\left(\\underset{(q,A,q’)\\in T}{\\bigvee}at_q\\wedge\\bigcirc at_{q’}\\wedge\\left(\\underset{p\\in A}{\\bigwedge}p\\right)\\wedge\\left(\\underset{p\\in AP\\setminus A}{\\bigwedge}\\neg p\\right)\\right)$$ Additionally, we also need to ensure that there’s exactly one current state $q_i$, which is $at_{q_i}$ in propositions: $$\\square\\left(\\overset{n}{\\underset{i=1}{\\bigwedge}}\\underset{j\\neq i}{\\bigwedge}\\neg(at_{q_i}\\wedge at_{q_j})\\right)$$ SummaryAs above, we can see that every feature of a Büchi automaton can be expressed in QPTL. What’s next? Can we use simpler syntax to express same formula? Next chapter: Monadic Second-Order Logic of One Successor (S1S) Further Reading:","link":"/AGV/agv6-4/"},{"title":"AGV 6.6 -- Express QPTL using S1S","text":"Previous chapter: Monadic Second-Order Logic of One Successor (S1S) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionWe already showed, in Theorem 6.2, that every Büchi-recognizable language is QPTL-definable. We now complete a full circle by showing that every QPTL-definable language is S1S-definable, and that every S1S-definable language is Büchi-recognizable. Hence, QPTL, S1S, and Büchi automata are equally expressive. $\\textbf{Theorem 6.3. } \\textit{Every QPTL-definable language is S1S-definable.}$ ProofIn section 6.4, we defined the language of QPTL as: $\\mathcal{L}(\\varphi)=\\lbrace\\alpha\\in{(2^{AP’})}^{\\omega}\\mid\\alpha\\models\\varphi\\rbrace$, and In section 6.5, we defined the language of S1S as: $\\mathcal{L}(\\varphi)=\\lbrace \\alpha_{\\sigma_1,\\sigma_2}\\in(2^{V’_1\\cup V’_2})^\\omega\\mid\\sigma_1,\\sigma_2\\models\\varphi\\rbrace$ Notice main difference comes from $\\alpha\\models\\varphi$ and $\\sigma_1,\\sigma_2\\models\\varphi$. Also, QPTL uses propositions but S1S uses term. We thus define S1S formula as $T(\\varphi,t)$, where $\\varphi$ is a QPTL-formula over $AP$ and $t$ is a S1S-term. Lastly, with $V_2=AP$, we can now define a S1S formula for all $\\alpha\\in(2^{AP})^\\omega$, $$\\alpha\\lbrack\\lbrack t\\rbrack_{\\sigma_1},\\infty\\rbrack\\models_{QPTL}\\varphi\\hspace{0.5cm}\\text{iff}\\hspace{0.5cm}\\sigma_1,\\sigma_2\\models_{S1S}T(\\varphi,t)\\varphi,\\hspace{1cm}\\text{where }\\ \\sigma_2:P\\mapsto\\lbrace i\\in\\mathbb{N}\\mid P\\in\\alpha(i)\\rbrace$$ $\\begin{array}{llll}\\hspace{1cm}\\cdot&amp;T(P,t)&amp;=&amp;t\\in P\\text{, for }P\\in AP\\newline\\hspace{1cm}\\cdot&amp;T(\\neg\\varphi,t)&amp;=&amp;\\neg T(\\varphi,t)\\newline\\hspace{1cm}\\cdot&amp;T(\\varphi\\wedge\\psi,t)&amp;=&amp;T(\\varphi,t)\\wedge T(\\psi,t)\\newline\\hspace{1cm}\\cdot&amp;T(\\bigcirc\\varphi,t)&amp;=&amp;T(\\varphi,S(t))\\newline\\hspace{1cm}\\cdot&amp;T(\\Diamond\\varphi,t)&amp;=&amp;\\exists x.(x\\geq t\\wedge T(\\varphi, x))\\newline\\hspace{1cm}\\cdot&amp;T(\\exists P.\\varphi,t)&amp;=&amp;\\exists P. T(\\varphi,t)\\newline\\end{array}$ Therefore, the language of $\\varphi$ is then defined by the S1S formula $T(\\varphi, 0)$. ExplanationWe apply every basic QPTL operators into $\\varphi$, and transform them using S1S operators. For proposition $P$, we can assign its value based on the temporal state when it holds. For example, for $\\alpha={PPQPQQQ\\dots}$, $P$ is in position $\\alpha[0], \\alpha[1], \\alpha[3]$. So $\\sigma_2:P\\mapsto\\lbrace0,1,3\\rbrace$And $T(P,t)$ is true when $t$ have value equals to {1,2,3}. For the Finally operator $\\Diamond$, it is expressed as true when the value is greater than or equals to t. Next chapter: S1S$_0$ and Büchi-recognizable Language Further Reading:","link":"/AGV/agv6-6/"},{"title":"AGV 6.7 -- S1S$_0$ and Büchi-recognizable Language","text":"Previous chapter: Express QPTL using S1S This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionTo prepare for the proof that every S1S-definable language is Büchi-recognizable, we show in the following lemma that we can focus on a restricted sublogic, called S1S$_0$, which is defined by the following grammar: $$\\varphi::=0\\in X\\mid x\\in Y\\mid x=0\\mid x=y\\mid x=S(y)\\mid \\neg\\varphi\\mid\\varphi\\wedge\\varphi\\mid\\exists x.\\varphi\\mid\\exists X.\\varphi$$ Membership tests $(\\in)$: variables $(x,y,\\dots)$ and $0$ only Equalities $(=)$: variables $(x,y,\\dots)$, $0$ and a single successor operation $(S(t))$ only i.e. $S(S(t))$ is not allowed. Complex formula in S1S can then be simplified by introducing additional variables. From S1S to S1S$_0$ $\\textbf{Lemma 6.1. } \\textit{For every S1S formula }\\varphi\\textit{ there is an S1S}_0\\text{ formula }\\varphi’\\textit{ such that }\\mathcal{L}(\\varphi)=\\mathcal{L}(\\varphi’).$ ProofWe rewrite a given S1S formula $\\varphi$ into the S1S$_0$ formula $\\varphi$’ using the following rewrite rules: S1S S1S_0 Explanation $S(t)\\in X$ $\\exists y. y=S(t)\\wedge y\\in X$ only $0$ and First-order variable $(x,y,\\dots)$ is allowed on the L.H.S of $\\in$ $0=x$ $x=0$ First-order variable has higher priority than $0$ $S(t)=0$ $0=S(t)$ successor operation not allowed on L.H.S $S(t)=x$ $x=S(t)$ successor operation not allowed on L.H.S $S(t)=S(t’)$ $t=t’$ successor operation not allowed on L.H.S $0=0$ $\\exists Y.0\\in Y\\vee0\\notin Y$ only accept the form $x=0$ $0=S(t)$ $\\exists y.y=S(t)\\wedge y=0$ only accept the form $x=0$ or $x=S(y)$ $t=S(0)$ $\\exists x.x=0\\wedge t=S(x)$ only accept the form $x=S(y)$ $t=S(S(t’)$ $\\exists y.y=S(t’)\\wedge t=S(y)$ only allow one successor operation From S1S-definable to Büchi-recognizable $\\textbf{Theorem 6.4. } \\textit{Every S1S-definable language is Büchi-recognizable.}$ ProofLet $\\varphi$ be an S1S-formula. We construct a Büchi automaton $\\mathcal{A}$ with $\\mathcal{L}(\\varphi)=\\mathcal{L}(\\mathcal{A})$. Step 1: We begin by translating $\\varphi$ into an equivalent S1S$_0$ according to the above Lemma 6.1. Step 2: Express every basic S1S$_0$ formula from the grammar above using Büchi automaton. Remember in last section, when we translate QPTL to S1S, we defined Second-order variable $X$ as atomic proposition of QPTL. Here same definitions continue: First of all, $A$ is the set of atomic propositions $(A\\subseteq AP)$ which is avaliable in the current state. For example, in state $q_2$, $A = \\lbrace X,Y\\rbrace$, where $AP = \\lbrace X,Y,Z\\rbrace$. Which means in $q_2$ there exists transitions only when proposition $X$ or $Y$ is $\\textit{true}$. Then, words are defined as a sequence containing atomic propositions. For example, a possible structure for word $\\alpha$ may look like this: $$\\alpha=\\lbrace XYYXXYY\\dots\\rbrace$$ Finally, our definition of Second-order variable is set of positions, and First-order variable are the positions. The relationship between proposition and variables in S1S is essentially $\\alpha[x] = X$, where $x$ is some First-order variable and $X$ is some Second-order variable. We can also use the S1S way to interpret, that proposition $X$ holds $\\textit{true}$ in position x and y, i.e. $X=\\lbrace x,y\\rbrace$. Therefore, when we see transitions like $\\lbrace A\\mid X\\in A\\rbrace$, it means if $X$ is part of the avaliable propositions for the current state and holds in certain letter(position), then the automata can take this transition as a path. (Here the position is not specified, will see more examples below) $0\\in X$: This statement holds $\\textit{true}$ iff. theres a word that contains $X$ in zero position. $x\\in Y$: This statement holds $\\textit{true}$ iff. theres a word that contains $Y$ in $x$ position. $x=0$: This statement holds $\\textit{true}$ iff. the word exist some propositions that only holds $\\textit{true}$ in $x$-th position, assign $x$ as $0$. $x=y$: This statement holds $\\textit{true}$ iff. the word exist some propositions that only holds $\\textit{true}$ in $x$-th and $y$-th position, which are indeed the same. $x=S(y)$: This statement holds $\\textit{true}$ iff. the word exist some propositions that only holds $\\textit{true}$ in $x$-th and $y$-th position, where $x$ and $y$ are different and $x$ is the next position after $y$. $\\varphi\\wedge\\psi$: Let $\\mathcal{A_\\varphi}$ and $\\mathcal{A_\\psi}$ be the automata constructed for $\\varphi$ and $\\psi$, respectively. We obtain the automaton $\\mathcal{A}_{\\varphi\\wedge\\psi}$ by constructing the automaton that recognizes the intersection of $\\mathcal{L(A_\\varphi)}$ and $\\mathcal{L(A_\\psi)}$.Below is the example of $0\\in X \\wedge x \\in Y$: $\\neg\\varphi$: let $\\mathcal{A_{\\varphi}}$ be the automaton constructed for $\\varphi$. We obtain the automaton $\\mathcal{A_{\\neg\\varphi}}$ by first constructing the automaton that recognizes the complement of $\\mathcal{L(A_{\\varphi})}$ and then intersecting it with $A_x$ for each free first-order variable $x$, which ensures that $x$ appears exactly once. Below we use $\\neg(x\\in Y)$ as an example:Notice that $(\\lbrace x,Y\\rbrace\\subseteq A)\\wedge(x\\in A)=\\lbrace x,Y\\rbrace\\subseteq A$: $\\exists X.\\varphi$: Let $\\mathcal{A_{\\varphi}}$ be the automaton constructed for $\\varphi$. We obtain the automaton $\\mathcal{A}_{\\exists X. \\varphi}$ for by eliminating $X$ from the input alphabet, i.e., we replace each transition $(q,A,q’)$ by $(q,A\\setminus\\lbrace X\\rbrace,q’)$.(Because the Büchi-recognizable Language only contains the set of free atomic propositions) $\\exists x. \\varphi$: Similarly, we replace each transition $(q,A,q’)$ by $(q,A\\setminus\\lbrace x\\rbrace,q’)$. Next chapter: Alternating Büchi Automata Further Reading:","link":"/AGV/agv6-7/"},{"title":"AGV 7.1 -- Alternating Büchi Automata","text":"Previous chapter: S1S$_0$ and Büchi-recognizable Language This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionLogics are often significantly more concise than automata. For example, in the translation from S1S to Büchi automata in the proof of Theorem 6.4, each negation increases the size of the Büchi automaton exponentially, resulting in a non-elementary number of states. The blow-up when translating LTL formulas is less dramatic, but still exponential. In this section, we show that the conciseness of the logic and the automata can be brought closer together when the automata are equipped with both nondeterministic and universal choices. Alternating AutomataNondeterministic and UniversalIn previous sections, we discussed a lot about Nondeterministic transitions. Here we introduce a new concept Universal transitions for our new automaton called alternating automaton. We allow for both types of choices by defining, for each state and input letter, a positive Boolean formula over the successor states. Choices Symbol Definition Nondeterministic disjunction $(\\vee)$ the suffix of an input word is accepted by SOME successor state Universal conjunction $(\\wedge)$ the suffix of an input word is accepted by ALL successor states Positive Boolean Formulas $\\ \\mathbb{B}^+(X)$ $\\textbf{Definition 7.1. } \\text{The }\\textit{positive Boolean formulas }\\text{over a set }X\\text{, denoted }\\mathbb{B}^+(X)\\text{, are the formulas}\\newline\\text{built from elements of }X\\text{, conjunction }\\wedge\\text{, disjunction }\\vee,\\textit{ true}\\text{ and }\\textit{false.}$ In our automata construction, elements of $X$ will be states. Let say we have a set of states $Y\\subseteq X$.We denote $Y\\models\\varphi$ if $Y$ satisfies a formula $\\varphi\\in\\mathbb{B^+}(X)$. In other words: all states in $Y$ will be assigned as $\\textit{true}$ by $\\varphi$, and all states in $X\\setminus Y$ will be assigned as $\\textit{false}$ by $\\varphi$. Trees and Runs $\\textbf{Definition 7.2. } \\text{ An } \\textit{Alternating automaton over infinite words }\\mathcal{A}\\newline\\text{ is a tuple }\\mathcal{A} = (\\Sigma,Q,q_0,\\delta,Acc)\\text{, where}\\newline\\begin{array}{ll}\\hspace{1cm} \\cdot \\ Q &amp;\\text{ is a finite set of states} \\newline\\hspace{1cm} \\cdot \\ q_0 \\in Q&amp; \\text{ is the initial states} \\newline\\hspace{1cm} \\cdot \\ \\delta:Q\\times\\Sigma\\rightarrow\\mathbb{B^+}(Q)&amp; \\text{ is the } \\textit{transition functions}\\text{, and} \\newline\\hspace{1cm} \\cdot \\ Acc \\subseteq Q^\\omega&amp; \\text{ is an accepting condition.}\\end{array}$ For alternating automata, runs generalize from sequences to trees. Here we define a tree as a prefix-closed subset, guarantees that all nodes must be able to trace all the back the root of the tree. prefix-closed subset:If a word $w=\\lbrace ab\\rbrace^\\omega$ is in the set, then all its prefix (i.e. $\\lbrace ab\\rbrace,\\lbrace aba\\rbrace,\\lbrace abab\\rbrace,\\dots$) must also be in the set. Name Symbol/Functions Definition Set of Squences $D^\\ast$ All possible sequence of every word by direction $D$ Set of Directions $D$ Branches possible in each node Tree $\\mathcal{T}$ Sequence of ONE word, a prefix-closed subset of $D^*$ Root $\\varepsilon$ The Empty Sequence Node $n\\in\\mathcal{T}$ Depends on the label, it may be the states or the letters Children of $n$ $\\text{children}(n)=\\lbrace n\\cdot d\\in \\mathcal{T}\\mid d\\in D\\rbrace$ Succssor states of $n$ over direction $d$ $Q$-labeled tree $(\\mathcal{T},\\ell)$ A tree that labels nodes with the states $\\Sigma$-labeled tree $(\\mathcal{T},\\ell)$ A tree that labels nodes with the input letters Labeling Function $\\ell:\\mathcal{T}\\rightarrow\\Sigma$ Label each node of the tree with the input letters for $\\Sigma$-labeled tree Below, we define a run of an alternating autoamton using $Q$-labeled tree: $\\textbf{Definition 7.3. } \\text{ A } \\textit{run }\\text{of an alternating automaton on a word }\\alpha\\in\\Sigma^\\omega\\newline\\text{ is a Q-labeled tree (T , r) with the following properties:}\\newline\\begin{array}{l}\\hspace{1cm} \\cdot \\ r(\\varepsilon)=q_0\\text{ and}\\newline\\hspace{1cm} \\cdot \\ \\text{for all }n\\in\\mathcal{T}\\text{, if }r(n)=q\\text{, then }\\lbrace r(n’)\\mid n’\\in\\text{children}(n)\\rbrace\\ \\text{ satisfies }\\delta(q,\\alpha(|n|)).\\end{array}$ ExampleThe following alternating Büchi automaton recognizes the language $L=((a+b)^\\ast b)^\\omega$. Universal choice $(\\wedge)$ are depicted by connecting the edges with a small arc $(\\blacksquare)$. The transition function $(\\delta)$ is given as follows: $\\delta(p,a)=p\\wedge q$, $\\delta(p,b)=p$, $\\delta(q,a)=q$, $\\delta(q,b)=\\textit{true}$. On the input word $\\alpha=(aab)^\\omega$, our automaton has the following run. Note that, in general, an alternating automaton may have more than one run on a particular word, or also no run at all. We use a dotted line to indicate that the subtree repeats infinitely often. Similar to DAG, we can apply the acceptance condition only on all infinite branches of the run tree. A branch of a tree $\\mathcal{T}$ is a maximal sequence of words $n_0n_1n_2\\dots$ such that $n_0=\\varepsilon$ and $n_{i+1}$ is a child of $n_i$ for $i\\geq0$. Obviously, if every infinite branch is accepting, then the entire tree is thus accepting, i.e. there’s no way to pick a non-accepting path for to be the run. $\\textbf{Definition 7.4. } \\text{A run }(\\mathcal{T},r)\\text{ is }\\textit{accepting }\\text{iff, for every infinite branch }n_0n_1n_2\\dots,$$$r(n_0)r(n_1)r(n_2)\\dots\\in Acc.$$ Next chapter: From LTL to Alternating Büchi Automata Further Reading:","link":"/AGV/agv7-1/"},{"title":"AGV -- (Exercise 7.2) LTL to Alternating Büchi Automata","text":"Previous Exercise: This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner This is an example exercise to express LTL formula into Alternating Büchi Automata. For further definitions, you may check Section 7.2. QuestionUse the construction from the lecture to construct an alternating Büchi automaton $\\mathcal{A}$ such that $$\\mathcal{L(A)=L}((\\Diamond p)\\ \\mathcal{U}\\ (\\square q))$$ Solutionwe build the following alternating Büchi automaton $\\mathcal{A} = (2^{p,q},Q,\\varphi,\\delta,\\small\\text{BÜCHI} \\normalsize(F))$ which recognizes the models of $\\varphi=(\\Diamond p)\\ \\mathcal{U}\\ (\\square q)$. First we consider the transition function $\\delta$ for an arbitrary symbol $a\\in2^{p,q}:$ $\\begin{array}{lll}\\hspace{1cm}\\bullet &amp;&amp;\\delta(\\square q,a)\\newline&amp;=&amp;\\delta(\\neg(\\textit{true}\\ \\mathcal{U}\\ \\neg q) ,a)\\newline&amp;=&amp;\\overline{\\delta(\\textit{true}\\ \\mathcal{U}\\ \\neg q ,a)}\\newline&amp;=&amp;\\overline{\\delta(\\neg q,a)\\vee(\\delta(\\textit{true},a)\\wedge(\\textit{true}\\ \\mathcal{U}\\ \\neg q))}\\newline&amp;=&amp;\\overline{\\delta(\\neg q,a)}\\wedge\\overline{(\\delta(\\textit{true},a)\\wedge (\\textit{true}\\ \\mathcal{U}\\ \\neg q))}&amp;(\\textit{true}\\wedge \\psi=\\psi)\\newline&amp;=&amp;\\overline{\\delta(\\neg q,a)}\\wedge\\overline{(\\textit{true}\\ \\mathcal{U}\\ \\neg q)}&amp;(\\text{Using line 1})\\newline&amp;=&amp;\\overline{\\delta(\\neg q,a)}\\wedge\\square q\\newline&amp;=&amp;\\left\\lbrace \\begin{array}{lll}\\square q&amp;\\text{if }q\\in a\\newline\\textit{false}&amp;\\text{if }q\\notin a\\newline\\end{array}\\right.\\end{array}\\ \\newline \\ \\newline\\begin{array}{lll}\\hspace{1cm}\\bullet &amp;&amp;\\delta(\\Diamond p,a)\\newline&amp;=&amp;\\delta(\\textit{true}\\ \\mathcal{U}\\ p ,a)\\newline&amp;=&amp;\\delta(p,a)\\vee(\\delta(\\textit{true},a)\\wedge\\Diamond p)&amp;(\\textit{true}\\wedge \\psi=\\psi)\\newline&amp;=&amp;\\delta(p,a)\\vee\\Diamond p\\newline&amp;=&amp;\\left\\lbrace \\begin{array}{lll}\\Diamond p&amp;\\text{if }p\\notin a\\newline\\textit{true}&amp;\\text{if }p\\in a\\newline\\end{array}\\right.\\end{array}$ By Substitution above result into $\\delta((\\Diamond p)\\ \\mathcal{U}\\ (\\square q),a)=\\delta(\\square q,a)\\vee(\\delta(\\Diamond p,a)\\wedge(\\Diamond p)\\ \\mathcal{U}\\ (\\square q))$, we have: $$\\delta((\\Diamond p)\\ \\mathcal{U}\\ (\\square q),a)=\\left\\lbrace\\begin{array}{lll}\\Diamond p\\wedge((\\Diamond p)\\ \\mathcal{U}\\ (\\square q))&amp;\\text{if }a =\\varnothing &amp;(\\vee\\ \\textit{false}\\text{ is omitted.})\\newline(\\Diamond p)\\ \\mathcal{U}\\ (\\square q)&amp;\\text{if }a =\\lbrace p\\rbrace&amp;(\\wedge\\ \\textit{true}\\text{ is omitted.})\\newline\\square q\\vee(\\Diamond p\\wedge((\\Diamond p)\\ \\mathcal{U}\\ (\\square q)))&amp;\\text{if }a =\\lbrace q\\rbrace\\newline\\square q\\vee((\\Diamond p)\\ \\mathcal{U}\\ (\\square q))&amp;\\text{if }a =\\lbrace p,q\\rbrace&amp;(\\wedge\\ \\textit{true}\\text{ is omitted.})\\newline\\end{array}\\right.$$ For each case, when we see $\\vee$, that’s a nondeterministic transitions, we need draw two seperate transitions for each successors. On the other hand $\\wedge$ is a universal transitions, it is a single transition towards both successors, we split the extra branches from the path to indicate that. Thus we have the following automaton $\\mathcal{A}$: Next Exercise: Further Reading:","link":"/AGV/agv7-2-eg/"},{"title":"AGV 7.2 -- From LTL to Alternating Büchi Automata","text":"Previous chapter: Alternating Büchi Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIt is usually much simpler to translate a logical formula into an alternating automaton than into a nondeterministic automaton. We illustrate this with the translation of LTL formulas into equivalent alternating Büchi automata. The states are simply the subformulas of the given formula and their negations (this set is called the closure of the formula). The transition function is derived from the expansion laws of the logic. For example, an Until formula $\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$ holds if: $\\varphi_2$ holds or $\\varphi_1$ holds and the entire formula holds in the next step. The boolean formula produced by the transition function from the state $\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$ is therefore: a disjunction $(\\wedge)$ between the transition function for $\\varphi_2$ and a conjunction $(\\vee)$ between the transition function for $\\varphi_1$ and the state $\\varphi_1\\ \\mathcal{U}\\ \\varphi_2$. From LTL to Alternating Automata $\\textbf{Construction 7.1. }\\text{Let }\\varphi\\text{ be an LTL formula. We construct the alternating Büchi automaton }\\newline\\mathcal{A_\\varphi}=(\\Sigma,Q,\\varphi,\\delta,\\small\\text{BÜCHI} \\normalsize(F))\\text{ using:}$$\\begin{array}{ll}\\hspace{0.5cm} \\cdot \\ Q = \\text{closure}(\\varphi):=\\lbrace\\psi,\\neg\\psi\\mid\\psi\\text{ is subformula of }\\varphi\\rbrace \\newline\\hspace{0.5cm} \\cdot \\ \\delta(p,a)= \\left\\lbrace \\begin{array}{ll}\\textit{true}&amp;\\text{if }p\\in a\\newline \\textit{false}&amp;\\text{if }p\\not\\in a\\end{array}\\right. &amp;\\cdot \\ \\delta(\\neg\\psi,a)=\\overline{\\delta(\\psi,a)}\\newline\\hspace{0.5cm} \\cdot \\ \\delta(\\psi_1\\wedge\\psi_2,a)=\\delta(\\psi_1,a)\\wedge\\delta(\\psi_2,a) &amp;\\cdot \\ \\delta(\\psi_1\\vee\\psi_2,a)=\\delta(\\psi_1,a)\\vee\\delta(\\psi_2,a)\\newline\\hspace{0.5cm} \\cdot \\ \\delta(\\psi_1\\ \\mathcal{U}\\ \\psi_2,a)=\\delta(\\psi_2,a)\\vee(\\delta(\\psi_1,a)\\wedge\\psi_1\\ \\mathcal{U}\\ \\psi_2)&amp;\\cdot \\ \\delta(\\bigcirc\\psi,a)=\\psi\\newline\\hspace{0.5cm} \\cdot \\ F = \\lbrace\\neg(\\psi_1\\ \\mathcal{U}\\ \\psi_2)\\in\\text{clousure}(\\varphi)\\rbrace\\end{array}\\newline$$\\text{where we define }\\overline{\\varphi}=\\neg\\varphi\\text{ for all other }\\psi\\in Q\\text{ and }\\overline{\\ \\cdot\\ }\\text{ for }\\psi,\\psi_1,\\psi_2\\in Q\\text{ via:}$$\\begin{array}{ll}\\hspace{0.5cm} \\cdot \\ \\overline{\\neg\\varphi}=\\varphi\\hspace{0.5cm} \\cdot \\ \\overline{\\psi_1\\wedge\\psi_2}=\\overline{\\psi_1}\\vee\\overline{\\psi_2}\\hspace{0.5cm} \\cdot \\ \\overline{\\psi_1\\vee\\psi_2}=\\overline{\\psi_1}\\wedge\\overline{\\psi_2}\\hspace{0.5cm} \\cdot \\ \\overline{\\textit{true}}=\\overline{\\textit{false}}\\hspace{0.5cm} \\cdot \\ \\overline{\\textit{false}}=\\overline{\\textit{true}}\\end{array}$ Explanation and Examples in Human languageRemember that $\\delta$ function returns set of states. As we can see most of the function here don’t actually have a designated successor except for atomic proposition $p$, neXt $\\bigcirc$, and Until $\\mathcal{U}$. For example, here are simple steps to construct a automaton for a formula $p\\wedge\\bigcirc q$: List of all possible atomic propositions input: $\\sigma=\\lbrace\\lbrace\\varnothing\\rbrace,\\lbrace p\\rbrace,\\lbrace q\\rbrace,\\lbrace p,q\\rbrace\\rbrace$ Write a truth table basic on the formula and its corresponding function $\\delta$:$\\ \\newline\\hspace{1cm}\\delta(p\\wedge\\bigcirc q,\\sigma) = \\delta(p,\\sigma)\\wedge\\delta(\\bigcirc q,\\sigma)=\\delta(p,\\sigma)\\wedge q\\newline\\ \\newline\\hspace{1cm}\\cdot \\ \\delta(\\bigcirc q,\\sigma)=\\left\\lbrace \\begin{array}{ll}q&amp;\\text{if }\\sigma=\\lbrace\\varnothing\\rbrace,\\lbrace q\\rbrace,\\lbrace p\\rbrace,\\lbrace p,q\\rbrace\\end{array} \\right. \\newline\\ \\newline\\hspace{1cm}\\cdot \\ \\delta(p,\\sigma)=\\left\\lbrace\\begin{array}{ll}\\textit{false}&amp;\\text{if }\\sigma=\\lbrace\\varnothing\\rbrace,\\lbrace q\\rbrace\\newline\\textit{true}&amp;\\text{if }\\sigma=\\lbrace p\\rbrace,\\lbrace p,q\\rbrace\\end{array}\\right.\\hspace{1cm}\\cdot \\ \\delta(q,\\sigma)=\\left\\lbrace\\begin{array}{ll}\\textit{false}&amp;\\text{if }\\sigma=\\lbrace\\varnothing\\rbrace,\\lbrace p\\rbrace\\newline\\textit{true}&amp;\\text{if }\\sigma=\\lbrace q\\rbrace,\\lbrace p,q\\rbrace\\end{array}\\right.\\newline\\ \\newline\\hspace{1cm}\\therefore\\ \\delta(p\\wedge\\bigcirc q,\\sigma)=\\left\\lbrace\\begin{array}{ll}\\textit{false}\\wedge q = \\textit{false}&amp;\\text{if }\\sigma=\\lbrace\\varnothing\\rbrace,\\lbrace q\\rbrace\\newline\\textit{true}\\wedge q = q&amp;\\text{if }\\sigma=\\lbrace p\\rbrace,\\lbrace p,q\\rbrace\\end{array}\\right.$ Initial state is always the orginal formula, then we add extra state according to the truth table we constructed. Here, our graph have no universal or nondeterministic transitions: For a more complicated example, i.e. $((\\Diamond p)\\ \\mathcal{U}\\ (\\square q))$, check here as an extra material $\\textbf{Theorem 7.1. } \\textit{For every LTL formula }\\varphi\\textit{, there is an alternating Büchi automaton }\\mathcal{A_\\varphi}\\newline\\textit{with }\\mathcal{L(A_\\varphi)} = \\mathcal{L(\\varphi)}.$ ProofWe can simply prove this by induction. First, any LTL formula $\\varphi$ can be recursively seperate into smaller subformula, namely $\\psi$. Similarly we can construct automaton $\\mathcal{A^\\psi_\\varphi}$ from $\\mathcal{A_\\varphi}$ according to above construction 7.1. By structural induction on $\\psi$, we can then prove that $\\mathcal{L(A^\\psi_\\varphi)=L(A_\\varphi)}$. Next chapter: Translating Alternating to Nondeterministic automata Further Reading:","link":"/AGV/agv7-2/"},{"title":"AGV 7.3 -- Translating Alternating to Nondeterministic automata","text":"Previous chapter: From LTL to Alternating Büchi Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionThe translation from alternating to nondeterministic automata is based on a representation of runs as directed acyclic graphs (DAGs). The idea is similar to the DAG representation we used in the complementation construction for nondeterministic Büchi automata in Section 5. There the DAG was used to represent the set of all runs of the nondeterministic automaton. The complement automaton would then ”guess” the DAG level-by-level. Here, the DAG is used to represent the branches of a (single) run of the alternating automaton. The idea is illustrated in the following example. Example $\\textbf{Definition 7.5. } \\text{ A } \\textit{run DAG }\\text{ of an alternating Büchi automaton }\\mathcal{A}\\text{ on an infinite word }\\alpha\\text{ is a}\\newline\\text{DAG }(V,E)\\text{, with }V\\subseteq Q\\times\\mathbb{N}\\text{ and }(q_0,0)\\in V\\text{, where}$ $\\begin{array}{ll} \\hspace{0.5cm} \\cdot &amp; E\\subseteq \\bigcup_{i\\in\\mathbb{N}}(Q\\times\\lbrace i\\rbrace)\\times(Q\\times\\lbrace i+1\\rbrace) \\newline \\hspace{0.5cm} \\cdot &amp; \\forall(q,i)\\in V \\ . \\ \\exists Y\\subseteq Q\\text{ s.t. } \\newline &amp; Y\\models\\delta(q,\\alpha(i)),Y\\times\\lbrace i+1\\rbrace\\subseteq V\\text{ and }\\lbrace(q,i)\\rbrace\\times (Y\\times\\lbrace i+1\\rbrace)\\subseteq E.\\end{array}$ Every vertices are expressed in (state, letter index) Edges are all possible paths from current state $q$ A run DAG is accepting if every infinite path has infinitely many visits to $F\\times\\mathbb{N}$. Our construction of the nondeterministic automaton will be based on run DAGs rather than trees.However, not every run tree can be represented as a DAG. This is illustrated by the following example: Example (cont.)The following run tree cannot be represented as a DAG, because there are two nodes that are both labeled with $\\mathbf{q}$ has different children (one transits to $p$ and the other to $q$). We say this tree has memory. Similar Nodes and Memoryless TreeWe call two nodes $x_1, x_2\\in\\mathcal{T}$ in a run tree $(\\mathcal{T},r)$ similar if $|x_1| = |x_2|$ (same index), and $r(x_1) = r(x_2)$ (same state). We call a run tree memoryless if the subtrees starting in similar nodes have the same labels.Memoryless run trees can be represented as DAGs. $\\textbf{Definition 7.5. } \\text{ A run tree }(\\mathcal{T},r)\\text{ is }\\textit{memoryless }\\text{ if for all similar nodes }x_1\\text{ and }x_2\\text{ and for all}\\newline y\\in D^\\ast\\text{ we have that }(x_1\\cdot y\\in\\mathcal{T}\\text{ iff }x_2\\cdot y\\in\\mathcal{T} )\\text{ and } r(x_1\\cdot y) = r(x_2\\cdot y).$ The following theorem shows that whenever there is an accepting run tree, there is also an accepting run tree that is memoryless. Hence, we can show some word are accpeted by the automaton by showing the existence of a memoryless run tree, or, equivalently, the existence of an accepting run DAG. $\\textbf{Theorem 7.2. } \\textit{ If an alternating Büchi automaton }\\mathcal{A}\\textit{ accepts a word }\\alpha\\textit{, then there exists a}\\newline\\textit{memoryless accepting run of }\\mathcal{A}\\textit{ on }\\alpha.$ Proof of Memoryless Run existsLet $(\\mathcal{T},r)$ be an accepting run tree on $\\alpha$ with directions $D$. We construct a memoryless run tree $(\\mathcal{T’},r’)$ by copying from $(\\mathcal{T},r)$. Inuitively, we pick, whenever there are multiple occurrences of the same state in $(\\mathcal{T},r)$, the occurrence where the last visit to the accepting states was the longest time ago. $\\gamma:\\mathcal{T}\\rightarrow\\mathbb{N}$: number of steps since the last visit to $F$ Initiate the run tree (root) with zero steps: $\\gamma(\\varepsilon)=0$ For $n$’s children $d$, it increase one step from $n$, unless $n$ is an accepting state then it resets to zero $\\gamma(n\\cdot d)=\\left\\lbrace\\begin{array}{ll} \\gamma(n)+1 &amp; \\text{if }r(n)\\notin F\\newline 0 &amp; \\text{if }r(n)\\in F\\end{array}\\right.$ Now, we can define the tree node that the last visit to the accepting states was the longest time ago based on $\\gamma$. $\\Delta:Q\\times\\mathbb{N}\\rightarrow\\mathcal{T}$: mapping to return node visit $F$ longest time agoFor state $q\\in Q$ and level $n\\in\\mathbb{N}$, it returns a tree node $y\\in\\mathcal{T}$ that is the leftmost and visit $F$ longest time ago: $$\\begin{array}{ll}\\Delta(q,n)=&amp;\\text{the leftmost }y\\in\\mathcal{T}\\text{ with }|y|=n\\ \\text{ s.t. }\\ r(y)=q\\newline &amp; \\text{and }\\forall z\\in\\mathcal{T}.\\ |z|=n\\wedge r(z)=q\\Rightarrow\\gamma(z)\\leq\\gamma(y)\\end{array}$$ Construction of Memoryless Run Tree $(\\mathcal{T’},r’)$We now construct $(\\mathcal{T’},r’)$ by copying from the nodes in $(\\mathcal{T},r)$ indicated by $\\Delta$: Both trees have same initial states (root): $\\varepsilon\\in\\mathcal{T’}\\text{ and }r’(\\varepsilon)=r(\\varepsilon)$ Children node $d$ in $\\mathcal{T’}$ are $d$ with longest steps, and of course a child of $n$: $d\\in\\mathcal{T’}\\text{ if and only if }\\Delta(r’(n),|n|)\\cdot d\\in\\mathcal{T}\\text{ and } r’(n\\cdot d)=r(\\Delta(r’(n),|n|)\\cdot d)$ $(\\mathcal{T’},r’)$ is a run of $\\mathcal{A}$ on $\\alpha$ The root is labeled by the initial state: $r’(\\varepsilon)=r(\\varepsilon)=q_0$. For some node $n\\in\\mathcal{T’}$, let node $q_n=\\Delta(r’(n),|n|)$ ($q_n$ visits $F$ longest time ago among all $n$) Then, the set $\\lbrace r(q_n\\cdot d)\\mid d\\in D, q_n\\cdot d \\in \\mathcal{T}\\rbrace$ satisfies $\\delta(r(q_n), \\alpha(|q_n|))$ (path of $q_n$ in original tree exists) Therefore $\\lbrace r’(n\\cdot d)\\mid d\\in D, n\\cdot d \\in \\mathcal{T’}\\rbrace\\models\\delta(r’(n), \\alpha(|n|))$ (by the construction above) $(\\mathcal{T’},r’)$ is acceptingFirst, we show that for every $n\\in\\mathcal{T’}$, the node obtained from the mapping is indeed the longest path, i.e. $\\gamma(n)\\leq\\gamma(\\Delta(r’(n),|n|))$. This is shown by induction on the length of $n$: for $n=\\varepsilon$ we have that $\\gamma(n)=0$ for $n=n’\\cdot d$ (where $d\\in D$) we have: if $r(n’)\\in F$, then $\\gamma(n)=0$ if $r(n’)\\notin F$, then $$\\begin{array}{lcl} \\gamma(\\Delta(r’(n’\\cdot d),|n’\\cdot d|))&amp;\\overset{\\text{Def. }\\Delta}{\\geq}&amp;\\gamma(\\Delta(r’(n’),|n’|)\\cdot d)\\overset{\\text{Def. }\\gamma}{=}1+\\gamma(\\Delta(r’(n’),|n’|))\\newline&amp;\\overset{\\text{IH}}{\\geq}&amp;1+\\gamma(n’)\\overset{\\text{Def. }\\gamma}{=}\\gamma(n’\\cdot d)\\end{array}$$ (Last visit of the Mapping of children of $n’$ $\\geq$ Last visit of mapping of $n$’s children. By induction hypothesis, the children of $n’$ through the mapping $\\Delta$ is never smaller than any other possible children of $m’$) Assume $(\\mathcal{T’},r’)$ constructed from a accepting $(\\mathcal{T},r)$ is not accepting. Then there is an infinite branch that does not visit $F$ infinitely often, i.e. $n_0, n_1, n_2,\\dots$ in $\\mathcal{T’}$ and $\\exists k\\in\\mathbb{N}$ such that $\\forall j\\geq k. r’(n_j)\\notin F$. Let $m_i=\\Delta(r’(n_i), |n_i|)\\text{ for }i\\geq k$. We have, $$\\begin{array}{ccccc} \\gamma(n_k)&amp;&lt;&amp;\\gamma(n_{k+1})&amp;&lt;&amp;\\dots\\newline /\\mathord{\\bigwedge} &amp;&amp; /\\mathord{\\bigwedge}\\newline\\gamma(m_k)&amp;&lt;&amp;\\gamma(m_{k+1})&amp;&lt;&amp;\\dots\\end{array}$$ So, for any $j\\geq k$ it holds that $\\gamma(m_j)\\geq j−k$ (because there are at least $j-k$ steps without visiting $F$). Since $\\mathcal{T}$ is finitely branching, there must be a branch with an infinite suffix of non-$F$ labeled positions. So we can always find the branch in $\\mathcal{T}$ identical with the path with $m_i$ This contradicts the assumption $(\\mathcal{T},r)$ is accepting. $\\textbf{Corollary 7.1. }\\textit{A word }\\alpha\\textit{ is accepted by an alternating Büchi automaton }\\mathcal{A}\\textit{ if and only if}\\newline\\mathcal{A}\\textit{ has an accepting run DAG on }\\alpha$ Translating alternating to nondeterministic automataWe are now ready to translate an alternating Büchi automaton into an equivalent nondeterministic Büchi automaton. The construction is due to Miyano and Hayashi (1984). $\\textbf{Construction 7.2. }\\text{For an alternating Büchi automaton }\\mathcal{A}=(\\Sigma,Q,q_0,\\delta,\\small\\text{BÜCHI} \\normalsize(F))\\text{, we}\\newline\\text{construct a nondeterministic Büchi automaton }\\mathcal{A’}=(\\Sigma,Q’,I’,T’,\\small\\text{BÜCHI} \\normalsize(F’))\\text{ with }\\mathcal{L(A)}=\\newline\\mathcal{L(A’)}\\text{ as follows:}$ $\\begin{array}{ll}\\hspace{0.5cm} \\cdot \\ Q’&amp;= 2^Q\\times2^Q \\newline\\hspace{0.5cm} \\cdot \\ I’&amp;= \\lbrace(\\lbrace q_0\\rbrace,\\varnothing)\\rbrace\\newline\\hspace{0.5cm} \\cdot \\ T’&amp;= \\lbrace((X,\\varnothing),\\sigma,(X’,X’\\setminus F))\\mid X’\\models\\wedge_{q\\in X}\\delta(q,\\sigma)\\rbrace\\ \\cup\\newline &amp;\\hspace{0.5cm} \\lbrace((X,W),\\sigma,(X’,W’\\setminus F))\\mid W\\neq\\varnothing,W’\\subseteq X’, X’\\models\\wedge_{q\\in X}\\delta(q,\\sigma),W’\\models\\wedge_{q\\in W}\\delta(q,\\sigma)\\rbrace\\newline\\hspace{0.5cm} \\cdot \\ F’&amp;= \\lbrace(X,\\varnothing)\\mid X\\subseteq Q\\rbrace\\end{array}$ Modified Example from section 7.1 Example Explaination $Q’$ $X=\\lbrace\\lbrace p\\rbrace,\\lbrace q\\rbrace,\\lbrace p,q\\rbrace\\rbrace$ $W = \\lbrace\\varnothing,\\lbrace q\\rbrace\\rbrace$ States Q’ is a tuple consist of two set of states in $Q$, first one must be non-empty and the second one does not contain any states $q\\in F$ $I’$ $I’=(\\lbrace p\\rbrace,\\varnothing)$ 1st element is the initial state while the second element is empty $T’$ $(\\lbrace p\\rbrace,\\varnothing)\\overset{b}{\\longrightarrow}(\\lbrace p,q\\rbrace,$ $\\lbrace \\lbrace p,q\\rbrace\\setminus\\lbrace p\\rbrace\\rbrace)=(\\lbrace p,q\\rbrace,\\lbrace q\\rbrace)$ $X$ represent the behaviour of the original automata, while $W$ tracks whether the original accepting states are visited after the transition. $T’$ $(\\lbrace p,q\\rbrace,\\lbrace q\\rbrace) \\overset{b}{\\longrightarrow}(\\lbrace p,q\\rbrace,\\varnothing)$ $(\\delta(p,b)=p\\wedge q,\\delta(q,b)=\\textit{true}=\\varnothing$ $\\therefore\\delta(p\\wedge q,b)=p\\wedge q.)$ If $W$ is empty, it means accepting states is reached and next transition we start again on tracking. If $W$ is non-empty, then its behaviour align with $X$ $F’$ $F’=\\lbrace(\\lbrace p\\rbrace,\\varnothing), (\\lbrace p,q\\rbrace,\\varnothing)\\rbrace$ The run/word is aceepting if $W$ is empty infinite often. $\\textbf{Theorem 7.3. } \\text{(Miyano and Hayashi, 1984). }\\textit{For every alternating Büchi automaton }\\mathcal{A}\\textit{, we can}\\newline\\textit{effectively construct a nondeterministic Büchi automaton }\\mathcal{A’}\\textit{ with }\\mathcal{L(A)}=\\mathcal{L(A’)}.$ Proof $\\mathcal{L(A)}\\supseteq\\mathcal{L(A’)}$ (all word accepted by $\\mathcal{L(A’)}$ must also be accepted by $\\mathcal{L(A)}$): Let $\\alpha\\in\\mathcal{L(A’)}$ with an accepting run $r’=(X_0,W_0)(X_1,W_1)(X_2,W_2)\\dots$where $W_0 =\\varnothing$ and $X_0=\\lbrace q_0\\rbrace$. We construct the run DAG $(V,E)$ for $\\mathcal{A}$ on $\\alpha$: All vertices in DAG in level $i$ come from the tree in level $i$: $V=\\lbrace (x,i)\\mid i\\in\\mathbb{N}, x\\in X_i\\rbrace$ $X_i$ represent the behaviour of the automaton and $W_i$ is for tracking whether the states are accepting. If the state is not tracked by $W_i$, i.e. $(x\\in X_i \\setminus W_i)$, that means it is accepted, no extra tracking is needed, If it is tracked, i.e. $(x\\in W_i)$, it either moves to some accepting state or it is continue tracked $(F\\cup W_{i+1})$: $E=\\lbrace((x,i),(x’,i+1))\\mid i\\in\\mathbb{N}, x\\in X_i \\setminus W_i, x’\\in X_{i+1}\\rbrace\\cup\\newline\\hspace{0.95cm}\\lbrace((x, i),(x’,i+1))\\mid i\\in\\mathbb{N}, x\\in W_i, x’\\in X_{i+1}\\cap (F\\cup W_{i+1})\\rbrace$ First, we show that $(V,E)$ is a run DAG: $(q_0,0)\\in V$ and for every $(x,i)\\in V$: The state is accepted, any transitions from here are valid: if $x\\in X_i\\setminus W_i,\\ X_{i+1}\\models\\delta(x,\\alpha(i))$; The state is tracked, only transitions to accepting state or continue tracked are valid: if $x\\in W_i, x’\\in X_{i+1}\\cap (F\\cup W_{i+1})\\models\\delta(x,\\alpha(i))$. Since the automata is accepting, so it has $W_i=\\varnothing$ exist for infinitely many $i$. So the run DAG is accepting, because there is $x\\in X_i\\setminus W_i$ infinitely often, every path through the run DAG visits $F$ infinitely often. $\\mathcal{L(A)}\\subseteq\\mathcal{L(A’)}$ (all word accepted by $\\mathcal{L(A)}$ must also be accepted by $\\mathcal{L(A’)}$): Let $\\alpha\\in\\mathcal{L(A)}$ and $(V,E)$ be an accepting run DAG of $\\mathcal{A}$ on $\\alpha$.We construct a run $r’=(X_0,W_0)(X_1,W_1)(X_2,W_2)\\dots$ on $\\mathcal{A’}$ as follows: $X_0=\\lbrace q_0\\rbrace$ and $W_0=\\varnothing$ We simulate the behaviour using $X_i$: for $i\\geq 0$, let $X_{i+1} = \\lbrace x’\\in Q\\mid ((x,i),(x’,i+1))\\in E, x\\in X_i\\rbrace$ If the state is accepted, align the behaviour with $X$:$W_{i+1} = X_{i+1} \\setminus F$ if $W_i=\\varnothing$ If not, track and see if the accepting condition is fulfiled: $W_{i+1} = \\lbrace x’\\in Q \\setminus F \\mid\\exists(x,i)\\in V, ((x,i),(x’,i+1))\\in E,x\\in W_i\\rbrace$. Clearly, $r’$ is a run: it starts with $(\\lbrace q_0\\rbrace,\\varnothing)$ and obeys $T’$. That is, $(X_{i+1},W_{i+1})$ contains states in $\\delta(x,\\alpha(i))$: For $x\\in X_i\\setminus W_i$, we have that $X_{i+1}\\models\\delta(x,\\alpha(i))$; For $x\\in W_i$, $X_{i+1}\\cap (F\\cup W_{i+1})$ satisfies $\\delta(x,\\alpha(i))$. The run $r’$ is accepting, otherwise some state $(X_{i+1},W_{i+1})$ is rejected, and thus rejects the path in $(V,E)$. Example with the construction $\\textbf{Corollary 7.2. }\\textit{A language is }\\omega\\textit{-regular if and only if it is recognizable by an alternating Büchi automaton.}$ SummaryIn this section, we have proved that any $\\omega$-regular language can be recongnized by some alternating Büchi automaton. Let see how it is done. Every accepted word in an alternating Büchi automaton has an Memoryless Accepting Run (Theorem 7.2). Every Memoryless Run Tree can be represented as a Run DAG Thus, Every accepted word in an alternating Büchi automaton has an Accepting Run DAG (Corollary 7.1). For every Alternating Büchi Automaton there exists a Nondeterministic Büchi Automaton (Theorem 7.3). An $\\omega$-language is Büchi-recognizable iff it is $\\omega$-regular (Büchi’s Characterization Theorem) (Theorem 3.6). An $\\omega$-language is Alternating Büchi-recognizable iff it is $\\omega$-regular (Corollary 7.2). Next chapter: Linear Arithmetic (Theory) Further Reading:","link":"/AGV/agv7-3/"},{"title":"AGV 8.2 -- Encoding Real Numbers","text":"Previous chapter: Linear Arithmetic (Theory) This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn last section, we have seen the definiton of Linera Arithmetic, which is similar to S1S. Let see how we can apply it in math and moreover, the connection between logic and automata. Encoding Real NumbersWe can encode any real number $x\\in\\mathbb{R}$ into the form $(0+1)(0+1)^\\ast{$}(0+1)^\\omega$ in two steps: Step 1For a real number $x\\in\\mathbb{R}$, represent it as a pair $(x_I,x_F)$ so that $x=x_I+x_F$.integer part = $x_I\\in\\mathbb{Z}$, and fractional part = $x_F\\in[0, 1]$, a real number between $0$ and $1$ (both inclusive) For example, the $1.5 = (1, 0.5)$, and $-{2\\over 3}=(−1, {1\\over3})$. Note that integers always have two different representations depends on setting the fractional part as $0$ or $1$, and Negative sign can only be represented using the integer part. For example $3=(2,1)$ or $(3,0)$ Step 2Then we can further encode $(x_I,x_F)$ as an infinite word $w_I{$}\\beta_F$,$x_I$ is encoded as $w_I=a_na_{n−1}\\dots a_0\\in\\lbrace 0,1\\rbrace^\\ast$, a finite nonempty word, and$x_F$ is encoded as $\\beta_F=b_1b_2\\dots\\in\\lbrace 0,1\\rbrace^\\omega$, an infinite word such that $$x_I=-a_n\\cdot 2^n+ \\sum_{i=0}^{n-1}a_i\\cdot 2^i\\hspace{2cm}\\text{and}\\hspace{2cm}x_F= \\sum_{i=1}^{\\infty}b_i\\cdot 2^{-i}$$ Note that now every pair has (infinitely) many different encodings because digits are forever:for example, $(0,{1\\over2})$ is encoded by all words in $0^\\ast0{$}10^\\omega = {1\\over2}+0+0+\\dots$ and $0^\\ast0{$}01^\\omega=0+{1\\over4}+{1\\over8}+\\dots$. As you may notice, we can always increase the length of the finite integer part by ‘padding’ arbitrary finitely long $0$ and $1$ on positive and negative number respectively. This is becasue of the following property of the geometric-series sum: for all $a$, $n$, $k$, we have that: $$-a_n\\cdot 2^n=-a\\cdot 2^{n+k}+ \\sum_{j=0}^{k}a\\cdot 2^{n+j}$$ For example, $110{$}0^\\omega=-1\\ast2^2+1\\ast2^1=-2$, $11110{$}0^\\omega=-1\\ast2^4+1\\ast2^3+1\\ast2^2+1\\ast2^1=-2$, Example $x\\in\\mathbb{R}$ $(x_I,x_F)$ $w_I{$}\\beta_F$ $1$ $(1, 0)$ $0^\\ast01{$}0^\\omega$ and $0^\\ast0{$}1^\\omega$ ${4\\over 3}$ $(1, {1\\over 3})$ $0^\\ast01{$}(01)^\\omega$ $-{3\\over 2}$ $(-2, {1\\over 2})$ $1^\\ast10{$}10^\\omega$ $0^\\ast/1^\\ast$ may be empty so make sure there is at least one $0$ or $1$ to ensure it is positive or negative, respectively. Encoding Valuations $\\sigma:V’\\rightarrow\\mathbb{R}$Assume there’s an arbitrary ordering of the free variables $V’=\\lbrace x_0,x_1,\\dots,x_k\\rbrace$. The valuation is then encoded as a word $\\alpha_\\sigma$ over the alphabet $\\lbrace0,1\\rbrace^k\\cup\\lbrace{$}\\rbrace^k$. Each letter of the word $\\alpha_\\sigma$ is a tuple, where the $i$-th position indicates the encoding of $x_i$. Also, we ensure that the encodings of all $x_i$ synchronize on the separation symbol ${$}$ Continue the example, one possible encoding of the valuation $x_1\\mapsto1$, $x_2\\mapsto{4\\over3}$, $x_3\\mapsto-{3\\over2}$ is the infinite word $$\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}{$}\\newline {$}\\newline {$}\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\left(\\begin{bmatrix}0\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\right)^\\omega$$ In the next section, we will illustrate an example of translation from linear arithmetic to automata. Next chapter: Translation from Linear Arithmetic to Automata Further Reading:","link":"/AGV/agv8-2/"},{"title":"AGV 8.1 -- Linear Arithmetic (Theory)","text":"Previous chapter: Translating Alternating to Nondeterministic automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn the previous sections, we have seen that the automata machinery can be applied to logical problems by translating formulas into automata. We now study another application of the powerful connection between logic and automata, this time in the setting of real numbers. Theory of Linear ArithmeticDefinitions and SemanticsLet $V$ be a set of (first-order) variables. The terms of linear arithmetic are defined by the following grammar: $$t::=0\\mid1\\mid x\\mid t+t$$ The formulas of linear arithmetic are defined by the following grammar, where $x\\in V$ is a variable: $$\\varphi::=t\\leq t\\mid\\neg\\varphi\\mid\\varphi\\wedge\\varphi\\mid\\exists x.\\varphi$$ Additionally, we allow the usual boolean connectives and introduce the following abbreviations: $$\\begin{array}{lrllllrll}\\cdot&amp;n&amp;:=&amp;\\overbrace{1+1+\\dots+1}^{n\\text{ times}},&amp;\\hspace{2cm}&amp;\\cdot&amp;nx&amp;:=&amp;\\overbrace{x+x+\\dots+x}^{n\\text{ times}},\\newline\\cdot&amp;t\\geq t’&amp;:=&amp;t’\\leq t,&amp;\\hspace{2cm}&amp;\\cdot&amp;t&lt;t’&amp;:=&amp;t\\leq t’\\wedge\\neg(t=t’)\\newline\\cdot&amp;t=t’&amp;:=&amp;t\\leq t’\\wedge t\\geq t’,&amp;\\hspace{2cm}&amp;\\cdot&amp;t&gt;t’&amp;:=&amp;t’&lt;t\\end{array}$$ The semantics of a formula is given relative to a valuation $\\sigma:V\\rightarrow\\mathbb{R}$ that assigns to each variable a real number. The value of a term is then defined as follows: $$\\begin{array}{llll}\\hspace{1cm}\\cdot\\ \\lbrack 0\\rbrack_{\\sigma} = 0 &amp;\\hspace{1cm}\\cdot\\ \\lbrack 1\\rbrack_{\\sigma} = 1 &amp;\\hspace{1cm}\\cdot\\ \\lbrack x\\rbrack_{\\sigma} = \\sigma(x) &amp;\\hspace{1cm}\\cdot\\ \\lbrack t+u\\rbrack_{\\sigma} =\\lbrack t\\rbrack_{\\sigma}+\\lbrack u\\rbrack_{\\sigma}\\end{array}$$ $\\models$ is the smallest relation that satisfies the following: $$\\begin{array}{lllllllllllll}\\cdot&amp;\\sigma\\models t\\leq u&amp;\\text{ iff }&amp;\\lbrack t\\rbrack_{\\sigma}\\leq\\lbrack u\\rbrack_{\\sigma}\\newline\\cdot&amp;\\sigma\\models\\neg\\varphi&amp;\\text{ iff }&amp;\\sigma\\neg\\models\\varphi\\newline\\cdot&amp;\\sigma\\models\\varphi_0\\wedge\\varphi_1&amp;\\text{ iff }&amp;\\sigma\\models\\varphi_0\\text{ and }\\sigma\\models\\varphi_1\\newline\\cdot&amp;\\sigma\\models\\exists x.\\varphi&amp;\\text{ iff }&amp;\\text{there is an }a\\in\\mathbb{R}\\ \\text{ s.t. }\\ \\sigma’\\models\\varphi\\ \\text{ and }\\ \\sigma’(y)=\\left\\lbrace\\begin{array}{cll}\\sigma(y)&amp;\\text{if}&amp;y\\neq x\\newline a&amp;\\text{if}&amp;y=x\\end{array}\\right.\\end{array}$$ Let $V’\\subseteq V$ be the set of free variables occurring in the formula $\\varphi$. The solutions of $\\varphi$ are the set of all valuations $\\sigma$ of $V’$ such that $\\sigma\\models\\varphi$. Example formula Solution Space Explaination $x−1\\geq1$ $\\lbrace x\\mapsto a\\in\\mathbb{R}\\mid a\\geq2\\rbrace$ $x$ can be any real number, as long as it has to be larger than 2. $\\exists y.\\ x−1\\geq y$ $\\lbrace x\\mapsto a\\mid a\\in\\mathbb{R}\\rbrace$ For any real number $y$, we can always find an $x$ that is larger than $y$ after minus 1. Next chapter: Encoding real numbers Further Reading:","link":"/AGV/agv8-1/"},{"title":"AGV 8.3 -- Translation from Linear Arithmetic to Automata","text":"Previous chapter: Encoding real numbers This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner In this section, we will illustrate the key ideas in the translation from linear arithmetic to automata through a simple example. Consider the formula $x+y=z$ and the following encoding of the valuation $x\\mapsto1$, $y\\mapsto1$, $z\\mapsto2$: $$\\alpha_\\sigma=\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}{$}\\newline {$}\\newline {$}\\end{bmatrix}\\left(\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\right)^\\omega = \\begin{bmatrix}x\\newline y\\newline z\\end{bmatrix} = \\begin{bmatrix}1\\newline 1\\newline 2\\end{bmatrix}$$ HomogeneityA equation is homogenous if there are no constant terms. The key property of homogeneity is that we can simply scaling without being unbounded. In other words, for a homogenous equation $D$, we have: $D∼0$ (where $∼$ is an arbitrary (in)equality) if and only if, for all $k\\in\\mathbb{Z},2^k\\cdot D∼0$. (Scaling behaviour) As we can see the equation $x+y=z$ is homogenous, i.e., $D: x+y−z = 0$ and $2^k\\cdot x+2^k\\cdot y−2^k\\cdot z∼0$ We immediately observe that: $2^k\\cdot D$ has constant sign, If $D\\neq0$, it is strictly increasing in $k$, and thus For any threshold $N$, we will have that $|2^k\\cdot D|&gt;N$ for all sufficiently large $k$. Shifting the ${$}$ DelimiterThe key observation is that multiplying by a power of $2$ = shifting the ${$}$ delimiter in the binary representation. $$\\begin{bmatrix}x\\newline y\\newline z\\end{bmatrix} =\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}{$}\\newline{$}\\newline{$}\\end{bmatrix}\\left(\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\right)^\\omega=\\begin{bmatrix}1\\newline 1\\newline 2\\end{bmatrix},\\newline\\begin{bmatrix}2^2\\cdot x\\newline 2^2\\cdot y\\newline 2^2\\cdot z\\end{bmatrix} =\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}{$}\\newline{$}\\newline{$}\\end{bmatrix}\\left(\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\right)^\\omega=\\begin{bmatrix}2^2\\cdot 1\\newline 2^2\\cdot 1\\newline 2^2\\cdot 2\\end{bmatrix}=\\begin{bmatrix}4\\newline 4\\newline 8\\end{bmatrix},$$ For any variable $z$, and $t\\geq1$, we denote by $z_t$ the integer encoded by the leftmost $t$ digits of $z$ (the ${$}$ is ignored): $$\\begin{bmatrix}x\\newline y\\newline z\\end{bmatrix} =\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\dots$$ $\\begin{array}{lclc}\\hspace{1cm}\\cdot\\ x_5 =&amp; -2^4\\cdot0+2^3\\cdot0+2^2\\cdot0+2^1\\cdot1+2^0\\cdot1&amp;=&amp;3\\newline\\hspace{1cm}\\cdot\\ y_3 =&amp; -2^2\\cdot0+2^1\\cdot0+2^0\\cdot0&amp;=&amp;0\\newline\\hspace{1cm}\\cdot\\ z_4 =&amp; -2^3\\cdot0+2^2\\cdot1+2^1\\cdot0+2^1\\cdot0&amp;=&amp;4\\newline\\hspace{1cm}\\cdot\\ z_5 =&amp; -2^4\\cdot0+2^3\\cdot1+2^1\\cdot0+2^2\\cdot0+2^0\\cdot0&amp;=&amp;8\\newline\\end{array}$ key linear recurrenceWe can see the above definition is initialized as $z_1=−c_{z,1}$, and key linear recurrence holds for any variable: $z_{t+1}=2z_t+c_{z,t+1}$, where $c_{z,t+1}$ is the $(t+1)$-th digit from the left in the encoding of $z$. Now, in our example we can define $D_t=x_t+y_t−z_t$, and by linearity, the same recurrences hold for $D_t$, too: $$D_1=c_{z,1}-c_{x,1}-c_{y,1}\\newline D_{t+1}=2D_t+c_{z,t}-c_{x,t}-c_{y,t}$$ Thus, we have $D_1=0,D_2=−1,D_3=−2$, and $D_t=−2$ for subsequent $t$. Fractional partTo account for the digits other than the $t$ leftmost digits, we define the fractional part $\\lbrack z\\rbrack_t = \\sum_{i=1}^{\\infty}2^{-i}\\cdot c_{z,t+i}$. $$\\begin{bmatrix}x\\newline y\\newline z\\end{bmatrix} =\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\dots$$ $\\begin{array}{lclc}\\hspace{1cm}\\cdot\\ [x]_1 =&amp; 2^{-1}\\cdot0+2^{-2}\\cdot0+2^{-3}\\cdot1+2^{-4}\\cdot1+\\dots&amp;=&amp;{1\\over4}\\newline\\hspace{1cm}\\cdot\\ [y]_2 =&amp; 2^{-1}\\cdot0+2^{-2}\\cdot1+2^{-3}\\cdot1+2^{-4}\\cdot1+\\dots&amp;=&amp;{1\\over2}\\newline\\hspace{1cm}\\cdot\\ [z]_1 =&amp; 2^{-1}\\cdot1+2^{-2}\\cdot0+2^{-3}\\cdot0+\\dots&amp;=&amp;{1\\over2}\\newline\\hspace{1cm}\\cdot\\ [z]_3 =&amp; 2^{-1}\\cdot0+2^{-2}\\cdot0+2^{-3}\\cdot0+\\dots&amp;=&amp;0\\newline\\end{array}$ Here, we can see $0\\leq[z]_t\\leq1$, and that $z_t+[z]_t = 2^k\\cdot z$ for some $k\\in\\mathbb{Z}$. If the $2^k\\cdot z$ doesn’t seem intuitive to you, check this out:let say we have $z=011{$}0111\\dots = 3+{1\\over2}=3.5$, and we set $t=7$, so $z_7=-2^6\\cdot0+2^5\\cdot1+2^4\\cdot1+2^3\\cdot0+2^2\\cdot1+2^1\\cdot1+2^0\\cdot1=55$ and $[z]_7={1\\over2}+{1\\over4}+{1\\over8}+\\cdots=1$. Thus we have $z_7+[z]_7 = 56 = 2^4\\cdot 3.5$. We can then extend the definition of fractional part to $D$ in the obvious way: $[D]_t = [x]_t+[y]_t−[z]_t$.We thus have that $−1\\leq [D]_t\\leq 2$, and that $D_t+[D]_t=2^k\\cdot D$ for some $k$. If $D_t&lt;−2$ for some $t$. then $D_t+[D]_t&lt;0$. For the corresponding $k$, we have that $2^k \\cdot D&lt;0$. Therefore $x+y-z&lt;0$ and thus we conclude $x+y&lt;z$. If $D_t&gt;1$ for some $t$, then $D_t+[D]_t&gt;0$. For the corresponding $k$, we have that $2^k \\cdot D&gt;0$. Therefore $x+y-z&gt;0$ and thus we conclude $x+y&gt;z$. If $−2\\leq D_t\\leq 1$ for all $t$, then $|D_t+[D]_t|\\leq 3$. For arbitrarily large $k$, $|2^k\\cdot D|\\leq3$. This implies that $x+y=z$ (only option left) From Linear Arithmetic to AutomatonFrom the above deduction, $x+y=z$ holds if and only if $−2\\leq D_t\\leq 1$. To check whether it holds with an automaton, we use the encoding to compute the sequence $D_t$ term by term, and accept if and only if each term is at least $−2$ and at most $1$. Indeed, in our example, the sequence of terms is $0,−1,−2,−2,\\dots$, and we accept. As an example, we could also consider a different encoding of $x\\mapsto1$, $y\\mapsto1$, $z\\mapsto2$: $$\\alpha’_\\sigma=\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 1\\end{bmatrix}\\begin{bmatrix}{$}\\newline {$}\\newline {$}\\end{bmatrix}\\left(\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\right)^\\omega = \\begin{bmatrix}x\\newline y\\newline z\\end{bmatrix} = \\begin{bmatrix}1\\newline 1\\newline 2\\end{bmatrix}$$ In this example the sequence of $D_t$ is $0, 1, 1, 1,\\dots$, which is again bounded, and we accept. On the other hand, a rejecting example may look like this: $$\\alpha_{\\sigma’}=\\begin{bmatrix}0\\newline 0\\newline 1\\end{bmatrix}\\begin{bmatrix}1\\newline 1\\newline 0\\end{bmatrix}\\begin{bmatrix}{$}\\newline {$}\\newline {$}\\end{bmatrix}\\left(\\begin{bmatrix}0\\newline 0\\newline 0\\end{bmatrix}\\right)^\\omega = \\begin{bmatrix}x\\newline y\\newline z\\end{bmatrix} = \\begin{bmatrix}1\\newline 1\\newline -2\\end{bmatrix}$$ The sequence of $D_t$ is $1, 4, 8, 15,\\dots$, which is out of bounded, and we reject. Next chapter: Homogenous Inequality Testing is Automatic Further Reading:","link":"/AGV/agv8-3/"},{"title":"AGV 8.4 -- Homogenous Inequality Testing is Automatic","text":"Previous chapter: Translation from Linear Arithmetic to Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionIn the previous section, we used $x+y=z$ as an example. By extend that into a more general inequality $$g_1x_1+\\dots+g_\\ell x_\\ell\\leq h_1y_1+\\dots +h_my_m$$ where $x_1,\\dots,x_\\ell,y_1,\\dots,y_m$ are free variables, and $g_1,\\dots,g_\\ell,h_1,\\dots,h_m$ are positive integer constants. Such inequality can be described by a Büchi automaton. $\\textbf{Lemma 8.1. }\\textit{Let }\\lbrace x_1,\\dots,x_\\ell,y_1,\\dots,y_m\\textit{ be a set of free variables, and let }g_1,\\dots,g_\\ell,h_1,\\dots,h_m\\newline\\textit{be positive integer constants. There exists a Büchi automaton }\\mathcal{A}\\textit{ such that}$ $$\\mathcal{L(A)}=\\lbrace\\alpha_\\sigma\\mid\\sigma\\models g_1x_1+\\dots+g_\\ell g_\\ell\\leq h_1 y_1+\\dots+h_m y_m\\rbrace$$ It is easy to start with constructing an automaton $\\mathcal{A}_{\\textsf{valid},\\ell+m}$ that checks whether the word $\\alpha_\\sigma$ is well-formed: checks that the separation symbol $({$},\\dots,{$})$ occurs exactly once, and the separation symbol does not appear at the beginning of the word. A Generalize Example $D:\\sum_{i}g_ix_i-\\sum_{j}h_jy_j\\leq0$, where $G=\\sum_{i}g_i$, and $H=\\sum_{j}h_j$. We define the $D_t$ = integer part, $[D]_t$ = fractional part, and observe that for all $t$: $$D_1=-\\sum_{i}g_ic_{x_{i},1}-(-\\sum_{j}h_jc_{y_{j},1})=\\sum_{j}h_jc_{y_{j},1}-\\sum_{i}g_ic_{x_{i},1},\\newline D_{t+1}=2D_t+\\sum_{i}g_ic_{x_{i},t+1}-\\sum_{j}h_jc_{y_{j},t+1},\\newline -H\\leq[D]_t\\leq G.$$ The trichotomy is also completely analogous. If, for some $t$, we have $D_t&gt;H$, we have that $D&gt;0$, and the formula is violated. If, for some $t$, we have $D_t&lt;−G$, we have that $D&lt;0$, and the formula is satisfied. If, for all $t$, we have $−G\\leq D_t\\leq H$ we have that $D=0$, and the formula is satisfied. Our automaton construction keeps track of $D_t$ using the recurrence above. Due to the trichotomy, we only need compute $D_t$ precisely as long as it is at least $−G$ and at most $H$. Thus, we have 3 types of states in the automaton: initialization state Precise value states within $−G,\\dots,0,\\dots,H$ of $D_t$ trap states ($D_t$ has crossed $−G (−\\infty)$ or $H (\\infty)$) $\\textbf{Construction 8.1. } \\text{Let }x_1,\\dots,x_\\ell,y_1,\\dots,y_m\\text{ be free variables, and let }g_1,\\dots,g_\\ell,\\text{ and}\\newline h_1,\\dots,h_m,\\text{ be positive integer constants with }\\sum_{i}g_i=G\\text{ and }\\sum_{j}h_j=H\\text{. We construct a}\\newline\\text{(deterministic) Büchi Automaton}\\mathcal{A}=(\\Sigma,Q,I,T,\\small\\text{BÜCHI}\\normalsize (F))\\text{, such that}$ $$\\mathcal{L(A)}=\\lbrace\\alpha_\\sigma\\mid\\sigma\\models g_1x_1+\\dots+g_\\ell g_\\ell\\leq h_1 y_1+\\dots+h_m y_m\\rbrace \\hspace{1cm}\\text{ as follows,}$$ $\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\Sigma=\\lbrace {$}^{l+m}\\cup(\\lbrace0,1\\rbrace^\\ell\\times\\lbrace0,1\\rbrace^m)\\rbrace\\hspace{1cm}((\\ell\\text{+m)-fold Cartesian product)}\\newline\\hspace{1cm} \\cdot \\ Q=\\lbrace\\textsf{init},-\\infty,-G,-G+1,\\dots,-1,0,1,\\dots,H,\\infty\\rbrace\\newline\\hspace{1cm} \\cdot \\ I=\\lbrace\\textsf{init}\\rbrace\\newline\\hspace{1cm} \\cdot \\ F=\\lbrace-\\infty,-G,-G+1,\\dots,-1,0,1,\\dots,H\\rbrace\\newline\\hspace{1cm} \\cdot \\ T\\text{ is defined as follows:}\\newline\\hspace{1.5cm} 1. \\ (\\textsf{init},(c_{x_1},\\dots,c_{y_m}),q’))\\in T\\text{ if and only if }q’\\in\\sum_{j}h_jc_{y_j}-\\sum_{i}g_ic_{x_i}\\newline\\hspace{1.5cm} 2. \\ (q,({$}\\dots{$}),q)\\in T\\text{ for all }q\\in Q\\setminus\\lbrace\\textsf{init}\\rbrace,\\newline\\hspace{1.5cm} 3. \\ (q_\\infty,c,q_\\infty)\\in T\\text{ for all }c\\in\\Sigma,q_\\infty\\in\\lbrace-\\infty,\\infty\\rbrace,\\newline\\hspace{1.5cm} 4. \\ \\text{For }q\\in\\lbrace-G,\\dots,0,\\dots,H\\rbrace,(q,(c_{x_1},\\dots,c_{y_m}),q’)\\in T\\text{ if and only if }q’=\\textsf{next}\\text{, where}\\newline\\end{array}$$$\\textsf{next}=\\left\\lbrace\\begin{array}{cll}2q+\\sum_{i}g_ic_{x_{i}}-\\sum_{j}h_jc_{y_{j}}&amp;\\text{if}&amp;-G\\leq2q+\\sum_{i}g_ic_{x_{i}}-\\sum_{j}h_jc_{y_{j}}\\leq H \\newline\\infty&amp;\\text{if}&amp;2q+\\sum_{i}g_ic_{x_{i}}-\\sum_{j}h_jc_{y_{j}}&gt; H \\newline-\\infty&amp;\\text{if}&amp;2q+\\sum_{i}g_ic_{x_{i}}-\\sum_{j}h_jc_{y_{j}} &lt;-G \\end{array}\\right.$$ ExplanationAll possible transitions are those the criterion we mentioned above (according to the numbering): $D_1=\\sum_{j}h_jc_{y_{j},1}-\\sum_{i}g_ic_{x_{i},1}$ The separation symbol does not appear at the beginning of the word. $D_t$ has crossed $−G (−\\infty)$ or $H (\\infty)$ move to trap states and cannot leave According to the trichotomy, we only need compute $D_t$ precisely as long as it is $−G\\leq D_t\\leq H$. This completes the proof of Lemma 8.1. We remark that the same construction also works to check the following: Strict Inequality: $g_1x_1+\\dots+g_\\ell x_\\ell &lt; h_1y_1+\\dots +h_my_m$ (by setting $F=\\lbrace−\\infty\\rbrace$), Equality: $g_1x_1+\\dots+g_\\ell x_\\ell = h_1y_1+\\dots +h_my_m$ (by setting $F=\\lbrace−G,\\dots,H\\rbrace$), Reverse Inequality: $g_1x_1+\\dots+g_\\ell x_\\ell \\geq h_1y_1+\\dots +h_my_m$ (by setting $F=\\lbrace−G,\\dots,H,\\infty\\rbrace$), Reverse Strict Inequality: $g_1x_1+\\dots+g_\\ell x_\\ell &gt; h_1y_1+\\dots +h_my_m$ (by setting $\\infty\\rbrace$). Next chapter: From Linear Arithmetic to Automata Further Reading:","link":"/AGV/agv8-4/"},{"title":"AGV 8.5 -- From Linear Arithmetic to Automata","text":"Previous chapter: Homogenous Inequality Testing is Automatic This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionWith all the setup in the previous sections, We are now ready to prove the main result of this chapter. $\\textbf{Theorem 8.1. }\\textit{Let }\\varphi\\textit{ be a linear arithmetic formula. We can effectively construct a Büchi}\\newline\\textit{Automaton }\\mathcal{A}\\textit{ such that }\\mathcal{L(A)}=\\lbrace\\alpha_\\sigma\\mid\\sigma\\models\\varphi\\rbrace$ We follow the same strategy as for the analogous result for S1S in Section 6.7. Introduce a logic with a (slightly) restricted syntax from S1S, Show that the restriction does not come at the cost of expressive power, and Use structural induction on restricted-syntax formulas to construct our desired automata. Restricted Linear ArithmeticThe formulas of syntactically restricted linear arithmetic are defined by the following grammar: $$\\varphi::=z=1\\mid g_1x_1+\\dots+g_\\ell x_\\ell\\leq h_1 y_1+\\dots+h_m y_m\\mid\\neg\\varphi\\mid\\varphi\\wedge\\varphi\\mid\\exists x.\\varphi$$ where $z, x_1,\\dots,x_\\ell,y_1,\\dots,y_m\\in V$ are variables, and $p_1,\\dots,p_\\ell,q_1,\\dots,q_m$ are positive constants. The restriction is that all inequalities must be homogenous, except $z=1$ being the only non-homogenous relation. homogenous inequalities conversionIn fact, “all inequalities must be homogenous” is not a semantic restrictionm, since we can always convert non-homogenous inequalities into a homogenous version: For a formula $\\varphi_0$ contains non-homogenous inequalities, first replace all constants $r$ by $r\\cdot z$, where $z$ is a fresh variable to obtain $\\varphi’_0$, then replace $\\varphi_0$ with $\\varphi::=\\exists z. (z=1\\wedge\\varphi’_0)$. For example, $\\varphi_0::=y\\leq2,\\varphi_0’::=y\\leq2\\cdot z,$ now we have $\\varphi_0::=\\exists z. (z=1\\wedge(y\\leq2\\cdot z)))$ To include all possible expression of the real number $1$, the encoding must be a word of the form $00^\\ast(1{$} 0^\\omega + {$} 1^\\omega)$. It is thus trivial to construct an automaton corresponding to $z=1$. From Linear Arithmetic to AutomataWe use Construction 8.1 to construct automata for homogenous inequalities. Note that the construction works even when one of the sides of the inequality is equal to $0$ (i.e., the empty sum). Now we start with inductive cases. Recall that for any $k$ we can construct an automaton $\\mathcal{L(A_{\\textsf{valid},k})}$ that checks whether a word $\\alpha\\in(\\lbrace 0,1\\rbrace^k\\cup\\lbrace{$}\\rbrace)^k)^\\omega$ is a well-formed encoding of some valuation $\\sigma$ to $k$ free variables. Negation $\\mathcal{A_{\\neg\\varphi}}$The Büchi automaton $\\mathcal{A_{\\neg\\varphi}}$ is obtained through complementation and intersection of Büchi automata. Let $\\varphi$ have $k$ free variables. For negation, we have $$\\mathcal{L(A_{\\neg\\varphi})}=((\\lbrace 0,1\\rbrace^k\\cup\\lbrace{$}\\rbrace)^k)^\\omega\\setminus\\mathcal{L(A_\\varphi)})\\cap\\mathcal{L(A_{\\textsf{valid},k})}$$ Conjunction $\\mathcal{A_{\\varphi_1\\wedge\\varphi_2}}$$\\mathcal{A_{\\varphi_1\\wedge\\varphi_2}}$ can be obtained similarly straightforward through the intersection of two Büchi automata because $$\\mathcal{L(A_{\\varphi_1\\wedge\\varphi_2})}=\\mathcal{L(A_{\\varphi_1})}\\cap\\mathcal{L(A_{\\varphi_2})}$$ Existential QuantificationTo handle projection, i.e. construct $\\mathcal{A_{\\exists x.\\varphi}}$ from $\\mathcal{A_{\\varphi}}$, we first try to see their difference: $\\mathcal{A_{\\varphi}}$ runs over the alphabet $\\lbrace 0, 1\\rbrace^{k+1}\\cup\\lbrace{$}\\rbrace^{k+1}$ and reads the encoding of a valuation of $\\lbrace x,y_1,\\dots, y_k\\rbrace$. $\\mathcal{A_{\\exists x.\\varphi}}$ runs over the alphabet $\\lbrace 0, 1\\rbrace^k\\cup\\lbrace{$}\\rbrace^k$ and reads the encoding of a valuation of $\\lbrace y_1,\\dots, y_k\\rbrace$, Same as before, the principle behind the construction is to create a automaton that simulate identical behaviour with the absence of $x$. This is simple for most of the transitions except for the initial transition. For a automaton that does not contain $x$, it has to “guess” an encoding of $x$ such that $\\varphi(x,y_1,\\dots, y_k)$ holds. However, the given encoding of the valuation of $\\lbrace y_1,\\dots, y_k\\rbrace$ may not be appropriately padded. For example, let $y_1 = 1$ and we encode it as $01{$}0^\\omega$. If we guess $x=4$, then two digits of integer part is not enough. The encoding must be “padded” so to synchronize “may be” accepted by $\\mathcal{A_{\\varphi}}$: $$\\begin{bmatrix}x\\newline y_1\\end{bmatrix}=\\begin{bmatrix}0\\newline 0\\end{bmatrix}\\begin{bmatrix}1\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 0\\end{bmatrix}\\begin{bmatrix}0\\newline 1\\end{bmatrix}\\begin{bmatrix}{$}\\newline{$}\\end{bmatrix}\\left(\\begin{bmatrix}0\\newline 0\\end{bmatrix}\\right)^\\omega$$ Therefore, when we convert from $c_y$ to $(c_x, c_y)$, We have to repeat the first letter $c_{y,n}$ as padding: $$(a_{x,n+j}, a_{y,n})(a_{x,n+j−1}, a_{y,n})\\dots(a_{x,n}, a_{y,n}).$$ Since $c_{y,n}$ is simply repeating, creating transition for each letter of $a_x$ unnecessarily enlarge the size of the automaton. Instead, we can make one initial transition that encapsulate the full padding of the first letter of $c_y$. $$(q,(c_{x,1},c_{y_1},\\dots,c_{y_k})\\dots(c_{x,n},c_{y_1},\\dots,c_{y_k}),q’)\\in T’$$ $\\textbf{Construction 8.2. } \\text{Let }x,y_1,\\dots, y_k\\text{ be free variables in the linear arithmetic formula }\\varphi,\\text{ and}$ $$\\mathcal{A_\\varphi}=(\\lbrace 0, 1\\rbrace^{k+1}\\cup\\lbrace{$}\\rbrace^{k+1},Q,I,T,\\small\\text{BÜCHI}\\normalsize (F))$$ $\\text{be a Büchi Automaton such that }\\mathcal{L(A)}=\\lbrace\\alpha_\\sigma\\mid\\sigma\\models\\varphi\\rbrace.\\text{ We construct a Büchi Automaton}$ $$\\mathcal{A_{\\exists x.\\varphi}}=(\\lbrace 0, 1\\rbrace^{k}\\cup\\lbrace{$}\\rbrace^{k},Q\\cup\\textsf{Inits},\\textsf{Inits},T’,\\small\\text{BÜCHI}\\normalsize (F))$$ $\\text{such that }\\mathcal{L(A_{\\exists x.\\varphi})}=\\lbrace\\alpha_\\sigma\\mid\\sigma\\models\\exists x.\\varphi\\rbrace\\text{ as follows.}$ $\\begin{array}{l}\\hspace{1cm} \\cdot \\ \\textsf{Inits}=\\lbrace(q,\\star)\\mid q\\in I\\rbrace\\newline\\hspace{1cm} \\cdot \\ (q,(c_{y_1},\\dots,c_{y_k}),q’)\\in T’\\text{ if and only if there exists }c_x\\text{ such that }(q,(c_x,c_{y_1},\\dots,c_{y_k}),q’)\\in T’\\newline\\hspace{1cm} \\cdot \\ ((q,\\star),(c_{y_1},\\dots,c_{y_k}),q’)\\in T’\\text{ if and only if there exists a word }c_{x,1}\\dots c_{x,n}\\in\\lbrace0,1\\rbrace^{+}\\newline\\hspace{1cm} \\ \\text{ such that }(q,(c_{x,1},c_{y_1},\\dots,c_{y_k})\\dots(c_{x,n},c_{y_1},\\dots,c_{y_k}),q’)\\in T’\\end{array}$ Two remarks are in order: We extended the transition relation $T$ to $Q\\times\\Sigma^+\\times Q$: if $(q,u,q’)\\in T$ and $(q’,v,q’’)\\in T$, then $(q,uv,q’’)\\in T$ (where $\\Sigma^+ = \\Sigma\\Sigma^\\ast$). The transitions from the freshly added initial states can be readily determined, via, for example, a depth-first search by considering an appropriate subgraph induced by the automaton $\\mathcal{A_\\varphi}$. This concludes the final inductive case, and, hence, the proof of Theorem 8.1. Next chapter: Automata-based LTL Model Checking with Sequential Circuits Further Reading:","link":"/AGV/agv8-5/"},{"title":"AGV 9.1 -- Automata-based LTL Model Checking with Sequential Circuits","text":"Previous chapter: From Linear Arithmetic to Automata This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner Right hand side: Hondeterministic Büchi automataIn order to find system executions that violate a given LTL formula (1), we negate the formula (2) and build an automaton that is equivent to the negated formula. For this, we can use the translaton from LTL to alternating automata (3) from Construction 7.1 followed by the Miyano-Hayashi translation from alternating Büchi automata to nondeterministic Büchi automata (4) from Construction 7.2. Left hand side: Safety AutomatonWe represent the system (1) executions as a Safety Automaton (2), which is then intersected (using Construction 3.2 with the Büchi automaton for the negated LTL formula. The actual search for a violating execution then happens as the emptiness check of the resulting Büchi automaton. In the remainder of this section, we first quickly discuss the representation of the system as a Safety Automaton, using sequential circuits as an example, and then focus on the emptiness check of Büchi automata. Model Checking Sequential CircuitsAs an illustration of how automata can be used to represent system behaviors, we consider the representation of sequential circuits to safety automata. For a more general discussion of how to represent different types of systems, such as protocols or software, we refer the reader to textbooks on model checking, such as Principles of Model Checking by Baier and Katoen. Sequential Circuits $\\textbf{Definition 9.1. } \\text{A }\\textit{sequential circuit }\\text{is given as a tuple }S=(I,O,R,\\theta,\\lambda,\\delta)\\text{, where}$ $\\begin{array}{lcl}\\hspace{1cm}\\cdot&amp;I&amp;\\text{is a set of input bits}\\newline\\hspace{1cm}\\cdot&amp;O&amp;\\text{is a set of output bits}\\newline\\hspace{1cm}\\cdot&amp;R&amp;\\text{is a set of registers}\\newline\\hspace{1cm}\\cdot&amp;\\theta\\subseteq R&amp;\\text{is an initial register evaluation}\\newline\\hspace{1cm}\\cdot&amp;\\lambda:O\\rightarrow(2^{I\\cup R}\\rightarrow\\mathbb{B})&amp;\\text{assigns to each output bit a control function }2^{I\\cup R}\\rightarrow\\mathbb{B}\\newline\\hspace{1cm}\\cdot&amp;\\delta:R\\rightarrow(2^{I\\cup R}\\rightarrow\\mathbb{B})&amp;\\text{assigns to each output bit a update function }2^{I\\cup R}\\rightarrow\\mathbb{B}\\newline\\end{array}$ Safety AutomatonA safety automaton is a Büchi automaton where all states are accepting. Here, Input $(I)$ and Output $(O)$ are represented as words, and register valuation $(R)$ is represented as states: $\\text{A sequential circuit can be represented as a safety automaton}\\newline\\mathcal{A}_S=(2^{I\\cup O},2^R,I,T,\\small\\text{BÜCHI} \\normalsize (Q))\\text{, where}$ $\\begin{array}{ll}\\cdot\\ Q = &amp;2^R\\hspace{1cm}\\text{consist of all valuations of the registers;}\\newline\\cdot\\ I =&amp; \\lbrace\\theta\\rbrace\\hspace{0.8cm}\\text{corresponds to the inital register valuation;}\\newline\\cdot\\ T=&amp;\\lbrace(q,\\sigma,q’)\\mid\\lbrace\\forall y\\in O:y\\in \\sigma\\text{ iff }\\lambda(y)(q\\cup(\\sigma\\cap I))\\rbrace\\wedge\\lbrace\\forall r\\in R:r\\in q’\\text{ iff }\\delta(r)(q\\cup(\\sigma\\cap I))\\rbrace\\rbrace\\newline&amp;\\hspace{1.5cm}\\text{reflect the outputs specified by the control functions, and}\\newline&amp;\\hspace{1.5cm}\\text{the new register valuation specified by the update function}\\end{array}$ We say that a circuit $S$ satisfies an LTL formula $\\varphi$ if $\\mathcal{L}(\\mathcal{A}_S) ⊆ \\mathcal{L(\\varphi)}$. ExampleThe example circuit shown on the left has input $I = \\lbrace x\\rbrace$, output $O = \\lbrace y\\rbrace$, and a single register $R = \\lbrace r\\rbrace$. The control function of $y$ is $x\\text{ XOR }r$, the update function of $r$ is $x\\vee r$. We assume an initial register valuation $\\theta = \\varnothing$. The circuit is then represented as the automaton shown on the right. Note that that accepting state only have transtions $\\lbrace x\\rbrace, \\lbrace y\\rbrace$, becasue if $\\lbrace x\\rbrace$ holds, $\\lbrace y\\rbrace$ cannot hold because of the $\\text{XOR}$ gate and vice versa. Suppose now that we wish to verify whether our circuit satisfies the LTL formula $\\varphi = \\square(x\\leftrightarrow y)$. We negate $\\varphi$, and translate the resulting formula into the nondeterministic Büchi automaton shown on the left: The intersection of the languages of the automaton representing the circuit and the automaton representing the negation of $\\varphi$ results in the automaton shown above on the right. Since there are some words that can be accepted by both automaton and their intersections, the language of this automaton is not empty: for example, the word $\\lbrace x, y\\rbrace(\\lbrace x\\rbrace)^\\omega$. The circuit, hence, does not satisfy $\\varphi$. Next chapter: Nested depth-first search Further Reading:","link":"/AGV/agv9-1/"},{"title":"AGV 9.2 -- Nested depth-first search","text":"Previous chapter: Automata-based LTL Model Checking with Sequential Circuits This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionWe now develop an algorithm for checking whether the language of a given Büchi automaton is empty. A natural idea is to use depth-first search (DFS) twice. The language is non-empty $\\Leftrightarrow$ it is accepted by some words: there exists an accepting state $q$, $q$ is reachable from some initial state (discovered by 1st DFS), and $q$ can again be reached from $q$ (discovered by 2nd DFS), Example (Simple DFS)Consider the following Büchi automaton (edge labels do not matter and are omitted). Step 1: discovers $q_0$, $q_1$, and $q_3$.Step 2: searches from $q_0$ and $q_3$: not successful; searches from $q_1$: discovers the path back to $q_1$ via $q_2$. The drawback of the algorithm discussed so far is its quadratic running time: potentially, each state in $F$ discovered by the first DFS requires a fresh second DFS. The quadratic running time can be avoided by stopping the DFS in Step 2 whenever a state is encountered that was already visited during Step 2. However, this is only sound if the searches in Step 2 are executed in the right order. If we first execute the DFS from $q_0$ in Example 9.2, then this search visits all states; in a subsequent search from $q_1$, we would, therefore, no longer discover the successful path back to $q_1$ via $q_2$! It turns out that it is sound to restrict the searches in Step 2 if they are executed in order of increasing finishing times of the DFS in Step 1. The emptiness check with nested DFS therefore uses Step 1 to order the reachable accepting states according to their finishing times; in Step 2, a DFS is initiated from each reachable accepting state in this order until a cycle is detected. Step 2 marks the visited states and restricts the searches so that no state is visited twice during Step 2. A Modified Example (Nested DFS)Continue from above, a possible annotation of the states with pairs (discovery, finishing) of discovery and finishing times during the DFS in Step 1 is the following: Ordering the accepting states according to increasing finishing times, we obtain the order $q_3$, $q_1$, $q_0$. In Step 2, the DFS from $q_3$ visits (unsuccessfully) $q_3$, $q_4$ and $q_5$. The DFS from $q_1$then only visits $q_1$ and $q_2$, upon which it has successfully discovered the path from $q_2$ to $q_2$. Nested DFS and Büchi Automaton $\\textbf{Theorem 9.1. }\\textit{For a Büchi automaton }\\mathcal{A}\\textit{, nested DFS is successful iff }\\mathcal{L(A)}\\textit{ is nonempty.}$ Proof$”\\Rightarrow”$If the nested DFS is successful then there exists a state $q$ that is reachable from some initial state such that there is a path from $q$ back to $q$. Hence, $\\mathcal{L(A)}$ is non-empty. $”\\Leftarrow”$To show that we can safely ignore the states that were visited in previous searches in Step 2, we consider the situation at the beginning of a DFS from some accepting state $q\\in F$ in Step 2. Let $T$ be the set of states visited in previous searches in Step 2. We prove that there is no cycle $q_0,q_1,\\dots,q_k$ with $q=q_0=q_k$ such that $\\lbrace q_0,q_1,\\dots,q_k\\rbrace\\cap T\\neq \\varnothing$ i.e., the states in $T$ can be ignored while looking for $q$-cycles. Assume, by way of contradiction, that there is a state $q$ where this condition is violated for the first time. Let $t\\in\\lbrace q_0,q_1,\\dots,q_k\\rbrace\\cap T$, and let $u\\in F$ be the accepting state such that $t$ has been added to $T$ during the DFS in Step 2. This means that the DFS from $u$ was invoked before the DFS from $q$; hence, $u$ has an earlier finishing time than $q$ in den DFS of Step 1. Case 1: $u$ was discovered before $q$ in the DFS of Step 1. This cannot be the case, because $q$ is reachable from $u$, and, thus, the finishing time of $q$ would have been earlier than that of $u$. Case 2: u was discovered after $q$ in the DFS of Step 1. Then $q$ was still on the stack of the DFS when $u$ was finished. Hence, $u$ is reachable from $q$. Thus, $u$ and $q$ are on a cycle. This cycle (or some other cycle) would have been discovered during the DFS from $u$ in Step 2. Next chapter: The Emerson-Lei algorithm Further Reading:","link":"/AGV/agv9-2/"},{"title":"AGV 9.3 -- The Emerson-Lei algorithm","text":"Previous chapter: Nested depth-first search This is a learning note of a course in CISPA, UdS. Taught by Bernd Finkbeiner IntroductionAs an alternative algorithm for checking language emptiness of Büchi automata we now discuss a classic algorithm due to Emerson and Lei. Unlike the depth-first search of the previous subsection, the Emerson-Lei algorithm is based on a breadth-first search implemented as a fixpoint construction over sets of states. A disadvantage of this algorithm is that its running time is quadratic. Nevertheless, algorithms of this type play a major role in symbolic model checking, because the sets of states can often be represented efficientlyusing data structures like binary decision diagrams. Live statesA state $q$ of a Büchi automaton is live if some infinite path starting in $q$ visits accepting states infinitely often. This definition is the opposite of safe states, where it never visits accepting states. The idea of the algorithm is to identify the set of live states. The language of a Büchi automaton is non-empty iff it has a live initial state. The Emerson-Lei algorithm is based on the following inductive definition: $\\textbf{Definition 9.1. } \\text{For a Büchi automaton and a number }n\\in\\mathbb{N}\\text{, the set of }\\textit{n-live states}\\text{ is}\\newline\\text{defined as follows:}$ $\\begin{array}{l}\\hspace{1cm}\\cdot\\ \\text{every state is 0-live}\\newline\\hspace{1cm}\\cdot\\ \\text{a state q is (n + 1)-live if some path containing at least one transition leads from }q\\newline\\hspace{1.3cm}\\text{to an accepting n-live state}\\newline\\end{array}$ FixpointLet $\\mathit{live}_n$ denote the set of $n$-live states. It is easy to see that $\\mathit{live}_{n} \\supseteq \\mathit{live}_{n+1}$, because $\\mathit{live}_{0}$ represents the set of all states in the automaton, and $\\mathit{live}_{1}$ are only those which can reach the accepting states. Then $\\mathit{live}_{2}$ are those who can only reach the accepting states through some states in $\\mathit{live}_{1}$. Therefore set $\\mathit{live}_{n+1}$ can never be larger than its previous set $\\mathit{live}_{n}$. Since the set of states is finite, there exists a fixpoint $\\mathit{live}_k$ such that $\\mathit{live}_k=\\mathit{live}_{k+1}$. Then the set $\\mathit{live}_k$ is the set of live states. Example In the example from last section, states $q_0$, $q_1$ and $q_2$ are live; states $q_3$, $q_4$, and $q_5$ are only $0$-$\\textit{live}$ (you cannot visit other accepting states starting from $q_3$ even though it is an accepting state). To describe the algorithm, we first introduce a construction that implements a backward breadth-first search as a least fixpoint. The construction computes all states from which a given set of states is reachable. $\\textbf{Construction 9.1. } \\text{For a Büchi Automaton }\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))\\text{ and a set of states}\\newline R\\subseteq Q\\text{, we compute the set of backwards reachable states from }R\\text{ as follows:}$ $\\begin{array}{llll}\\hspace{1cm} \\cdot &amp; \\textit{Pre}(R)&amp;=&amp;\\lbrace q\\in Q\\mid \\exists q’\\in R,\\sigma\\in\\Sigma,(q,\\sigma,q’)\\in T\\rbrace\\newline\\hspace{1cm} \\cdot &amp; \\textit{BackwardReach}_0(R)&amp;=&amp;R\\newline\\hspace{1cm} \\cdot &amp; \\textit{BackwardReach}_{n+1}(R)&amp;=&amp;\\textit{BackwardReach}_{n}(R)\\cup\\textit{Pre}(\\textit{BackwardReach}_{n}(R))\\newline\\hspace{1cm} \\cdot &amp; \\textit{BackwardReach}(R)&amp;=&amp;\\underset{n\\in\\mathbb{N}}{\\bigcup}\\textit{BackwardReach}_{n}(R)\\newline\\end{array}$ By this construction, it returns a set of states that can reach some states in $R$. For example, $\\lbrace q_0,q_1,q_2\\rbrace$ can reach $q_3$. Using this construction as a subroutine, we can compute the live states as a greatest fixpoint: $\\textbf{Construction 9.2. } \\text{For a Büchi Automaton }\\mathcal{A} = (\\Sigma,Q,I,T,\\small\\text{BÜCHI} \\normalsize(F))\\text{ we compute the set}\\newline\\text{of live states as follows:}$ $\\begin{array}{llll}\\hspace{1cm} \\cdot &amp; \\textit{live}_0&amp;=&amp;Q\\newline\\hspace{1cm} \\cdot &amp; \\textit{live}_{n+1}&amp;=&amp;\\textit{BackwardReach}(\\textit{Pre}(\\textit{live}_n\\cap F))\\newline\\hspace{1cm} \\cdot &amp; \\textit{live}&amp;=&amp;\\underset{n\\in\\mathbb{N}}{\\bigcap}\\textit{live}_{n}\\newline\\end{array}$ The set of $\\textit{live}$ is the smallest subset of all $\\textit{live}_n$ in any $n\\in\\mathbb{N}$. Therefore the function will stop when $\\mathit{live}_k$ such that $\\mathit{live}_k=\\mathit{live}_{k+1}$, which is the greatest fixpoint of the set of live states. Then we can verify whether the automaton is non-empty, i.e. $q_0\\in\\textit{live}$. Example (cont.)We compute the live states as follows: $\\begin{array}{llll}\\hspace{1cm} \\cdot &amp; \\textit{live}_0&amp;=&amp;\\lbrace q_0,q_1,q_2,q_3,q_4,q_5\\rbrace\\newline\\hspace{1cm} \\cdot &amp; \\textit{live}_1&amp;=&amp;\\lbrace q_0,q_1,q_2\\rbrace\\newline\\hspace{1cm} \\cdot &amp; \\textit{live}_2&amp;=&amp;\\lbrace q_0,q_1,q_2\\rbrace\\newline\\hspace{1cm} \\cdot &amp; \\textit{live}&amp;=&amp;\\lbrace q_0,q_1,q_2\\rbrace\\newline\\end{array}$ Since the initial state $q_0$ is live, we have that the language of the automaton is non-empty. Next chapter: The Muller Acceptance Condition Further Reading:","link":"/AGV/agv9-3/"},{"title":"Cantonese Ch.1-2 -- Rimes with a &amp; aa (Phonology)","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Previous lesson: Introduction Recap of RimesRimes is a syllable of vowels with consonants that you can rhyme! For example, in English, the Rime of “Cow” is “ow”; in Cantonese, cow is 牛(ngau4), and the Rime is “au”. Difference of a and aaTwo vowels in Cantonese start with a. a sounds like “uh” in English, and aa “ahhh” in English. And all their combinations as well. For example, 分(fan1) sounds like “fun”, and 飯(faan6) sounds like “fran” in France without “r” sound. Still, it may be difficult to distinguish their difference when they form rimes with other sounds. A trick to pronounce any combinations of the aa sound is to try to add the “ahhh” sound before a.For example, ap sounds like “up” in English, and aap sounds like “ahh-up” in one syllable. Let’s see all the rimes combinations with a and aa! Rimes with a Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example a uh in bruh 嘞(laa3) ang ung in sung 增(zang1) ai i in night 擠(zai1) ap up in cup 汁(zap1) au ull in dull 周(zau1) at utt in butt 質(zat1) am um in sum 針(zam1) ak uck in duck 則(zak1) an un in fun 珍(zan1) Rimes with aa Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example aa a in far 渣(zaa1) aang an in France but ends with ng not n 爭(zang1) aai i in high 齋(zaai1) aap alm in calm with p instead 集(zaap6) aau ow in wow 嘲(zaau1) aat alm in calm with t instead 紮(zaat3) aam alm in calm 站(zaam6) aak alm in calm with k instead 窄(zaak3) aan an in France 讚(zaan3) Learn a new word before you move onIn Cantonese, 早抖(zou2 tau2) will be another most common thrase you may hear, which means good night. Its literally meaning is sleep early, it is a good wish because nowadays people sleep quite late right? So again… 下次見(haa6 ci3 gin3)! Next lesson: Rimes with e Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-2/"},{"title":"Cantonese Ch.1-1 -- Onset (Phonology)","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Previous lesson: Introduction Introduction of OnsetIn this lesson, We start with Onset. They are all consonants that are used before vowels. For example, “good morning” in Cantonese is 早晨(zou2 san4), where “z” and “s” are Onset here. OnsetBasically, these consonant looks identical, or you may find alternative spelling in English, exceptkw, z, and c might be a little unique. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example - character that have no starting consonant (Null initial) 呀(aa3) k k in king 卡(kaa1) b b in bar 巴(baa1) ng ng in sing 牙(ngaa4) p p in palm 怕(paa3) h h in harp 蝦(haa1) m m in mat 媽(maa1) gw gu in guava 瓜(gwaa1) f f in foul 花(faa1) kw qu in aqua 誇(kwaa1) d d in dip1 打(daa2) w w in wow 蛙(waa1) t t in tip1 他(taa1) z j in job but with a ‘t’ sound in front of it2 揸(zaa1) n n in nap 那(naa5) c c in chat2 without ‘h’ sound 叉(caa1) l l in lap 啦(laa1) s s in soup 沙(saa1) g g in gum 家(gaa1) j y in yes3 也(jaa5) 1If you are a native English speaker, you may notice that the Cantonese “d” sound is softer than in English, but the “t” sound is more challenging (the difference is negligible here). But if you have a problem pronouncing this pair, try to make your tongue touch the upper front teeth when creating the “d” and “t” sounds. 2In Cantonese, “c” and “z” are clear sounds without aspirated. You can try to not stress your lip and open your mouth wider. 3In most European languages, “j” sounds like “y” in English. For example, “ja” in German. Therefore, Jyutping chose “j” to represent this sound. Learn a new word before you move onIn Cantonese, bye bye will be the most common way to say goodbye. But you may also say 再見(zoi3 gin3)， which means see you. or you may say 下次見(haa6 ci3 gin3) which means see you next time! So… 下次見(haa6 ci3 gin3)! Next lesson: Rimes with a &amp; aa Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-1/"},{"title":"Cantonese Ch.1-0 -- Introduction to Phonology &amp; Tones","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Overview of Cantonese PhonologyMaybe you already know that we use one syllable for each character in Cantonese,just like all other Chinese languages. It contains 3 crucial parts in order to pronounce the word.That is the consonances, vowels, and tones. In Cantonese, there are 19 consonances, 9 vowels and 6 tones.That creates 1,760 different sounds to cover over 10,000 Chinese Characters. Every Cantonese consonance and vowel pair can always be described as an Onset-Rime. Phonology StructureSo what is actually an Onset-Rimes?An Onset-Rimes is either a consonant-vowel or a consonant-vowel-consonant structure, For example: consonant-vowel: the word “rich” = 富(fu3) (sounds like “foo” in English). consonant-vowel-consonant: the word “bamboo” = 竹(zuk1) (sounds like “joke” in English). TonesTones are a unique feature of the Chinese language; in Cantonese, it is slightly more fancy than in Mandarin.But don’t worry, in this Chapter, we will break it down and make it easy for you! SummarySo today, we’ve learned the structure of Cantonese, and know that it contains Onsets, Rimes, and Tones.It’s time to learn your first Cantonese phrase! In Cantonese, the most common greeting we use is Good Morning, that is 早晨(zou2 san4)! zou2 san4… How do we pronounce the “z” sound? Now, let’s move on to the next lesson for Onset! Portal: Chapter 0-1: Onset Chapter 0-2: Rimes with a &amp; aa Chapter 0-3: Rimes with e Chapter 0-4: Rimes with i &amp; o Chapter 0-5: Rimes with u &amp; yu Chapter 0-6: Tones Chapter 0-7: Onset, Rimes and Tones (Summary) Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-0/"},{"title":"Cantonese Ch.1-3 -- Rimes with e (Phonology)","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Previous lesson: Rimes with a &amp; aa All Rimes with eIn this lesson, we talk about Rimes with e. e is a very naughty sound because it has multiple sounds based on how you spell it. Also, e can combine with o to create oe and eo sounds. Rimes with e Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example e ai in pair 些(se1) eng eng in leng2 鄭(zeng6) ei ey in prey 四(sei3) ep ep in bicep 夾(gaap3) eu eil in veil 掉(diu6) et et in pet 坺(paat6) em em in gem 舔(tim2) ek ek in trek 石(sek6) We can see most of them are the same as e pronounced in English. Rimes with oe and eooe and eo sound very similar. They were considered the same sound before, but soon linguists found that this sound behaved differently and had different consonances.Therefore, they are split for specific use cases. For example, if there’s an eoi sound, there won’t be a oei sound. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example oe ur in fur 鋸(goe3) eoi an in France but ends with ng not n 衰(seoi1) oeng ern in cern2 薑(goeng1) eon alm in calm with p instead 詢(seon1) oek erk in berserk 腳(goek3) eot alm in calm with t instead 摔(seot1) 1meaning of “leng” 2 Obviously, the oeng sound should have a “g” sound, so precisely, it should sound like “caring.” Here, the “ng” is similar to the “ng” in “sing.” Learn a new word before you move onLet’s continue with greetings, how to impress your Hong Kong friend that you haven’t seen for so long, by showing your Cantonese skills? You may say 好耐無見(hou2 noi6 mou4 gin3), which means Long time no see! Fun fact is, long time no see may acutally come from Cantonese users, who try to do direct translate from cantonese to english. So again… 下次見(haa6 ci3 gin3)! Next lesson: Rimes with i &amp; o Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-3/"},{"title":"Cantonese Ch.1-4 -- Rimes with i &amp; o (Phonology)","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Previous lesson: Rimes with e In this lesson, we talk about Rimes with i and o. Both are pretty intuitive for English speakers. Rimes with iFor i, iu, im, in, ip, it, i sounds like “yee”. For example, “ee” in “see” or “keen”. For ing and ik, i sound like “egg”. For example, “i” in “ignore” or “sing”. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example i ee in see 思(si1) ip eep in weep 攝(sip3) iu eal in seal 燒(siu1) it eet in meet 舌(sit3) im eam in beam 閃(sim2) ing ing in sing 升(sing1) in een in seen 先(sin1) ik ic in acidic 識(sik1) Rimes with oEvery Rime with o sounds like “oy” in “joy” or “ahoy”. Except for ou, it sounds more like “oh”. For example, “ou” in “soul”. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example o ee in see 可(ho2) ot ought in thought 渴(hot3) oi oy in joy 開(hoi1) ok orch in orchestra 學(hok6) on on in gone 看(hon3) ou ou in soul 好(hou2) ong ong in long 康(hong1) Learn a new word before you move onExcuse me, it’s time for your daily dose of Cantonese vocab! 唔好意思(m4 hou2 ji3 si1), which means **Excuse me**! You can use it in many scenario, and politeness is always the key to success. So again… 下次見(haa6 ci3 gin3)! Next lesson: Rimes with u Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-4/"},{"title":"Cantonese Ch.1-5 -- Rimes with u &amp; yu (Phonology)","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Previous lesson: Rimes with e In this lesson, we talk about Rimes with u, yu. Here we use u similar to languages like German or Italian. And yu is equivalent to “ü” in German. Let’s take a look. Rimes with uFor u, ui, un, ut, u sounds like “oo” in “foo”. For ung and uk, u sound like “one” in “tone”. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example u oo in foo 夫(fu1) ut oot in boot 闊(fut3) ui ewy in chewy1 灰(fui1) ung one in tone 風(fung1) un oon in cartoon 歡(fun1) uk ook in cook 福(fuk1) 1“Chewy” is two syllables, but in Cantonese this is a diphthong. So try to blend it into one sound, treat the “ew” as the major sound and the “j” as a small tip at the end of the syllable. Rimes with yuUnfortunately, in English there is no word sound exactly as yu. But you can imitate it by positioning your tongue more forward, closer to your front teeth (not touching them!), and try to say the word “foo”. In some accents, when people say the word “occupy” or “education”, they may use the yu sound as well. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example yu u in occupy 書(syu1) yut uned in tuned 雪(syut3) yun une in tune 酸(syun1) Learn a new word before you move onYou may already know that 多謝(do1 ze6) means Thank you, but you can take one step further! Try to use 感激不準(gam2 gik1 bat1 zeon2) or simply 多謝曬(do1 ze6 saai3) to show your appreciation. You can use it in many scenario, and politeness is always the key to success. So again… 下次見(haa6 ci3 gin3)! Next lesson: Tones Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-5/"},{"title":"Cantonese Ch.1-6 -- Tones","text":"Golden Rule of becoming a native Cantonese speaker: Tones &gt; Everything! In this blog, we use Jyutping to indicate the pronunciation of Cantonese characters. Previous lesson: Rimes with e In this lesson, we talk about Rimes with u, yu. Here we use u similar to languages like German or Italian. And yu is equivalent to “ü” in German. Let’s take a look. Rimes with uFor u, ui, un, ut, u sounds like “oo” in “foo”. For ung and uk, u sound like “one” in “tone”. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example u oo in foo 夫(fu1) ut oot in boot 闊(fut3) ui ewy in chewy1 灰(fui1) ung one in tone 風(fung1) un oon in cartoon 歡(fun1) uk ook in cook 福(fuk1) 1“Chewy” is two syllables, but in Cantonese this is a diphthong. So try to blend it into one sound, treat the “ew” as the major sound and the “j” as a small tip at the end of the syllable. Rimes with yuUnfortunately, in English there is no word sound exactly as yu. But you can imitate it by positioning your tongue more forward, closer to your front teeth (not touching them!), and try to say the word “foo”. In some accents, when people say the word “occupy” or “education”, they may use the yu sound as well. Jyutping Sounds in English Cantonese Example Jyutping Sounds in English Cantonese Example yu u in occupy 書(syu1) yut uned in tuned 雪(syut3) yun une in tune 酸(syun1) Next lesson: Tones Further reading: Jyutping, The linguistic Society of Hong Kong","link":"/Canto/canto1-6/"},{"title":"Leetcode Walkthrough (Portal 🚪)","text":"This is a portal for my walkthroughs of Leetcode Questions SQL 50Question List on Leetcode","link":"/LeetCode/index/"},{"title":"Leetcode SQL 50 -- 1068. Product Sales Analysis I","text":"QuestionTable name: Sales Column Name Type sale_id int product_id int year int quantity int price int Each row of this table shows a sale on the product product_id in a certain year.Note that the price is per unit. Table name: Product Column Name Type product_id int product_name varchar Each row of this table indicates the product name of each product. Write a solution to report the product_name, year, and price for each sale_id in the Sales table. ExplainationThis question is a basic usage of JOIN query. Just be careful on which table you are referring when you select the columns. SolutionSELECT Product.product_name, Sales.year, Sales.price FROM Sales INNER JOIN Product ON Sales.product_id = Product.product_id; More SQL 50 questions: here","link":"/LeetCode/sql50-1068/"},{"title":"Leetcode SQL 50 -- 1327. List the Products Ordered in a Period","text":"QuestionTable name: Products Column Name Type product_id int product_name varchar product_category varchar This table contains data about the company’s products. Table name: Orders Column Name Type product_id int order_date date unit int unit is the number of products ordered in order_date. Write a solution to get the names of products,that have at least 100 units ordered in February 2020 and their amount.Return the result table in any order. ExplainationRetrieve the amount of productsHere, we can use SUM() function. Similar to COUNT(),we can use GROUP BY so that multiple rows that contain the same product_name will sum up into one row. SELECT based on Time RangeTo validate the date, we can use EXTRACT() function. EXTRACT(part FROM date) &lt;part&gt; in this function refers to specific piece of information about the time. Here, we can use YEAR_MONTH to check whether the date in within February 2020. For all possible parts defined by SQL, you may check the list below. SolutionSELECT p.product_name, SUM(o.unit) AS unit FROM Products p JOIN Orders o ON p.product_id = o.product_id AND EXTRACT(YEAR_MONTH FROM order_date) = 202002 GROUP BY product_name HAVING unit &gt;= 100 List of part in EXTRACT() functionHere’s the list of all possible component you can extract from date: Parts MICROSECOND WEEK MINUTE_MICROSECOND DAY_MICROSECOND SECOND MONTH MINUTE_SECOND DAY_SECOND MINUTE QUARTER HOUR_MICROSECOND DAY_MINUTE HOUR YEAR HOUR_SECOND DAY_HOUR DAY SECOND_MICROSECOND HOUR_MINUTE YEAR_MONTH More SQL 50 questions: here","link":"/LeetCode/sql50-1327/"},{"title":"Leetcode SQL 50 -- 1148. Article Views I","text":"QuestionTable name: Views Column Name Type article_id int author_id int viewer_id int view_date date There is no primary key (column with unique values) for this table, the table may have duplicate rows. Each row of this table indicates that some viewer viewed an article (written by some author) on some date. Note that equal author_id and viewer_id indicate the same person. Write a solution to find all the authors that viewed at least one of their own articles.Return the result table sorted by id in ascending order. ExplainationWe have two tasks: Authors viewed their own articles → WHERE author_id = viewer_id Sort table by id in ascending order → ORDER BY author_id ASC SQL ORDER BYORDER BY &lt;column1&gt;, &lt;column2&gt;, ... ASC|DESC; By default, ORDER BY sort rows in ascending order. And if there are multiple columns, sort the rows according to the first column, and then the second column when the rows contain same values in the first column and so on. SolutionSELECT distinct author_id AS id FROM Views WHERE author_id = viewer_id ORDER BY author_id ASC More SQL 50 questions: here","link":"/LeetCode/sql50-1148/"},{"title":"Leetcode SQL 50 -- 1378. Replace Employee ID With The Unique Identifier","text":"QuestionTable name: Employees Column Name Type id int name varchar id is the primary key (column with unique values) for this table.Each row of this table contains the id and the name of an employee in a company. Table name: EmployeeUNI Column Name Type id int unique_id int (id, unique_id) is the primary key (combination of columns with unique values) for this table.Each row of this table contains the id and the corresponding unique id of an employee in the company. Write a solution to show the unique ID of each user,If a user does not have a unique ID replace just show null.Return the result table in any order. ExplainationIn this question, we need to use JOIN: FROM &lt;table1&gt; JOIN &lt;table2&gt; ON &lt;table1.column_n&gt; = &lt;table2.column_m&gt; JOIN always comes with ON, which indicates the column used as a reference for joining.Also, there are different types referring to different usage. We are trying to include unique_id to the table EmployeesSince we need to include every users even if they don’t have a unique ID, we need to use LEFT JOIN. SolutionSELECT EmployeeUNI.unique_id, Employees.name FROM Employees LEFT JOIN EmployeeUNI ON EmployeeUNI.id = Employees.id; More SQL 50 questions: here","link":"/LeetCode/sql50-1378/"},{"title":"Leetcode SQL 50 -- 1484. Group Sold Products By The Date","text":"QuestionTable name: Activities Column Name Type sell_date date product varchar Each row of this table contains the product name and the date it was sold in a market. Write a solution to find for each date the number of different products sold and their names.The sold products names for each date should be sorted lexicographically.Return the result table ordered by sell_date. ExplainationAgain, there are two tasks we need to solve: Count how many types of products are sold in one day List all types of items into a single column Type of ItemsWe can simply count how many products are sold using COUNT, groupped by the date they belong to: SELECT COUNT(*) AS num_sold, FROM Activities GROUP BY sell_date However, if a product is sold twice in one day, they will be counted twice as well.Since the question asked for the number of different products sold,we need to specify the column we are counting, and use the keyword DINSTINCT to remove duplicates: SELECT COUNT(DISTINCT product) AS num_sold, FROM Activities GROUP BY sell_date GROUP_CONCAT(): Rolling up multiple rows in single rowTo roll data from multiple rows in single row, we may use the GROUP_CONCAT() function,and below is the syntax: GROUP_CONCAT(expr [ORDER BY {unsigned_integer | col_name | expr} ASC | DESC] [SEPARATOR str_val] ) It can be sorted with respect to other column by using ORDER BY inside the function You may define custom string as the separator of the output (default is comma) SolutionSELECT sell_date, COUNT(DISTINCT product) AS num_sold, GROUP_CONCAT(DISTINCT product) AS products FROM Activities GROUP BY sell_date More SQL 50 questions: here","link":"/LeetCode/sql50-1484/"},{"title":"Leetcode SQL 50 -- 1517. Find Users With Valid E-Mails","text":"QuestionTable name: `` Column Name Type user_id int name varchar mail varchar This table contains information of the users signed up in a website. Some e-mails are invalid. A valid e-mail has a prefix name and a domain. domain: @leetcode.com.prefix name is a string that may contain: letters (upper or lower case) digits underscore _ period . dash - (The prefix name must start with a letter.) Write a solution to find the users who have valid emails.Return the result table in any order. ExplainationFor string validation, there is a powerful tool for pattern matching called RegEx. In SQL, it provides the keyword REGEXP to enter your pattern for validation.Here, we simply explain the patterns we used to match the requirement. Pattern Meaning ^ Match Beginning of string $ Match The End of string [a-zA-Z] Match any upper and lower case letter [a-zA-Z0-9_.-] Match any upper and lower case letter, digits, underscore, period, dash * Zero or more instances of string preceding it Therefore, ^[a-zA-Z][a-zA-Z0-9_.-]* means: Begins the string with any upper or lower case letter,follows by zero or more upper and lower case letter, digits, underscore, period, and/or dash. SolutionSELECT * FROM Users WHERE mail REGEXP '^[a-zA-Z][a-zA-Z0-9_.-]*@leetcode\\\\.com$' More SQL 50 questions: here","link":"/LeetCode/sql50-1517/"},{"title":"Leetcode SQL 50 -- 1527. Patients With a Condition","text":"QuestionTable name: Patients Column Name Type patient_id int patient_name varchar conditions varchar conditions contains zero or more code separated by spaces.This table contains information of the patients in the hospital. Write a solution to find the patient_id, patient_name, and conditions of the patients who have Type I Diabetes.Type I Diabetes always starts with DIAB1 prefix. Return the result table in any order. ExplainationSQL LIKE OperatorIn SQL, to extract data that contain certain specified pattern, we can use LIKE in the WHERE clause.For example, we can use SELECT * FROM Patients WHERE conditions LIKE 'DIAB1%' In here, % is a wildcard, refers to any number of characters can exist after the string DIAB1.DIAB1, DIAB1234, DIAB1eg are valid outputs. There is another wildcard _, refers to exactly one single character.aDIAB12, 2DIAB1 are valid outputs of _DIAB1 Valid outputsIn this question, conditions which contain valid DIAB1 are either: DIAB1 is after some conditions, for exmaple ACNE DIAB100, or the list of conditions begins with DIAB1, for example DIAB100 MYOP. Therefore, we have to include both cases with two separate LIKE operator. SolutionSELECT * FROM Patients WHERE conditions LIKE '% DIAB1%' OR conditions LIKE 'DIAB1%' More SQL 50 questions: here","link":"/LeetCode/sql50-1527/"},{"title":"Leetcode SQL 50 -- 1581. Customer Who Visited but Did Not Make Any Transactions","text":"QuestionTable name: Visits Column Name Type visit_id int customer_id int This table contains information about the customers who visited the mall. Table name: Transactions Column Name Type transaction_id int visit_id int amount int This table contains information about the transactions made during the visit_id. Write a solution to find the IDs of the users who visited without making any transactions, andthe number of times they made these types of visits.Return the result table sorted in any order. ExplainationThis is another typical JOIN query question.The tricky part is the column we join on doesn’t necessarily be selected. Moreover, we can control which rows to be joined using WHERE, as simple SELECT. Lasting to remember is to use GROUP BY for your COUNT() function,to make sure they counted separately based on customer_id. SolutionSELECT customer_id, count(*) AS count_no_trans FROM Visits LEFT JOIN Transactions ON Visits.visit_id=Transactions.visit_id WHERE transaction_id IS NULL GROUP BY customer_id More SQL 50 questions: here","link":"/LeetCode/sql50-1581/"},{"title":"Leetcode SQL 50 -- 1667. Fix Names in a Table","text":"QuestionTable name: Users Column Name Type user_id int name varchar This table contains the user_id and the name of the user. The name consists of only lowercase and uppercase characters. Write a solution to fix the names so that only the first character is uppercase and the rest are lowercase.Return the result table ordered by user_id.The result format is in the following example. ExplainationWe can split the tasks into three part. Split the name into first character and the remaining characters → SUBSTRING() Modify them into upper case and lower case respectively → UPPER() and LOWER() Order the result by user_id → ORDER BY SQL SUBSTRING() FunctionFor extracting characters from a string, we can use the function SUBSTRING(&lt;string&gt;, start, end).For example, SUBSTRING(name, 1, 1) returns the first character from string name,and name without the first letter. SUBSTRING(name, 2, LENGTH(name)) SQL UPPER(), LOWER(), and CONCAT()For text converting, we can simply use UPPER(&lt;text&gt;) and LOWER(&lt;text&gt;).Then we can combine two substring together by using CONCAT(&lt;string1&gt;, &lt;string2&gt;, ... , &lt;string_n&gt;) and finally, use ORDER BY user_id. SolutionSELECT user_id, CONCAT( UPPER(SUBSTRING(name, 1, 1)), LOWER(SUBSTRING(name, 2, LENGTH(name))) ) AS name FROM Users ORDER BY user_id More SQL 50 questions: here","link":"/LeetCode/sql50-1667/"},{"title":"Leetcode SQL 50 -- 1683. Invalid Tweets","text":"QuestionTable name: Tweets Column Name Type tweet_id int content varchar tweet_id is the primary key (column with unique values) for this table.This table contains all the tweets in a social media app. Write a solution to find the IDs of the invalid tweets.The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.Return the result table in any order. ExplainationTo check the number of character, we can use the LENGTH(&lt;string&gt;) function. If you use this function on numbers (e.g. int), it will turn it into string and return number of digits. SolutionSELECT tweet_id FROM Tweets WHERE LENGTH(content)&gt;15; More SQL 50 questions: here","link":"/LeetCode/sql50-1683/"},{"title":"Leetcode SQL 50 -- 1757. Recyclable and Low Fat Products","text":"QuestionTable name: Products Column Name Type product_id int low_fats enum recyclable enum product_id is the primary key (column with unique values) for this table. low_fats is an ENUM of type (Y, N) where Y means this product is low fat and N means it is not. recyclable is an ENUM of types (Y, N) where Y means this product is recyclable and N means it is not. Write a solution to find the ids of products that are both low fat and recyclable.Return the result table in any order. ExplainationThis is one the most basic SQL SELECT query scenario,by using SELECT &lt;column_name&gt; FROM &lt;table_name&gt; WHERE &lt;conditions&gt; you may get the solution easily. SolutionSELECT product_id FROM Products WHERE low_fats = &quot;Y&quot; AND recyclable = &quot;Y&quot;; More SQL 50 questions: here","link":"/LeetCode/sql50-1757/"},{"title":"Leetcode SQL 50 -- 176. Second Highest Salary","text":"QuestionTable name: `` Column Name Type id int salary int Each row of this table contains information about the salary of an employee. Write a solution to find the second highest distinct salary from the Employee table.If there is no second highest salary, return null. ExplainationThe key idea of this question is to get the second value. Here I have two ideas that come to my mind immediately First, order the list according to salary, then fetch the second row. First, find the highest salary, then do another SELECT query based on the highest salary. 1. Using LIMIT and OFFSETUsing LIMIT and OFFSET, we can simply retrieve the second row of a database: SELECT DISTINCT Salary FROM Employee ORDER BY salary DESC LIMIT 1 OFFSET 1 Here, we used ORDER BY salary DESC and DISTINCT to make sure the second row contains the second highest salary.Then LIMIT 1 constraints there will be only one row returned, OFFSET 1 means to skip 1 row before fetching. However, this query won’t return null if there isn’t a second row. To solve the problem, we can wrap this query by another SELECT query. 2. Recursive SELECTThere will be two SELECT function, the SELECT in WHERE clause returns the highest salary: WHERE salary &lt; (SELECT MAX(salary) FROM Employee) So that the SELECT query outside can directly compare with this and thus it returns the second highest salary. SolutionUsing LIMIT and OFFSETSELECT( SELECT DISTINCT Salary FROM Employee ORDER BY salary DESC LIMIT 1 OFFSET 1 ) AS SecondHighestSalary; Recursive SELECTSELECT MAX(salary) AS SecondHighestSalary FROM Employee WHERE salary &lt; (SELECT MAX(salary) FROM Employee); More SQL 50 questions: here","link":"/LeetCode/sql50-176/"},{"title":"Leetcode SQL 50 -- 196. Delete Duplicate Emails","text":"QuestionTable name: Person Column Name Type id int email varchar Each row of this table contains an email. The emails will not contain uppercase letters. Write a solution to delete all duplicate emails, keeping only one unique email with the smallest id.For SQL users, please note that you are supposed to write a DELETE statement and not a SELECT one.The final order of the table does not matter. ExplainationUsually, people will simply replace the old table with the new table using SELECT to remove the duplicates.The tricky part of this question is that you cannot create a new table, as the system only checks the original table Person. Since we cannot use SELECT, the only way to filter out unwanted rows is to use WHERE or HAVING. First of all, we can call the same table twice by giving them different name temporarily, so that we can compare between rows, We need to keep the same email which has smaller id, so p.Id &gt; q_Id, then we also need to nake sure that there are acutally duplicate email, so p.Email=q.Email SolutionDELETE p from Person p, Person q where p.Id&gt;q.Id AND q.Email=p.Email More SQL 50 questions: here","link":"/LeetCode/sql50-196/"},{"title":"Leetcode SQL 50 -- 197. Rising Temperature","text":"QuestionTable name: Weather Column Name Type id int recordDate date temperature int There are no different rows with the same recordDate.This table contains information about the temperature on a certain day. Write a solution to find all dates' id with higher temperatures compared to its previous dates (yesterday).Return the result table in any order. ExplainationHere, we need to learn how to handle the datatype date.To modify the date, we can use INTERVAL keyword, or DATEADD() function. INTERVAL &lt;value&gt; &lt;unit&gt;Interval is a special datatype, that can interact with time and date. Interval unit includesmicrosecond, millisecond, second, minute, hour, day, week, month, year, decade, century, millennium For example, '2001-09-28' + INTERVAL 1 DAY → 2002-09-28 DATEADD(&lt;interval&gt;, &lt;number&gt;, &lt;date&gt;)Similarly, we can use DATEADD() to modify data with datatype date. DATEADD(year, 1, '2001-09-28') → 2002-09-28 SolutionSELECT w1.id FROM Weather w1 JOIN Weather w2 ON w1.recordDate = w2.recordDate + INTERVAL 1 DAY WHERE w1.temperature &gt; w2.temperature; More SQL 50 questions: here","link":"/LeetCode/sql50-197/"},{"title":"Leetcode SQL 50 -- 584. Find Customer Referee","text":"QuestionTable name: Customer Column Name Type id int name varchar referee_id int id is the primary key column for this table.Each row of this table indicates the id of a customer, their name, and the id of the customer who referred them. Find the names of the customer that are not referred by the customer with id = 2.Return the result table in any order. ExplainationAgain we use SELECT &lt;column_name&gt; FROM &lt;table_name&gt; WHERE &lt;conditions&gt; to get the solution. The tricky part is some of the referee_id may contain NULL value. NULL handlingWe may simply think that the solution is WHERE referee_id !=2 However, NULL always return false in any comparison. For example: SELECT * FROM Customer WHERE referee_id != 2 OR referee_id = 2 This query will return all rows that their referee_id is not NULL. Because NULL returns false in both cases. In order to retrieve rows that contain NULL, we have to use the keyword IS NULL. For further explaination of NULL Handling in SQL, you may check the sqlite documentation SolutionSELECT &quot;name&quot; FROM Customer WHERE referee_id != 2 OR referee_id IS NULL; More SQL 50 questions: here","link":"/LeetCode/sql50-584/"},{"title":"Leetcode SQL 50 -- 595. Big Countries","text":"QuestionTable name: World Column Name Type name varchar continent varchar area int population int gdp bigint name is the primary key (column with unique values) for this table. Each row of this table gives information about the name of a country,the continent to which it belongs, its area, the population, and its GDP value. Big CountryA country is big if: it has an area of at least three million (i.e., 3000000 km2), or it has a population of at least twenty-five million (i.e., 25000000). Write a solution to find the name, population, and area of the big countries.Return the result table in any order. ExplainationAgain we use SELECT &lt;column_name&gt; FROM &lt;table_name&gt; WHERE &lt;conditions&gt; to get the solution. As question mention we should use OR and at least refers to &gt;=. SolutionSELECT name, population, area FROM World WHERE area &gt;= 3000000 OR population &gt;= 25000000; More SQL 50 questions: here","link":"/LeetCode/sql50-595/"},{"title":"Leetcode SQL 50 (Portal 🚪)","text":"This is a portal for my walkthroughs of SQL 50 on Leetcode Link of Leetcode SQL 50 Select 1757. Recyclable and Low Fat Products 584. Find Customer Referee 595. Big Countries 1148. Article Views I 1683. Invalid Tweets Basic Joins 1378. Replace Employee ID With The Unique Identifier 1068. Product Sales Analysis I 1581. Customer Who Visited but Did Not Make Any Transactions 197. Rising Temperature Advanced String Functions / Regex / Clause 1667. Fix Names in a Table 1527. Patients With a Condition 196. Delete Duplicate Emails 176. Second Highest Salary 1484. Group Sold Products By The Date 1327. List the Products Ordered in a Period 1517. Find Users With Valid E-Mails","link":"/LeetCode/sql50/"},{"title":"🍽 🥨 MensaarLecker -- A beloved tool to find out Mensa Ladies&#39; favourite menu using Selenium🥨 🍽","text":"Repository: MensaarLecker As an UdS Student,Are you tired of seeing french fries🍟 3 times a week, or wondering when I can have the best pizza 🍕 in the Mensacafe?MensaarLecker aims to collect all the data from Menu 1, 2, and Mensacafe to trace your favourite, or Mensa Ladies’, favourite menu! 🥗 DescriptionA fully automated scraper and static website for the Saarbrücken Mensa, powered by Python, Selenium, Google Sheets, and GitHub Actions. Get a clean and daily-updated overview of meals from mensaar.de, with searchable history, meal components, and frequency stats. 🌐 Live Demo👉 View Website👉 View Data in Google Sheets 📅 Features ✅ Scrapes the Saarbrücken Mensa daily menu ✅ Publishes structured data to a connected Google Sheet ✅ Generates static HTML pages: index.html – Today’s menu with meal frequency counts menu.html – Full searchable menu with DataTables ✅ Automatically updates via GitHub Actions at 10:00 AM UTC on weekdays ✅ Beautiful card-style layout &amp; component display ✅ No server required — 100% static 🧠 Meal Frequency Display ExampleThe homepage shows how often each meal has been served based on historical data since 2025.03.20: 🍽️ Pasta mit Tomatensoße📊 Seen since 2025.03.20✅ Geriebener Käse✅ Rucola 📁 Project Structure. ├── Mensaar_scraper.py # Scrapes from mensaar.de and writes to Google Sheet ├── generate_menu.py # Reads the sheet and generates index.html and menu.html ├── credentials.json # Google service account key (excluded from repo) ├── index.html # Main website page with today's menu ├── menu.html # Full searchable table of meals ├── .github/workflows/ │ └── update_menu.yml # GitHub Actions automation ├── src/ │ └── uds_spirit.jpg # Soul of this project └── README.md 📝 Development LogMensaarLecker Development Log 1 – Web CrawlingMensaarLecker Development Log 2 – Web Developing and GitHub Workflow","link":"/projects/mensaar/"},{"title":"MensaarLecker Development Log 1 -- Web Crawling","text":"Repository: MensaarLecker MotivationMe and my friends hatelove the UdS Mensa so much! The infinite frozen food and french fries menus give us so much energy and motivation for the 5-hour afternoon coding marathon. However, no one actually knows how many potatoes they have exterminated throughout the week. We have a genius webpage created by some Schnitzel lover. Personally, I like its minimalistic layout and determination on Schnitzel searching. However, we want more. It’s not just Schnitzel; we want to know everything about their menu. We want to know what’s inside the mensa ladies’ brains when they design next week’s menu. The desire never ends. We need more data, more details, more, More, MORE! Developing ProcessOur Goal here is simple: Scrape the Mensa menu every weekday and store it to Google Sheets Fetch the Data Collection from Google Sheets and update the website Web ScrapingTo collect the data, we can use Python libraries to simplfied the process. But the basic idea it the same: we try to find the pattern of the HTML tag and locate the desired data. Beautiful SoupI started my journey with Beautiful Soup, one of the most popular Python web scraper packages. However, as a Uni that is well-known for its computer science program, all the menus are rendered using JavaScript. And beautiful can only scrape HTML and XML tags. So the scraper can only see an empty skeleton page: SeleniumBasically, Selenium is a Webdriver that opens a browser naturally, like a human user. Then from there we can scrape the rendered information. Things get simpler once we can see the website as we see it on the browser. We just need to find the tag that contains the information we need and save it for storage. Desired Data and Tags Data Tag menus &lt;div class=&quot;counter&quot;&gt; date &lt;div class=&quot;cursor-pointer active list-group-item&quot;&gt; main dish &lt;span class=&quot;meal-title&quot;&gt; side dish &lt;div class=&quot;component&quot;&gt; The first part of the task is to get the daily menu. We also get the date on the website to make the following work easier. By the find_element and find_elements functions in Selenium, we can create a simple scraper like this: from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.by import By driver = webdriver.Firefox() driver.get(&quot;https://mensaar.de/#/menu/sb&quot;) counters = driver.find_elements(By.CLASS_NAME, &quot;counter&quot;) for counter in counters: meal_title = meal.find_element(By.CLASS_NAME, &quot;meal-title&quot;).text.strip() However, on the webpage there is also a counter called Wahlessen. Which refers to a more pricy and unpredictable menu, and we want to exclude its data: counter_title = counter.find_element(By.CLASS_NAME, &quot;counter-title&quot;).text.strip() # Filter for specified counter titles if counter_title in [&quot;Menü 1&quot;, &quot;Menü 2&quot;, &quot;Mensacafé&quot;]: meal_title = meal.find_element(By.CLASS_NAME, &quot;meal-title&quot;).text.strip() StorageIn order to make the database easy to be accessed by other users/students, we decided to deploy the data set to Google SpreadSheets. with open(output_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(result, f, ensure_ascii=False, indent=2) print(f&quot;Results saved to {output_file}&quot;) # Save the updated occurrence counts to the JSON file count_result = { &quot;meal_counts&quot;: dict(meal_count), &quot;component_counts&quot;: dict(component_count) } with open(count_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(count_result, f, ensure_ascii=False, indent=2) print(f&quot;Counts saved to {count_file}&quot;) Change the time formatOnce we fetch data, you may notice that the website display the date in german format e.g. “Freitag, 21. März 2025”, which is not recognized by Google Sheets directly. So we need to make a function to convert them before uploading: GERMAN_MONTHS = { &quot;Januar&quot;: &quot;01&quot;, &quot;Februar&quot;: &quot;02&quot;, &quot;März&quot;: &quot;03&quot;, &quot;April&quot;: &quot;04&quot;, &quot;Mai&quot;: &quot;05&quot;, &quot;Juni&quot;: &quot;06&quot;, &quot;Juli&quot;: &quot;07&quot;, &quot;August&quot;: &quot;08&quot;, &quot;September&quot;: &quot;09&quot;, &quot;Oktober&quot;: &quot;10&quot;, &quot;November&quot;: &quot;11&quot;, &quot;Dezember&quot;: &quot;12&quot; } def format_date(german_date): match = re.search(r&quot;(\\d{1,2})\\. (\\w+) (\\d{4})&quot;, german_date) if match: day, month, year = match.groups() month_number = GERMAN_MONTHS.get(month, &quot;00&quot;) return f&quot;{year}-{month_number}-{int(day):02d}&quot; return &quot;0000-00-00&quot; Upload the data to Google SheetsIn order to interact with the Google Sheets, we need to use Google API. First, go to Google Cloud Console. Create a new project. Next, go to API and Services, click Enable API and Services Search Google Sheets API, select it and choose Enable。 Move Credentials from the sidebar, then choose Create credentials → Create service account In step 2, choose the role Editor Now, when you come back to the Credentials page, you should see a newly generated email under Service Accounts, click it and select the tab Keys Select Create new key, choose JSON format, the file should start downloading automatically. Important NoticeThis JSON file contains sensitive data, you should NEVER directly use it in your code, save it as an environment variable or save it as a secret on Github With this key we can login the email we just created in Service Accounts, so that it is treated as a virtual users when running the script. Same as human users, in order to access the sheet we need to add this email as an editor in Google Sheets. try: # Read and validate credentials.json before using it if not os.path.exists(&quot;credentials.json&quot;): print(&quot;❌ credentials.json not found!&quot;) return with open(&quot;credentials.json&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: raw_creds = f.read() creds_data = json.loads(raw_creds) # Save to a temp file just in case gspread needs it as a file temp_path = &quot;parsed_credentials.json&quot; with open(temp_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(creds_data, f) creds = ServiceAccountCredentials.from_json_keyfile_name(temp_path, scope) client = gspread.authorize(creds) sheet = client.open(SHEET_NAME).sheet1 print(&quot;✅ Google Sheets Auth OK&quot;) End of ScrapingNow we all set! Next, we need to display our collected results on web interfaces. Continue Reading: MensaarLecker Development Log 2 – Web Developing and GitHub Workflow","link":"/projects/mensaarlog1/"},{"title":"MensaarLecker Development Log 2 -- Web Developing and GitHub Workflow","text":"Repository: MensaarLecker Fetching Data from Web DevelopmentContinuing from last post, we have already implemented a script that collect the Mensa menu and stored it on Google Sheets. It is time to build our web interface to connect the database. Fetch Data from Google Sheets using Publish First, we need to publish our spreadsheet so that it is public to fetch the data. In the Spreadsheet, click Share → Change access to Anyone with the link. Click File → Share → Publish to the web. Select Entire Document → Comma-separated values (.csv) and click Publish. Copy the public CSV link. SCRIPT_URL = {PUBLISH_LINK} # Fetch JSON data def fetch_menu(): try: response = requests.get(SCRIPT_URL) response.raise_for_status() # Raise error if bad response return response.json() except requests.exceptions.RequestException as e: print(f&quot;❌ Error fetching menu: {e}&quot;) return [] However, the script return no data, why? Access to fetch at 'https://docs.google.com/spreadsheets/...' from origin 'null' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled. CORS Policy and XSS Cross-origin resource sharing (CORS) is an extension of the same-origin policy. You need it for authorized resource sharing with external third parties. – Amazon Web Services This is a cyber security scheme to avoid XSS (Cross-site scripting), in a nutshell, when we run the script, the code cannot proceed because it doesn’t login to any Google account! You can imagine the request is block by a imaginary login page and our program doesn’t know how to react. Second Attempt – Google Apps ScriptGoogle doesn’t allow users to fetch their data casually, except this is executed under Google’s server. This means we need to run our fetching function using Google’s service. Apps Script provide a JavaScript editor to save your code. function doGet() { var sheet = SpreadsheetApp.openById(&quot;PUT_YOUR_SHEET_ID_HERE&quot;).getActiveSheet(); var data = sheet.getDataRange().getValues(); var headers = data[0]; var jsonData = []; for (var i = 1; i &lt; data.length; i++) { var row = {}; for (var j = 0; j &lt; headers.length; j++) { row[headers[j]] = data[i][j]; } jsonData.push(row); } var output = ContentService.createTextOutput(JSON.stringify(jsonData)); output.setMimeType(ContentService.MimeType.JSON); return output; } To get the sheet ID, we can simply open the sheet and it is part of the URL: Deploying the function and fetch the dataAfterwards, we can deploy this function and it will generate a unique URL for the function output and we can fetch the data (here we export the data in json format) and use it in our code. You can also double check the URL and make sure it does return the value correctly → link SCRIPT_URL = &quot;URL_DEPLOYED_FROM_APP_SCRIPT&quot;; # Fetch JSON data def fetch_menu(): try: response = requests.get(SCRIPT_URL) response.raise_for_status() # Raise error if bad response return response.json() except requests.exceptions.RequestException as e: print(f&quot;❌ Error fetching menu: {e}&quot;) return [] Webpage ImplementationTo simplify our automation process on GitHub, we will continue implement our HTML code using Python. Our website should have two pages: index.html contains today’s menu, and menu.html contains the history of collected menus. Static data – Tripe QuotesWe can put all the static code in strings. In Python we can store multi-line strings using triple quotes. html = f&quot;&quot;&quot;&lt;!DOCTYPE html&gt; &lt;html lang=&quot;en&quot;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Mensaar Today&lt;/title&gt; &lt;style&gt; body {{ font-family: Arial, sans-serif; padding: 20px; text-align: center; background-image: url('src/uds_spirit.jpg'); }} h1 {{ background: rgba(255, 255, 255, 0.8); color: #003C71; padding: 10px 20px; display: inline-block; border-radius: 10px; box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2); }} .container {{ width: 80%; margin: auto; }} .closed-message {{ font-size: 26px; color: red; font-weight: bold; padding: 20px; background: #fff3f3; border-radius: 10px; }} .menu-card {{ background: white; padding: 15px; margin: 10px 0; border-radius: 10px; box-shadow: 2px 2px 10px rgba(0,0,0,0.1); text-align: left; }} .meal-title {{ font-size: 20px; font-weight: bold; }} .meal-components {{ font-size: 16px; color: #666; }} .meal-frequency {{ font-size: 14px; color: #888; font-style: italic; }} .button {{ padding: 12px 20px; background: #007bff; color: white; border-radius: 5px; text-decoration: none; }} &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;Mensaar Menu for {today}&lt;/h1&gt;&lt;/br&gt; &lt;a href=&quot;menu.html&quot; class=&quot;button&quot;&gt;📜 View Full Menu&lt;/a&gt; &lt;div class=&quot;container&quot;&gt; &quot;&quot;&quot; Enhance the design using DataTableSince we will collect the menu everyday, the table in menu.html will become too long for loading and hard to check. We can use DataTable that provide basic table layout like filtering, searching, and sorting. Also, it is very easy to implement, simply include the JavaScript and CSS link in the HTML code and you can get the basic, but decent design. Automation with GitHub WorkflowFinally, after we deployed the code to GitHub, remember our original goal: Scrape the Mensa menu every weekday and store it to Google Sheets Fetch the Data Collection from Google Sheets and update the website In fact, we can run the python script periodically using Github workflow, here are the steps: The workflow has to be implemented in .yml format, and stored in .github/workflows/{workflow_name}.yml Before running the python script, make sure python is set with all the dependencies installed: - name: 🛠 Set up Python uses: actions/setup-python@v4 with: python-version: &quot;3.x&quot; - name: 📦 Install dependencies run: | pip install requests selenium webdriver-manager gspread oauth2client - name: 🚀 Run Mensaar Scraper (update Google Sheets) run: | echo &quot;🧪 Starting Mensaar_scraper...&quot; python Mensaar_scraper.py echo &quot;✅ Scraper completed.&quot; - name: 🖼️ Run HTML Generator run: | echo &quot;🧪 Generating index.html &amp; menu.html&quot; python generate_menu.py For the full workflow, you can find the skeleton code template on GitHub or you can check here.","link":"/projects/mensaarlog2/"}],"tags":[{"name":"Appium","slug":"Appium","link":"/tags/Appium/"},{"name":"Automation","slug":"Automation","link":"/tags/Automation/"},{"name":"App Testing","slug":"App-Testing","link":"/tags/App-Testing/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"GitHub","slug":"GitHub","link":"/tags/GitHub/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Latex","slug":"Latex","link":"/tags/Latex/"},{"name":"Mathjax","slug":"Mathjax","link":"/tags/Mathjax/"},{"name":"Node.js","slug":"Node-js","link":"/tags/Node-js/"},{"name":"HTML","slug":"HTML","link":"/tags/HTML/"},{"name":"Cantonese","slug":"Cantonese","link":"/tags/Cantonese/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"PV","slug":"PV","link":"/tags/PV/"},{"name":"LTS","slug":"LTS","link":"/tags/LTS/"},{"name":"AGV","slug":"AGV","link":"/tags/AGV/"},{"name":"Language Learning","slug":"Language-Learning","link":"/tags/Language-Learning/"},{"name":"Phonology","slug":"Phonology","link":"/tags/Phonology/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Scraper","slug":"Scraper","link":"/tags/Scraper/"},{"name":"Selenium","slug":"Selenium","link":"/tags/Selenium/"}],"categories":[{"name":"Coding","slug":"Coding","link":"/categories/Coding/"},{"name":"Site_Note","slug":"Site-Note","link":"/categories/Site-Note/"},{"name":"Projects","slug":"Projects","link":"/categories/Projects/"},{"name":"Notes","slug":"Notes","link":"/categories/Notes/"},{"name":"UdS","slug":"Notes/UdS","link":"/categories/Notes/UdS/"},{"name":"Cantonese","slug":"Cantonese","link":"/categories/Cantonese/"},{"name":"Full_Course","slug":"Cantonese/Full-Course","link":"/categories/Cantonese/Full-Course/"},{"name":"LeetCode","slug":"LeetCode","link":"/categories/LeetCode/"},{"name":"SQL_50","slug":"LeetCode/SQL-50","link":"/categories/LeetCode/SQL-50/"}],"pages":[{"title":"About Me","text":"Personal Info Click HERE for my detailed resume. Chun Ngai Li (Alex), M.Sc. Cybersecurity in Saarland University,B.Eng Information Engineering in CUHK. Recent WorksA C/C++ programmer. Also working on Backend &amp; Python developing.Current research: Social Engineering, Large Language Models (LLM), Machine Learning ContactsEmail: alexcnli@yahoo.com","link":"/about/index.html"},{"title":"GreenMeeple cannot find what you are looking for...","text":"","link":"/404.html"},{"title":"Resume of Chun Ngai Li (Alex)","text":"For pdf version, click HERE About MeCurrently pursuing a Master’s in Cybersecurity, showcasing a strong foundation in penetration testing and AI-driven security solutions. Proficient in Python, C/C++, and JavaScript, with a proven track record of developing automation tools and secure web applications. Passionate about enhancing cybersecurity measures and risk assessment, eager to contribute to cutting-edge projects in international teams that prioritize seurity and scalability. Personal DetailsName: Chun Ngai LiPhone: +49 163-513-2617Address: 66125 Saarbrücken, GermanyDate &amp; Place of Birth: 27.09.1999, Hong Kong Links and Websites Github LinkedIn LeetCode Personal Site (Here) EducationOctober 2021 – March 2025M.Sc. Cybersecurity in Universität des Saarlandes, Saarbrücken Thesis Topic: Exploring the Cybersecurity Threats in LLM-Powered Apps: Malicious Code Generation and Regulatory Challenges Current Grade: 2.4 September 2017 – May 2021B.Eng. Information Engineering in The Chinese University of Hong Kong, Hong Kong GPA: 3.275/4 (Equivalent to 1.54 in German System) Minor study: German Exchanged in KTH Royal Institute of Technology, Sweden in 2019-2020 Work ExperienceMarch 2022 – March 2023IT Research Assistant at Universität des Saarlandes, Saarbrücken Developed a full-stack Intranet for the Faculty of Business using PHP7, JavaScript, and MySQL to maintain appointment and administrative work Developed Monte Carlo Algorithm using Microsoft Excel and VBA to create automation and user-friendly interface Enhanced Student website portal to provide resources, lending, significant and minor selection, and transcript March 2021 – May 2021Software Engineer at Hong Kong Jockey Club, Hong Kong Worked for AI for the Future Project Developed a Mobile App using React Native and Python to control 3D-printing Robots via Bluetooth Created a documentation and user manual for the App and Robots to ensure effective communications and demonstrations to clients November 2020 – February 2021Cybersecurity Undergraduate Researcher at The Chinese University of Hong Kong, Hong Kong Research Topic: Morpheus: Bringing the (pkcs) one to meet the oracle Contributed to pioneering cybersecurity research led by Dr. Sherman S. M. Chow, focusing on RSA\\PKCS1 signature verification Developed versatile black-box fuzzing tools using C++, Java, Python, and C# to investigate signature verification vulnerabilities June 2020 – September 2020Software &amp; Technology Analyst at Sharp Peak Consulting Limited , Hong Kong Developed a Booking System for Clinic using Microsoft Power Apps Engaged in on-site visits to various corporations to install and maintain IoT products Contributed to Cybersecurity consulting towards server and network services Languages★★☆☆☆ German★★★★★ English★★★★★ Cantonese★★★★★ Mandarin Skills★★★★★ C★★★★★ C++★★★★★ SQL★★★★☆ PHP★★★★☆ Python★★★★☆ HTML &amp; CSS★★★★☆ JavaScript","link":"/about/resume.html"}]}